---
title: r-pca
author: ''
date: '2019-03-04'
slug: r-pca
draft: true
categories: []
tags: [R, pca, data-science]
thumbnailImagePosition: left
thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg
---

Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.

<!--more-->

```{r setup, include=FALSE}
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.



# 1. Introduction

Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.

# 2. PCA - A Brief Introduction

PCA dates back to the early 20th century, and is used to reduce the dimensionality of data (technically, it's a _linear_ dimensionality reduction procedure. We won't get into what that means in this post).

The reason you might want to do this is because high dimensional data might lie in a low dimensional subspace. We want to "re-state" this data in that lower dimensional space; that is, we want to get rid of those superfluous extra dimensions.

# 3. Intuition

A simple example could be collecting data about houses in your neighbourhood; you collect features such as:

- Number of windows
- Number of bedrooms
- House price

Each one of these is another dimension in your dataset. Say you mistakenly collect the area of your house both in square meters _and_ in square feet. You now have 5 dimensional data, but really, it only lies in a 4 dimensional subspace. 

This is obviously a contrived example, but hopefully you get the picture.

# 3. Implementation in R

Let's generate data which simulates some marks in two tests, a maths test and a physics test:

```{r}
set.seed(1337)
mathScore <- runif(100) * 100
physicsScore <- mathScore + rnorm(100, sd = 8)

#Let's make sure that there are no negative marks:
physicsScore <- pmax(0, physicsScore)

#And let's make sure that there are no marks greater than 100:
physicsScore <- pmin(100, physicsScore)

allTestResults <- data.frame(math = mathScore, physics = physicsScore)
plot(allTestResults$math, allTestResults$physics, xlab = "Math", ylab = "Physics")
```

We can see from the picture above that the data are pretty correlated: 

```{r}
cor(allTestResults$math, allTestResults$physics)
```

Maybe there is some underlying, latent "smartness" dimension upon which the data truly lie. Let us attempt to determine 

```{r}
pca_results <- prcomp(allTestResults, center = TRUE) #Don't need to scale, since both axes already on same scale, but centering is NB

print (pca_results)
plot(pca_results$x)
```

```{r}
# Let's scale the plot:
plot(pca_results$x, xlim = c(-60, 80), ylim = c(-60, 80))
```


```{r}
screeplot(pca_results)
```
```{r}
pov <- pca_results$sdev^2/sum(pca_results$sdev^2)
```
Percentage of variance explained by the first principal component: `r pov[[1]] * 100`%

```{r}
biplot(pca_results)
```

```{r}
# allTestResults[8, ]
# allTestResults[34, ]
# allTestResults[100, ]
# allTestResults[57, ]
# allTestResults[84, ]
# allTestResults[39, ]
```

```{r}
allTestResults[8, ]
allTestResults[34, ]
allTestResults[100, ]
allTestResults[57, ]
allTestResults[84, ]
allTestResults[39, ]
```
