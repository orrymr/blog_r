---
title: r h2o introduction
author: ''
date: '2018-12-04'
slug: r-h2o-introduction
draft: true
categories: []
tags: []
---

# 1. Introduction
[H2O](https://www.h2o.ai/) ["is an in-memory platform for distributed, scalable machine learning."](https://github.com/h2oai/h2o-3). In this post we unpack what this statement means, and get started working with H2O using R.

# 2. What is H2O?

The definition found on H2O's Github page is a lot to take in, especially if you're just starting out with H2O (like I am!): "H2O is an in-memory platform for distributed, scalable machine learning. H2O uses familiar interfaces like R, Python, Scala, Java, JSON and the Flow notebook/web interface, and works seamlessly with big data technologies like Hadoop and Spark."

Let's break down this statement piece by piece.

## 2.1 H2O is an In-Memory Platform

They start off by telling us that H2O is an "__in-memory platform__". Saying that it's in-memory means that the data and software being used is in main memory (RAM). Main memory, or __primary memory__ is typically much faster than secondary memory (such as a hard drive). A software platform is typically something which can be used to build something. We happen to know that this platform is used for building machine learning models. Putting this togther we now know that H2O is an in-memory environment for building machine learning models.

## 2.2 H2O is distributed and Scalable

__"...for distributed, scalable machine learning"__

H2O runs on clusters. Although I haven't actually tried this (yet), I've read that H2O works on clusters such as Hadoop. It's something that I still need to experiment with, and when I do, I'll post a link here. So, for now, I'm just going to interpret the phrase "distributed, scalable machine learning" as saying that we can run the H2O platform on a cluster.

One thing to note, is that H2O uses what's known as a Distributed Key Value (DKV). You can read more about it [here](https://www.h2o.ai/blog/h2o-architecture/), but essentially what this means, is that any object you create in H2O can be distributed amongst several nodes in the cluster. Of course, if you aren't using a cluster - and just using a laptop, like I did with the example that follows - you need need to have enough memory to hold your objects locally. Whether you are using a cluster, or running locally, once you've imported your data into the H2O platform, you get back a hexadecimal key which you can then use to access your data.

So, __Distributed Key Value__: It's distributed because your object can be spread amongst several nodes in your cluster. And the key-value part means that you get back a key into what is effectively a hashmap to your object.

This part doesn't flow nicely into the next

# 3. How H2O runs under the hood

You can install H2O using R: `install.packages("h2o")`. If you're having trouble with this, [have a look here.](http://h2o-release.s3.amazonaws.com/h2o/rel-xia/2/index.html)

We spoke earlier about H2O being a platform. It's important to distinguish between the R ___interface___ for H2O, and H2O itself. H2O can exist perfectly fine without R. H2O is just a [.jar](https://en.wikipedia.org/wiki/JAR_(file_format)) which can run on its own. If you don't know (or particularly care) what a .jar is - just think of it as a java program packaged with all the stuff it needs to run.

When you start H2O, you actually create a server which can respond to [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) calls. Again, you don't really need to know how REST works in order to use H2O. But if you do care, just know that you can use any HTTP client to speak with an H2O instance.

R is just a client interfact for H2O. All the R functions you call when working with H2O are actually calling H2O using a REST API (a JSON POST request) under the hood. The Python H2O library, as well as the Flow UI (link here) interfaces with H2O in a similar way. __If this is all very confusing just think about it like this: you use R to send commands to H2O.__ You could equally just use Flow or Python to send commands.

Data is not in R, R only has a pointer to the data.



Distributed Frame (15 minutes into the video)

Essentially, when reading in a csv, or data in general, you get back a pointer to the (potentially distributed) h2oFrame.

So, it seems like when you manipulate an H2O dataframe, even using syntax like you would with a normal df, like  `filtered <- my_df[bleh > 3, ]`, that it is actually doing this on the h2o instance. You send the command to the h2o server.


https://www.h2o.ai/blog/h2o-architecture/

# 4. Running An Example

```{r}
library(datasets)
library(h2o)
# 
# h2o.init() #Then check it in the browser
# # by default only uses 2 cores
# #h2o.init(nthreads = -1) will use all cores
# 
# data(iris)
# 
# 
# iris.hex <- as.h2o(iris)  
# h2o.ls()
# 
# h2o.describe(iris.hex)
# #Describe the output
# h2o.hist(iris.hex$Sepal.Length)
# 
# iris.hex$bleh <- iris.hex$Sepal.Length + 2
# #unary and binary operations are supported by h2o data frames. 
# #Why doesn't the "bleh" column show up in Flow's view of the iris dataframe... but it does create a new dataframe with a random name which has this new column? wtf....
# 
# r_df <- as.data.frame(iris.hex)
# 
# 
# iris.hex$bleh <- NULL
# 
# h2o.levels(iris.hex$Species)
# # Note, if you get the error "Error in chk.H2OFrame(x) : must be an H2OFrame" - check that you spelled the column name correctly, case sensitive and all
# 
# 
# # get a nice GLM dataset make it easier to explain
# 
# splits <- h2o.splitFrame(data = iris.hex, 
#                          ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
#                          seed = 198) 
# 
# train <- splits[[1]]
# valid <- splits[[2]]
# test <- splits[[3]]
# 
# #seed changes the ratio. ratio seems to be not exact thing.
# #h2o uses approximate splitting
# 
# rf <- h2o.randomForest(x = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width"), 
#                     y = c("Species"), 
#                     training_frame = train,
#                     model_id = "our.rf")
# 
# rf_perf1 <- h2o.performance(model = rf,
#                             newdata = test)


#keep going through: https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R
# read this: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html

```

# 5. Conclusion 

Link the final code here

https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R