---
title: ISLR Notes
author: Orry Messer
date: '2020-04-06'
slug: islr-notes
draft: false
categories: []
tags: [statistical-learning]
keywords:
  - statistical-learning
thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/welcome-to-tranquilpeak/city-750.jpg
thumbnailImagePosition: "top"
---

Just a place for me to keep my notes of the [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) book.

This is by no means a comprehensive summary; just a list of points I wish to remember.

<!--more-->

Just a place for me to keep my notes of the [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) book.

This is by no means a comprehensive summary; just a list of points I wish to remember.

# Chapter 2: Statistical Learning

Least squares is a popular method of doing regression, but one of many methods.

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the _reducible error_ and the _irreducible error_.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586180520/islr%20notes/red_ir_ibsdoc.png)

We can improve the reducible error by using an more appropriate statistical technique. We can never improve the irreducible error, as it is introduced by the $\epsilon$ term in: $Y = f(x) + \epsilon$, where $Y$ is the true output. 

There is no free lunch in statistics: no one method dominates all others over all possible data sets. 

## Inference vs Prediction

Inference: to do with how the various predictors affect the output. For example, how does TV advertising spend affect sales, how does radio advertising affect, how does newspaper advertising, how does some combination of these affect sales?

Prediction: Predict the output, like, at a given level of advertising spends, what will the sales be?

Different models better at different things. If just interested in prediction, then a very complicated black-box model is fine. If interested in inference, simpler and more interpretable may be better.

### Accuracy vs Interpretability

Lasso less flexible, more interpretable: sets some params to 0.

Generalized additive models (GAMs) extend the linear model to allow for certain non-linear relationships. GAMs are more flexible than linear regression. They are also less interpretable than linear regression.

Fully non-linear methods such as bagging, boosting, and support vector machines bagging boosting with non-linear kernels, discussed in Chapters 8 and 9, are highly flexible approaches that are harder to interpret.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182241/islr%20notes/flex_zposfv.png)

## Parametric vs Non-Parametric

Statistical learning methods can be characterized as either parametric or non-parametric.

### Parametric 

2 steps:

- Select a functional form (like $y = mx + c$)
- fit / train the model.

pro: assuming functional form, it may be easier (than non-parametric case) to find params.
con: choosing bad functional form is... bad.

### Non-Parametric 

No explict assumptions about functional form.

pro: avoid danger of choosing a bad functional form
con: since non-parametric do not reduce the problem to finding a fixed number of parameters, lots more training data needed.

Thin-plate spline is an example of non-parametric. Set a "smoothness" parameter.

### Train vs Test MSE

When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.

Cross-validation is a method for estimating test MSE using the training data.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182404/islr%20notes/trate_pkpnaf.png)

### Bias Variance Trade-off

Expected (since, expected over value of $\hat{f}(x_0)$ over many number of training sets used to estimate $f$) _test_ MSE of a given $x_0$ can be broken down into:

- The variance of $\hat{f}(x_0)$
- Square bias of $\hat{f}(x_0)$
- The variance of the $\epsilon$

The last one, corresponds to _irreducible_ error. We can control the other two somewhat by changing our choice of model.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182732/islr%20notes/bivar_hvi5il.png)

In this context, _variance_ refers to how much $\hat{f}$ would change if we used a different training set (Overfitting).
  _Bias_ refers to the error we introduce by approximating a (possibly very complicated) real-life problem with a simpler mathemetical model (Underfitting). 

Generally, more flexible methods result in less bias. More flexible statistical methods have higher variance.

## Bayes Classifier

Error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. Obviously we don't know the most likely class!

The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate.

The Bayes error rate is analogous to the irreducible error, discussed earlier.

## KNN

KNN classifier identifies the K points in the training data that are closest to $x_0$. Estimates class probability as proportion of these classes in the K points.

# Chapter 3: Linear Regression

New terms in this chapter:

- Interaction Effect
- Residual
- RSS
- population regression line
- least squares line
- biased / unbiased estimators
- standard error
- RSE
- confidence interval
- hypothesis test
- t-statistic
- $p$-value
- $R^2$
- Total Sum of Squares
- correlation
- $F$ statistic

This chapter reviews ideas underlying linear regression, as well as least squares approach that is commonly used to fit this model.

## Simple Linear Regression

<u>Simple</u> linear regression refers to predicting $Y$ from a <u>single</u> predictor, $X$.

Fit line that is closest to datapoints. most common measure of closeness involved minimizing least squares criterion.

The _residual_ is the difference between the $i$th observed response value and the $i$th response value predicted by the model: $e_i = y_i-\hat{y}_i$

The _residual sum of squares_:
RSS $= e_1^2 + e_2^2 + ... + e_n^2$

The least squares approach chooses $\hat{\beta}_1$  and $\hat{\beta}_0$ to minimize the RSS.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586345760/islr%20notes/mion_ylow1m.png)

The above define the _least squares coefficient estimates_ for simple linear regression.

### Assessing the Accuracy of Coefficient Estimates 

The model defined by the _population regression line_ is the <u>best</u> linear approximation to the relationship between $X$ and $Y$. It is given by: $Y = \beta_0 + \beta_1 + \epsilon$.

The least squares coefficients define the _least squares line_.

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346036/islr%20notes/pop_sq_mlkc0z.png)

#### Biased vs Unbiased Estimators

Unbiased estimator does not systematically over or under estimate the true parameter. The sample mean is an unbiased estimator: expect the sample mean to equal popln mean on average. 

Similar to how we try to estimate the population mean using the sample mean, we try use the least squares coefficients to estimate the coefficients of the population least squares line. Apt analogy, as $\hat{\beta_0}$ and $\hat{\beta_1}$ are _unbiased_ estimators, like the sample mean.

#### Standard Error

Continuing with the example of the sample mean, we'd like to be able to comment on the accuracy of the sample mean as an estimate of the population mean. That's where _standard error_ comes in, $SE(\hat{\mu})$.

$SE(\hat{\mu})^2$ = $Var(\hat{\mu})$ = $\frac{\sigma^2}{n}$

where $\sigma$ is the standard deviation of each of the realizations of $y_i$ (remember, you have this, as it's in the labeled training data). So, the standard error is a ratio of 2 quantities: the bigger the standard deviation, the bigger the standard error. The more observations you have, $n$, the smaller the standard error. Makes sense, since if the data vary wildly, you need lots of observations to be sure of it. Similarly, if the data is super clustered together, then you probably just need a few obersvations. Of course, the more observations you have, the better (reflected by the denominator of the Standard Error). 

The formula for the standard error holds provided that none of the observations are correlated.

We can also look at the standard errors of $\hat{\beta_0}$ and $\hat{\beta_1}$:

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346735/islr%20notes/se_b1_seB2_serjac.png)

Here, $\sigma^2 = Var(\epsilon)$

For these formulas to be strictly valid, we need to assume that the errors $\epsilon_i$ for each observation are uncorrelated with common variance.

Notice in the formula that $SE(\hat{\beta_1})$ is smaller when the $x_i$ are more spread out: intuitively we have more leverage
to estimate a slope when this is the case.

We also see that $SE(\hat{β}_0)$ would be the same as $SE(\hat{μ})$ if $\bar{x}$ were zero (in which case $\hat{β}_0$ would be equal to $\bar{y}$). 

In the above formulae, we needed $\sigma^2$ which was equal to $Var(\epsilon)$. In general, this quantity is not known, but it can be estimated from the data. The estimate for this is the _residual standard error_ 

$RSE = \sqrt{RSS / (n - 2)}$ 

Standard errors can be used to compute _confidence intervals_ - so, you can say with 95% confidence that $\hat{β}_0$ and $\hat{β}_1$ lie within a certain range of values.

Standard errors can also be used to perform _hypothesis tests_. The most common hypothesis test involves checking whether there is a relationship between $X$ and $Y$. Mathematically, corresponding to:

$H_0 : \beta_1 = 0$

$H_1 : \beta_1 \neq 0$

Note, the hypothesis test is about $\beta_1$, but we'll have to use $\hat{\beta_1}$.
To test this hypothesis, we need to test whether $\hat{\beta}_1$ is far away enough from zero, to conclude that $\beta_1$ is non-zero. But how far away is far enough? This depends on the accuracy of $\hat{\beta}_1$, of course. And we were able to comment about its accuracy using $SE(\hat{\beta}_1)$. If we are confident in our estimate of $\hat{\beta}_1$, in other words, $SE(\hat{\beta}_1)$ is small, then even relatively small values of $\hat{\beta}_1$ can lead us to conclude that there is a relationship between $X$ and $Y$. In contrast, if we're not that confident in the accuracy of $\hat{\beta}_1$, ie $SE(\hat{\beta}_1)$ is large, then we'll need some pretty large values of $\hat{\beta}_1$ to convince us that there is a relationship between $X$ and $Y$.

In practice, we use _t-statistic_.

$t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}$


which measures the number of standard deviations away that $\hat{\beta_1}$ is from $0$. Look at it like this - the t-statistic is the <u>ratio</u> of the distance of the $\hat{\beta_1}$ parameter to its hypothesized value ($0$ in this case), to its standard error.
The value of the t-statistic will lie somewhere on the t-distribution (with $n-2$ degrees of freedom). We want to compute the probability of observing this $t$ value, under the assumption of $\beta_1 = 0$. What is probability of having observed this value of $t$? We call this the _p-value_.  Small p-value means it's super unlikely to have observed it, so we reject $H_0$

### Assessing the Accuracy of The Model

Once we have rejected $H_0$ above, we need to comment on the accuracy of our model.

The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the
$R^2$ statistic.

#### RSE

Recall, we've already met the $RSE = \sqrt{RSS / (n - 2)}$. We used it to estimate the value of the standard deviation of the residuals (which were Gaussian, mean 0, s.d. = ?).


In the advertising example, RSE was 3.26 (measured in thousands of units). So any prediction could be off by about 3260 units (if it's within 1 standard deviation away from the line).


RSE is considered a measure of the lack of fit of the model to the data - is the choice of model, namely $Y = \beta_0 + \beta_1 + \epsilon$ a good one?

#### $R^2$

RSE is measured in units of Y, $R^2$ takes the form of a proportion. $R^2$ is the proportion of the variance explained by the model.

$R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}$

$0 \leq R^2 \leq 1$

TSS measures the total variance in the response $Y$:

$TSS = \sum(y_i - \bar{y})^2$

So, it's the variance in the <u>actual, observed</u> data.

RSS measures the variability that is left unexplained after performing the regression.

$RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2$

So, it's the variance left over from the difference between the actual $y_i$ and $\hat{y}_i$.

Ergo, $TSS - RSS$ measures the amount of variability in the response that is explained by the regression. $R^2$ measures the proportion of variability in in Y that can be explained using X.

In Table 3.2, the $R^2$ was 0.61, and so just under two-thirds of the variability in sales is explained by a linear regression on TV.

$R^2$ is a measure of the _linear_ relationship between $X$ and $Y$. _Correlation_ is also a measure of the linear relationship between $X$ and $Y$. In simple linear regression, $r^2 = R^2$. In mutliple linear regression, though, $R^2$ is different: The concept of correlation is between 2 variables, namely $X$ and $Y$ in this case. When we have $X_1, X_2, ... X_n$, the concept of correlation does not extend to these predictors and $Y$, as its no longer between 2 variables (it's between $n + 1$). But $R^2$ still plays this same role in multiple correlation setting.

### Additional Info on lm() from R:

https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output

## Multiple Linear Regression

More than one predictor.

$Y = \beta_0 + \beta_1x_1 + \beta_2x2 + ... + \beta_px_p + \epsilon$

### Estimating Regression Coefficients

Can use least squares (like in simple linear regression, but requires matrix algebra).

NB: simple and multiple regression coefficients can be quite different:

![Multiple Regression Coefficients](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427329/islr%20notes/mult_wefkis.png)

![TV](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/TV_yuqrlo.png)
![Newspaper](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/newspaper_idieyi.png)
![Radio](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427524/islr%20notes/radio_fyczo7.png)

Look at coefficient of Newspaper in multiple (-0.001) and simple (0.055) case. Difference stems from the fact that in the simple linear regression case, the slope term represents the average effect of a \\\$1000 increase in newspaper advertising, ignoring other predictors. In multiple regression setting, the coefficient for newspaper represents the average effect of increasing newspaper spending by \\\$1000 while holding TV and radio fixed.

How can make sense for multiple regression to suggest no relationship between sales and newspaper, where in simple regression case it does?

Look at the correlation matrix:

![](https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427957/islr%20notes/correlation_j2lazp.png)

Notice that the correlation between radio and newspaper is 0.35. Reveals tendency to spend more on newspaper in markets where more is spent on radio advertising.

If multiple regression is correct, then newspaper has no _direct_ impact on sales, but radio does. In markets where more spent on radio, sales will be higher AND, as correlation matrix shows, newspaper spend will also be higher. Therefore, in simple linear regression, only looking at sales vs newspaper, we observe that higher newspaper spend tends to be associated with higher sales. Even though it doesn't actually affect sales!!! Newspaper "takes the credit" in the absence of radio on sales.









