---
title: ISLR Notes
author: Orry Messer
date: '2020-04-06'
slug: islr-notes
draft: false
categories: []
tags: [statistical-learning]
keywords:
  - statistical-learning
thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/welcome-to-tranquilpeak/city-750.jpg
thumbnailImagePosition: "top"
---



<p>Just a place for me to keep my notes of the <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">Introduction to Statistical Learning</a> book.</p>
<p>This is by no means a comprehensive summary; just a list of points I wish to remember.</p>
<!--more-->
<p>Just a place for me to keep my notes of the <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">Introduction to Statistical Learning</a> book.</p>
<p>This is by no means a comprehensive summary; just a list of points I wish to remember.</p>
<div id="chapter-2-statistical-learning" class="section level1">
<h1>Chapter 2: Statistical Learning</h1>
<p>Least squares is a popular method of doing regression, but one of many methods.</p>
<p>The accuracy of <span class="math inline">\(\hat{Y}\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two quantities, which we will call the <em>reducible error</em> and the <em>irreducible error</em>.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586180520/islr%20notes/red_ir_ibsdoc.png" />

</div>
<p>We can improve the reducible error by using an more appropriate statistical technique. We can never improve the irreducible error, as it is introduced by the <span class="math inline">\(\epsilon\)</span> term in: <span class="math inline">\(Y = f(x) + \epsilon\)</span>, where <span class="math inline">\(Y\)</span> is the true output.</p>
<p>There is no free lunch in statistics: no one method dominates all others over all possible data sets.</p>
<div id="inference-vs-prediction" class="section level2">
<h2>Inference vs Prediction</h2>
<p>Inference: to do with how the various predictors affect the output. For example, how does TV advertising spend affect sales, how does radio advertising affect, how does newspaper advertising, how does some combination of these affect sales?</p>
<p>Prediction: Predict the output, like, at a given level of advertising spends, what will the sales be?</p>
<p>Different models better at different things. If just interested in prediction, then a very complicated black-box model is fine. If interested in inference, simpler and more interpretable may be better.</p>
<div id="accuracy-vs-interpretability" class="section level3">
<h3>Accuracy vs Interpretability</h3>
<p>Lasso less flexible, more interpretable: sets some params to 0.</p>
<p>Generalized additive models (GAMs) extend the linear model to allow for certain non-linear relationships. GAMs are more flexible than linear regression. They are also less interpretable than linear regression.</p>
<p>Fully non-linear methods such as bagging, boosting, and support vector machines bagging boosting with non-linear kernels, discussed in Chapters 8 and 9, are highly flexible approaches that are harder to interpret.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182241/islr%20notes/flex_zposfv.png" />

</div>
</div>
</div>
<div id="parametric-vs-non-parametric" class="section level2">
<h2>Parametric vs Non-Parametric</h2>
<p>Statistical learning methods can be characterized as either parametric or non-parametric.</p>
<div id="parametric" class="section level3">
<h3>Parametric</h3>
<p>2 steps:</p>
<ul>
<li>Select a functional form (like <span class="math inline">\(y = mx + c\)</span>)</li>
<li>fit / train the model.</li>
</ul>
<p>pro: assuming functional form, it may be easier (than non-parametric case) to find params. con: choosing bad functional form is… bad.</p>
</div>
<div id="non-parametric" class="section level3">
<h3>Non-Parametric</h3>
<p>No explict assumptions about functional form.</p>
<p>pro: avoid danger of choosing a bad functional form con: since non-parametric do not reduce the problem to finding a fixed number of parameters, lots more training data needed.</p>
<p>Thin-plate spline is an example of non-parametric. Set a “smoothness” parameter.</p>
</div>
<div id="train-vs-test-mse" class="section level3">
<h3>Train vs Test MSE</h3>
<p>When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.</p>
<p>Cross-validation is a method for estimating test MSE using the training data.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182404/islr%20notes/trate_pkpnaf.png" />

</div>
</div>
<div id="bias-variance-trade-off" class="section level3">
<h3>Bias Variance Trade-off</h3>
<p>Expected (since, expected over value of <span class="math inline">\(\hat{f}(x_0)\)</span> over many number of training sets used to estimate <span class="math inline">\(f\)</span>) <em>test</em> MSE of a given <span class="math inline">\(x_0\)</span> can be broken down into:</p>
<ul>
<li>The variance of <span class="math inline">\(\hat{f}(x_0)\)</span></li>
<li>Square bias of <span class="math inline">\(\hat{f}(x_0)\)</span></li>
<li>The variance of the <span class="math inline">\(\epsilon\)</span></li>
</ul>
<p>The last one, corresponds to <em>irreducible</em> error. We can control the other two somewhat by changing our choice of model.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182732/islr%20notes/bivar_hvi5il.png" />

</div>
<p>In this context, <em>variance</em> refers to how much <span class="math inline">\(\hat{f}\)</span> would change if we used a different training set (Overfitting). <em>Bias</em> refers to the error we introduce by approximating a (possibly very complicated) real-life problem with a simpler mathemetical model (Underfitting).</p>
<p>Generally, more flexible methods result in less bias. More flexible statistical methods have higher variance.</p>
</div>
</div>
<div id="bayes-classifier" class="section level2">
<h2>Bayes Classifier</h2>
<p>Error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. Obviously we don’t know the most likely class!</p>
<p>The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate.</p>
<p>The Bayes error rate is analogous to the irreducible error, discussed earlier.</p>
</div>
<div id="knn" class="section level2">
<h2>KNN</h2>
<p>KNN classifier identifies the K points in the training data that are closest to <span class="math inline">\(x_0\)</span>. Estimates class probability as proportion of these classes in the K points.</p>
</div>
</div>
<div id="chapter-3-linear-regression" class="section level1">
<h1>Chapter 3: Linear Regression</h1>
<p>New terms in this chapter:</p>
<ul>
<li>Interaction Effect</li>
<li>Residual</li>
<li>RSS</li>
<li>population regression line</li>
<li>least squares line</li>
<li>biased / unbiased estimators</li>
<li>standard error</li>
<li>RSE</li>
<li>confidence interval</li>
<li>hypothesis test</li>
<li>t-statistic</li>
<li><span class="math inline">\(p\)</span>-value</li>
<li><span class="math inline">\(R^2\)</span></li>
<li>Total Sum of Squares</li>
<li>correlation</li>
<li><span class="math inline">\(F\)</span> statistic</li>
</ul>
<p>This chapter reviews ideas underlying linear regression, as well as least squares approach that is commonly used to fit this model.</p>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<p><u>Simple</u> linear regression refers to predicting <span class="math inline">\(Y\)</span> from a <u>single</u> predictor, <span class="math inline">\(X\)</span>.</p>
<p>Fit line that is closest to datapoints. most common measure of closeness involved minimizing least squares criterion.</p>
<p>The <em>residual</em> is the difference between the <span class="math inline">\(i\)</span>th observed response value and the <span class="math inline">\(i\)</span>th response value predicted by the model: <span class="math inline">\(e_i = y_i-\hat{y}_i\)</span></p>
<p>The <em>residual sum of squares</em>: RSS <span class="math inline">\(= e_1^2 + e_2^2 + ... + e_n^2\)</span></p>
<p>The least squares approach chooses <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> to minimize the RSS.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586345760/islr%20notes/mion_ylow1m.png" />

</div>
<p>The above define the <em>least squares coefficient estimates</em> for simple linear regression.</p>
<div id="assessing-the-accuracy-of-coefficient-estimates" class="section level3">
<h3>Assessing the Accuracy of Coefficient Estimates</h3>
<p>The model defined by the <em>population regression line</em> is the <u>best</u> linear approximation to the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is given by: <span class="math inline">\(Y = \beta_0 + \beta_1 + \epsilon\)</span>.</p>
<p>The least squares coefficients define the <em>least squares line</em>.</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346036/islr%20notes/pop_sq_mlkc0z.png" />

</div>
<div id="biased-vs-unbiased-estimators" class="section level4">
<h4>Biased vs Unbiased Estimators</h4>
<p>Unbiased estimator does not systematically over or under estimate the true parameter. The sample mean is an unbiased estimator: expect the sample mean to equal popln mean on average.</p>
<p>Similar to how we try to estimate the population mean using the sample mean, we try use the least squares coefficients to estimate the coefficients of the population least squares line. Apt analogy, as <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are <em>unbiased</em> estimators, like the sample mean.</p>
</div>
<div id="standard-error" class="section level4">
<h4>Standard Error</h4>
<p>Continuing with the example of the sample mean, we’d like to be able to comment on the accuracy of the sample mean as an estimate of the population mean. That’s where <em>standard error</em> comes in, <span class="math inline">\(SE(\hat{\mu})\)</span>.</p>
<p><span class="math inline">\(SE(\hat{\mu})^2\)</span> = <span class="math inline">\(Var(\hat{\mu})\)</span> = <span class="math inline">\(\frac{\sigma^2}{n}\)</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the standard deviation of each of the realizations of <span class="math inline">\(y_i\)</span> (remember, you have this, as it’s in the labeled training data). So, the standard error is a ratio of 2 quantities: the bigger the standard deviation, the bigger the standard error. The more observations you have, <span class="math inline">\(n\)</span>, the smaller the standard error. Makes sense, since if the data vary wildly, you need lots of observations to be sure of it. Similarly, if the data is super clustered together, then you probably just need a few obersvations. Of course, the more observations you have, the better (reflected by the denominator of the Standard Error).</p>
<p>The formula for the standard error holds provided that none of the observations are correlated.</p>
<p>We can also look at the standard errors of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346735/islr%20notes/se_b1_seB2_serjac.png" />

</div>
<p>Here, <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p>
<p>For these formulas to be strictly valid, we need to assume that the errors <span class="math inline">\(\epsilon_i\)</span> for each observation are uncorrelated with common variance.</p>
<p>Notice in the formula that <span class="math inline">\(SE(\hat{\beta_1})\)</span> is smaller when the <span class="math inline">\(x_i\)</span> are more spread out: intuitively we have more leverage to estimate a slope when this is the case.</p>
<p>We also see that <span class="math inline">\(SE(\hat{β}_0)\)</span> would be the same as <span class="math inline">\(SE(\hat{μ})\)</span> if <span class="math inline">\(\bar{x}\)</span> were zero (in which case <span class="math inline">\(\hat{β}_0\)</span> would be equal to <span class="math inline">\(\bar{y}\)</span>).</p>
<p>In the above formulae, we needed <span class="math inline">\(\sigma^2\)</span> which was equal to <span class="math inline">\(Var(\epsilon)\)</span>. In general, this quantity is not known, but it can be estimated from the data. The estimate for this is the <em>residual standard error</em></p>
<p><span class="math inline">\(RSE = \sqrt{RSS / (n - 2)}\)</span></p>
<p>Standard errors can be used to compute <em>confidence intervals</em> - so, you can say with 95% confidence that <span class="math inline">\(\hat{β}_0\)</span> and <span class="math inline">\(\hat{β}_1\)</span> lie within a certain range of values.</p>
<p>Standard errors can also be used to perform <em>hypothesis tests</em>. The most common hypothesis test involves checking whether there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Mathematically, corresponding to:</p>
<p><span class="math inline">\(H_0 : \beta_1 = 0\)</span></p>
<p><span class="math inline">\(H_1 : \beta_1 \neq 0\)</span></p>
<p>Note, the hypothesis test is about <span class="math inline">\(\beta_1\)</span>, but we’ll have to use <span class="math inline">\(\hat{\beta_1}\)</span>. To test this hypothesis, we need to test whether <span class="math inline">\(\hat{\beta}_1\)</span> is far away enough from zero, to conclude that <span class="math inline">\(\beta_1\)</span> is non-zero. But how far away is far enough? This depends on the accuracy of <span class="math inline">\(\hat{\beta}_1\)</span>, of course. And we were able to comment about its accuracy using <span class="math inline">\(SE(\hat{\beta}_1)\)</span>. If we are confident in our estimate of <span class="math inline">\(\hat{\beta}_1\)</span>, in other words, <span class="math inline">\(SE(\hat{\beta}_1)\)</span> is small, then even relatively small values of <span class="math inline">\(\hat{\beta}_1\)</span> can lead us to conclude that there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In contrast, if we’re not that confident in the accuracy of <span class="math inline">\(\hat{\beta}_1\)</span>, ie <span class="math inline">\(SE(\hat{\beta}_1)\)</span> is large, then we’ll need some pretty large values of <span class="math inline">\(\hat{\beta}_1\)</span> to convince us that there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>In practice, we use <em>t-statistic</em>.</p>
<p><span class="math inline">\(t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}\)</span></p>
<p>which measures the number of standard deviations away that <span class="math inline">\(\hat{\beta_1}\)</span> is from <span class="math inline">\(0\)</span>. Look at it like this - the t-statistic is the <u>ratio</u> of the distance of the <span class="math inline">\(\hat{\beta_1}\)</span> parameter to its hypothesized value (<span class="math inline">\(0\)</span> in this case), to its standard error. The value of the t-statistic will lie somewhere on the t-distribution (with <span class="math inline">\(n-2\)</span> degrees of freedom). We want to compute the probability of observing this <span class="math inline">\(t\)</span> value, under the assumption of <span class="math inline">\(\beta_1 = 0\)</span>. What is probability of having observed this value of <span class="math inline">\(t\)</span>? We call this the <em>p-value</em>. Small p-value means it’s super unlikely to have observed it, so we reject <span class="math inline">\(H_0\)</span></p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3">
<h3>Assessing the Accuracy of The Model</h3>
<p>Once we have rejected <span class="math inline">\(H_0\)</span> above, we need to comment on the accuracy of our model.</p>
<p>The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the <span class="math inline">\(R^2\)</span> statistic.</p>
<div id="rse" class="section level4">
<h4>RSE</h4>
<p>Recall, we’ve already met the <span class="math inline">\(RSE = \sqrt{RSS / (n - 2)}\)</span>. We used it to estimate the value of the standard deviation of the residuals (which were Gaussian, mean 0, s.d. = ?).</p>
<p>In the advertising example, RSE was 3.26 (measured in thousands of units). So any prediction could be off by about 3260 units (if it’s within 1 standard deviation away from the line).</p>
<p>RSE is considered a measure of the lack of fit of the model to the data - is the choice of model, namely <span class="math inline">\(Y = \beta_0 + \beta_1 + \epsilon\)</span> a good one?</p>
</div>
<div id="r2" class="section level4">
<h4><span class="math inline">\(R^2\)</span></h4>
<p>RSE is measured in units of Y, <span class="math inline">\(R^2\)</span> takes the form of a proportion. <span class="math inline">\(R^2\)</span> is the proportion of the variance explained by the model.</p>
<p><span class="math inline">\(R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}\)</span></p>
<p><span class="math inline">\(0 \leq R^2 \leq 1\)</span></p>
<p>TSS measures the total variance in the response <span class="math inline">\(Y\)</span>:</p>
<p><span class="math inline">\(TSS = \sum(y_i - \bar{y})^2\)</span></p>
<p>So, it’s the variance in the <u>actual, observed</u> data.</p>
<p>RSS measures the variability that is left unexplained after performing the regression.</p>
<p><span class="math inline">\(RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2\)</span></p>
<p>So, it’s the variance left over from the difference between the actual <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y}_i\)</span>.</p>
<p>Ergo, <span class="math inline">\(TSS - RSS\)</span> measures the amount of variability in the response that is explained by the regression. <span class="math inline">\(R^2\)</span> measures the proportion of variability in in Y that can be explained using X.</p>
<p>In Table 3.2, the <span class="math inline">\(R^2\)</span> was 0.61, and so just under two-thirds of the variability in sales is explained by a linear regression on TV.</p>
<p><span class="math inline">\(R^2\)</span> is a measure of the <em>linear</em> relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. <em>Correlation</em> is also a measure of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In simple linear regression, <span class="math inline">\(r^2 = R^2\)</span>. In mutliple linear regression, though, <span class="math inline">\(R^2\)</span> is different: The concept of correlation is between 2 variables, namely <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in this case. When we have <span class="math inline">\(X_1, X_2, ... X_n\)</span>, the concept of correlation does not extend to these predictors and <span class="math inline">\(Y\)</span>, as its no longer between 2 variables (it’s between <span class="math inline">\(n + 1\)</span>). But <span class="math inline">\(R^2\)</span> still plays this same role in multiple correlation setting.</p>
</div>
</div>
<div id="additional-info-on-lm-from-r" class="section level3">
<h3>Additional Info on lm() from R:</h3>
<p><a href="https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output" class="uri">https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output</a></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple Linear Regression</h2>
<p>More than one predictor.</p>
<p><span class="math inline">\(Y = \beta_0 + \beta_1x_1 + \beta_2x2 + ... + \beta_px_p + \epsilon\)</span></p>
<div id="estimating-regression-coefficients" class="section level3">
<h3>Estimating Regression Coefficients</h3>
<p>Can use least squares (like in simple linear regression, but requires matrix algebra).</p>
<p>NB: simple and multiple regression coefficients can be quite different:</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427329/islr%20notes/mult_wefkis.png" alt="Multiple Regression Coefficients" />
<p class="caption">Multiple Regression Coefficients</p>
</div>
<p><img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/TV_yuqrlo.png" alt="TV" /> <img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/newspaper_idieyi.png" alt="Newspaper" /> <img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427524/islr%20notes/radio_fyczo7.png" alt="Radio" /></p>
<p>Look at coefficient of Newspaper in multiple (-0.001) and simple (0.055) case. Difference stems from the fact that in the simple linear regression case, the slope term represents the average effect of a \$1000 increase in newspaper advertising, ignoring other predictors. In multiple regression setting, the coefficient for newspaper represents the average effect of increasing newspaper spending by \$1000 while holding TV and radio fixed.</p>
<p>How can make sense for multiple regression to suggest no relationship between sales and newspaper, where in simple regression case it does?</p>
<p>Look at the correlation matrix:</p>
<div class="figure">
<img src="https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427957/islr%20notes/correlation_j2lazp.png" />

</div>
<p>Notice that the correlation between radio and newspaper is 0.35. Reveals tendency to spend more on newspaper in markets where more is spent on radio advertising.</p>
<p>If multiple regression is correct, then newspaper has no <em>direct</em> impact on sales, but radio does. In markets where more spent on radio, sales will be higher AND, as correlation matrix shows, newspaper spend will also be higher. Therefore, in simple linear regression, only looking at sales vs newspaper, we observe that higher newspaper spend tends to be associated with higher sales. Even though it doesn’t actually affect sales!!! Newspaper “takes the credit” in the absence of radio on sales.</p>
</div>
</div>
</div>
