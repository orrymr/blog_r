<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>orrymr.com</title>
    <link>/</link>
    <description>Recent content on orrymr.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 31 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Leadership Lab - 6 Articles on Leadership</title>
      <link>/2020/12/leadership-lab-6-articles-on-leadership/</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/leadership-lab-6-articles-on-leadership/</guid>
      <description>

&lt;h1 id=&#34;the-stage-between-stages&#34;&gt;The Stage Between Stages&lt;/h1&gt;

&lt;p&gt;Today, the 31st of December 2020, is my last official day as an employee of Discovery. I met some wonderful people, and learned a great deal. I expanded my techical repertoire: I learned more about R, statistics, machine learning and data science/analysis in general. I was fortunate to also be part of something called the Leadership Lab: once a week a group of about 15-20 people from across the group sat down with a top exec and discussed articles broadly encompassing the topic of leadership.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been meaning to document these articles, and my corresponding notes, as a blog post for some time. The liminal stage between stages I now find myself in allows for just that.&lt;/p&gt;

&lt;p&gt;The lab was divided into 6 weeks: 1 article per week. Unfortunately, I was unable to attend 2 sessions. Regardless, the experience was invaluable.&lt;/p&gt;

&lt;h1 id=&#34;week-1-in-a-difficult-conversation-listen-more-than-you-talk&#34;&gt;Week 1 - In a Difficult Conversation, Listen More Than You Talk&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://hbr.org/2017/02/in-a-difficult-conversation-listen-more-than-you-talk&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;3 main points in this article. When in conversation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Be Present (Really)&lt;/li&gt;
&lt;li&gt;Listen More&lt;/li&gt;
&lt;li&gt;Be Open&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The article discusses how each point can help avoid a breakdown in communication. I believe that these points all relate to the same idea: truly giving your time and focus to the matter being discussed, as well as the discussant(s). Quote the article: &amp;lsquo;As someone is speaking, notice: Are you already thinking about your rebuttal? Are you responding with a “yes,” followed by an immediate “but”? Or have you already interrupted? Be open to another person’s perspective.&amp;rsquo;&lt;/p&gt;

&lt;p&gt;An important point the article brings up is psychological safety, which is a necessary ingredient for successful teams. The article, as well as the discussion in the lab centered around work teams. Of course, there are other areas in life (family, friends, extended community) where one can apply these ideas. &lt;em&gt;Where&lt;/em&gt; are you a leader?&lt;/p&gt;

&lt;h1 id=&#34;week-2-how-to-boost-your-and-others-emotional-intelligence&#34;&gt;Week 2 - How to Boost Your (and Others’) Emotional Intelligence&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://hbr.org/2017/01/how-to-boost-your-and-others-emotional-intelligence?utm_campaign=hbr&amp;amp;utm_source=facebook&amp;amp;utm_medium=social&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This article discusses how EQ (with effort) is trainable. It mentions studies which have found EQ to be predictive of employability, job performance, leadership potential, etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;5 critical steps for developing EQ:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Turn self-deception into self-awareness&lt;/li&gt;
&lt;li&gt;Turn self-focus into other-focus&lt;/li&gt;
&lt;li&gt;Be more rewarding to deal with&lt;/li&gt;
&lt;li&gt;Control your temper tantrums&lt;/li&gt;
&lt;li&gt;Display humility, even if it’s fake&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these points have a common theme of awareness. In particular, self-awareness. This was probably my favourite article of the 6.&lt;/p&gt;

&lt;h2 id=&#34;other-notes-from-week-2-s-discussion&#34;&gt;Other notes from week 2&amp;rsquo;s discussion&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Gently, directly, firmly: how to deal with difficult discussions, like confrontations / someone not doing well / inconsistent work&lt;/li&gt;
&lt;li&gt;Make sure goals are clear. Make sure timeframes are clear&lt;/li&gt;
&lt;li&gt;Meaning -&amp;gt; what gives you meaning at work? Lack of meaning drains you of energy&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Authentically &lt;/u&gt; calling out good work: make this a habit. Don&amp;rsquo;t underestimate the meaning / power of this. But don&amp;rsquo;t be disingenuous.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of the above points could easily be turned into its own blog post. For now, I&amp;rsquo;m just satisfied to have them here; to not lose them.&lt;/p&gt;

&lt;h1 id=&#34;week-3-authentic-leadership-rediscovered&#34;&gt;Week 3 - Authentic Leadership Rediscovered&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://hbswk.hbs.edu/item/authentic-leadership-rediscovered&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Authentic leadership is built on your character, not your style&lt;/li&gt;
&lt;li&gt;Authentic leaders are real and genuine&lt;/li&gt;
&lt;li&gt;Authentic leaders are constantly growing&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Authentic leaders match their behavior to their context&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Authentic leaders are not perfect, nor do they try to be&lt;/li&gt;
&lt;li&gt;Authentic leaders are sensitive to the needs of others&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;week-4-how-managers-drive-results-and-employee-engagement-at-the-same-time&#34;&gt;Week 4 - How Managers Drive Results and Employee Engagement at the Same Time&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://hbr.org/2017/06/how-managers-drive-results-and-employee-engagement-at-the-same-time&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;week-5-how-humble-leadership-really-works&#34;&gt;Week 5 - How Humble Leadership Really Works&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://hbr.org/2018/04/how-humble-leadership-really-works&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;week-6-7-things-that-make-great-bosses-unforgettable&#34;&gt;Week 6 - 7 Things That Make Great Bosses Unforgettable&lt;/h1&gt;

&lt;p&gt;This article can be found &lt;a href=&#34;https://www.forbes.com/sites/travisbradberry/2015/10/15/7-things-that-make-great-bosses-unforgettable/?sh=1e2b98af2354&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;finally&#34;&gt;Finally&amp;hellip;&lt;/h1&gt;

&lt;p&gt;These readings and discussions gave me remarkable insight into the types of qualities which result in effective leadership. The question of how to authentically cultivate these qualities I found particularly relevant.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s insightful to see that leadership can be both firm and gentle at the same time; these are characteristics which appear to be at odds with one another.&lt;/p&gt;

&lt;p&gt;The topic of self-awareness and matching behaviour to context is a practical, implementable bit of advice which I&amp;rsquo;d like to carry forward with me.&lt;/p&gt;

&lt;p&gt;This type of discussion is never really complete. I guess this blog post is more of a starting point. It&amp;rsquo;s impossible to incorporate all the advice here. I don&amp;rsquo;t think that&amp;rsquo;s the point. If you can just do a little better than you would have otherwise done, then that&amp;rsquo;s a win.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An R Pipeline for XGBoost Part I</title>
      <link>/2020/10/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters-part-i/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters-part-i/</guid>
      <description>&lt;p&gt;Contents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgimpl&#34;&gt;XGBoost - An Implementation of Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loadxplor&#34;&gt;Load And Explore The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hyperprm&#34;&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trn&#34;&gt;Training The Model: Or, how I learned to stop overfitting and love the cross-validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#makpred&#34;&gt;Making Predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conc&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/&#34;&gt;XGBoost&lt;/a&gt; is an implementation of a machine learning technique known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;gradient boosting&lt;/a&gt;.
&lt;b&gt;In this blog post, we discuss what XGBoost is, and demonstrate a pipeline for working with it in R.&lt;/b&gt; We won’t go into too much theoretical detail. Rather, we’ll focus on application.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xgimpl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; XGBoost - An Implementation of Gradient Boosting&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;Gradient boosting is part of a class of machine learning techniques known as ensemble methods.&lt;/b&gt; An ensemble method leverages the output of many &lt;u&gt;weak learners&lt;/u&gt; in order to make a prediction. Typically, these weak learners are implemented as decision trees. While each individual weak learner might not get the answer right, on average, their combined answers should be pretty decent. What makes gradient boosting different from another popular ensemble method - &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random Forest&lt;/a&gt;, is that the construction of the weak learners depend on the previously constructed learners.&lt;/p&gt;
&lt;p&gt;In gradient boosting, each weak learner is chosen iteratively in a greedy manner, so as to minimize the loss function. &lt;b&gt;XGBoost is a highly optimized implementation of gradient boosting.&lt;/b&gt; The original paper describing XGBoost can be found &lt;a href=&#34;https://arxiv.org/pdf/1603.02754.pdf&#34;&gt;here&lt;/a&gt;. Although XGBoost is written in C++, it can be interfaced from R using the &lt;code&gt;xgboost&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;To install the package:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;install.packages(&#34;xgboost&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial we use the following packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(xgboost)
library(ggplot2)
library(ggthemes)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)

theme_set(theme_economist())

set.seed(1234) # For reproducibility.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loadxplor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Load And Explore The Data&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;We will use the &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Dataset&lt;/a&gt; which we get from Kaggle.&lt;/b&gt; Basically, we try predict whether a passenger survived or not (so this is a binary classification problem).&lt;/p&gt;
&lt;p&gt;Let’s load up the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Replace directoryWhichContainsTrainingData and directoryWhichContaintsTestData 
# with your path to your training and test data, respectively.
# For example mine looks like this:
directoryWhichContainsTrainingData &amp;lt;-  &amp;quot;./xg_boost_data/train.csv&amp;quot;
directoryWhichContaintsTestData &amp;lt;- &amp;quot;./xg_boost_data//test.csv&amp;quot;

titanic_train &amp;lt;- read_csv(directoryWhichContainsTrainingData)
titanic_test &amp;lt;- read_csv(directoryWhichContaintsTestData)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of brevity (and my laziness), I’ll only keep some of the features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pclass: this refers to passenger class (1st, 2nd or 3rd)&lt;/li&gt;
&lt;li&gt;Sex&lt;/li&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Embarked: Port of Embarkation - C = Cherbourg, Q = Queenstown, S = Southampton&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll use dplyr’s &lt;code&gt;select()&lt;/code&gt; to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_train &amp;lt;- titanic_train %&amp;gt;%
  select(Survived,
         Pclass,
         Sex,
         Age,
         Embarked)

titanic_test &amp;lt;- titanic_test %&amp;gt;%
  select(Pclass,
         Sex,
         Age,
         Embarked)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at our data after discarding a few features:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train, give.attr = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [891 x 5] (S3: tbl_df/tbl/data.frame)
##  $ Survived: num [1:891] 0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass  : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex     : chr [1:891] &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age     : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...
##  $ Embarked: chr [1:891] &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;b&gt;XGBoost will only take numeric data as input.&lt;/b&gt; Let’s convert our character features to factors, and one-hot encode. Since we’re one-hot encoding, we expect our matrix to be filled with lots of zeroes - in other words, we expect it to be sparse. We will use &lt;code&gt;sparse.model.matrix()&lt;/code&gt; to create a sparse matrix which will be used as input for our model. XGBoost has been written to take advantage of sparse matrices, so we want to make sure that we’re using this feature.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Unfortunately, in using R at least, sparse.model.matrix() will drop rows which contain NA’s&lt;/b&gt; if the global option &lt;code&gt;options(&#39;na.action&#39;)&lt;/code&gt; is set to &lt;code&gt;&#34;na.omit&#34;&lt;/code&gt;. So we use a fix outlined &lt;a href=&#34;https://stackoverflow.com/questions/29732720/sparse-model-matrix-loses-rows-in-r&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;previous_na_action &amp;lt;- options(&amp;#39;na.action&amp;#39;) #store the current na.action
options(na.action=&amp;#39;na.pass&amp;#39;) #change the na.action

titanic_train$Sex &amp;lt;- as.factor(titanic_train$Sex)
titanic_train$Embarked &amp;lt;- as.factor(titanic_train$Embarked)

#create the sparse matrices
titanic_train_sparse &amp;lt;- sparse.model.matrix(Survived~., data = titanic_train)[,-1] 
titanic_test_sparse &amp;lt;- sparse.model.matrix(~., data = titanic_test)[,-1] 

options(na.action=previous_na_action$na.action) #reset the na.action&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could have just used a sentinel value to replace the NA’s.&lt;/p&gt;
&lt;div id=&#34;interacting-with-the-sparse-matrix-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Interacting with the Sparse Matrix Object&lt;/h2&gt;
&lt;p&gt;The data are in the format of a &lt;strong&gt;dgCMatrix&lt;/strong&gt; class - this is the Matrix package’s implementation of sparse matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Formal class &amp;#39;dgCMatrix&amp;#39; [package &amp;quot;Matrix&amp;quot;] with 6 slots
##   ..@ i       : int [1:3080] 0 1 2 3 4 5 6 7 8 9 ...
##   ..@ p       : int [1:6] 0 891 1468 2359 2436 3080
##   ..@ Dim     : int [1:2] 891 5
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:891] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##   .. ..$ : chr [1:5] &amp;quot;Pclass&amp;quot; &amp;quot;Sexmale&amp;quot; &amp;quot;Age&amp;quot; &amp;quot;EmbarkedQ&amp;quot; ...
##   ..@ x       : num [1:3080] 3 1 3 1 3 3 1 3 3 2 ...
##   ..@ factors : list()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the dimension of the matrix directly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 891   5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The names are the features are given by &lt;code&gt;titanic_train_sparse@Dimnames[[2]]&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(titanic_train_sparse@Dimnames[[2]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Pclass&amp;quot;    &amp;quot;Sexmale&amp;quot;   &amp;quot;Age&amp;quot;       &amp;quot;EmbarkedQ&amp;quot; &amp;quot;EmbarkedS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If needed, you can convert this data (back) into a data frame, thusly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data_as_df &amp;lt;- as.data.frame(as.matrix(titanic_train_sparse))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperprm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Hyperparameters&lt;/h1&gt;
&lt;p&gt;Tuning hyperparameters is a vast topic. Without going into too much depth, I’ll outline some of the more commonly used hyperparameters:&lt;/p&gt;
&lt;p&gt;Full reference: &lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/parameter.html&#34; class=&#34;uri&#34;&gt;https://xgboost.readthedocs.io/en/latest/parameter.html&lt;/a&gt;&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-11&#34;&gt;Table 4.1: &lt;/span&gt;Parameters
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Parameter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Explanation
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
eta
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0.3 learning rate / shrinkage. Scales the contribution of each try by a factor of 0 &amp;lt; eta &amp;lt; 1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gamma
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0 minimum loss reduction needed to make another partition in a given tree. larger the value, the more conservative the tree will be (as it will need to make a bigger reduction to split) So, conservative in the sense of willingness to split.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
max_depth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 6 max depth of each tree…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
subsample
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1 (ie, no subsampling) fraction of training samples to use in each “boosting iteration”
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
colsample_bytree
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 (ie, no sampling) Fraction of columns to be used when constructing each tree. This is an idea used in RandomForests
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min_child_weight
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 This is the minimum number of instances that have to been in a node. It’s a regularization parameter So, if it’s set to 10, each leaf has to have at least 10 instances assigned to it. The higher the value, the more conservative the tree will be.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;b&gt;Note that the above hyperparameters are in the case of the weak learner being a tree.&lt;/b&gt; It is possible to have linear models as your weak learners.&lt;/p&gt;
&lt;p&gt;Let’s create the hyper-parameters list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# booster = &amp;#39;gbtree&amp;#39;: Possible to also have linear boosters as your weak learners.
params_booster &amp;lt;- list(booster = &amp;#39;gbtree&amp;#39;, eta = 1, gamma = 0, max.depth = 2, subsample = 1, colsample_bytree = 1, min_child_weight = 1, objective = &amp;quot;binary:logistic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One day, I shall write a blog post about various ways to tune your hyperparameters. But, today is not that day. If you are like me, and believe in serendipitous machine learning, then &lt;b&gt;try a random search of the hyperparameters&lt;/b&gt; (within reason, of course. Don’t set &lt;code&gt;max.depth = 9999999999&lt;/code&gt;. Or do, I’m not telling you how to live your life). You get surprisingly decent results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trn&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Training The Model: Or, how I learned to stop overfitting and love the cross-validation&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;The &lt;code&gt;xgb.train()&lt;/code&gt; and &lt;code&gt;xgboost()&lt;/code&gt; functions are used to train the boosting model&lt;/b&gt;, and both return an object of class xgb.Booster. &lt;code&gt;xgboost()&lt;/code&gt; is a simple wrapper for &lt;code&gt;xgb.train()&lt;/code&gt;. &lt;code&gt;xgb.train()&lt;/code&gt; is an advanced interface for training the xgboost model. We’re going to use &lt;code&gt;xgboost()&lt;/code&gt; to train our model. Yay.&lt;/p&gt;
&lt;p&gt;One of the parameters we set in the &lt;code&gt;xgboost()&lt;/code&gt; function is &lt;code&gt;nrounds&lt;/code&gt; - the maximum number of boosting iterations. So, how many weak learners get added to our ensemble. If we set this parameter too low, we won’t be able to model the complexity of our dataset very well. If we set it too high, we run the risk of overfitting. &lt;b&gt;We always need to be wary of overfitting our model to our training data. &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;This leads us to the &lt;code&gt;xgb.cv()&lt;/code&gt; function. &lt;b&gt;Let’s use xgb.cv() to determine how many rounds we should use for training.&lt;/b&gt; Important to note that xgb.cv() returns an object of type xgb.cv.synchronous, not xgb.Booster. So you won’t be able to call functions like xgb.importance() on it, as xgb.importance() takes object of class xgb.Booster &lt;strong&gt;not&lt;/strong&gt; xgb.cv.synchronous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# NB: keep in mind xgb.cv() is used to select the correct hyperparams.
# Here I&amp;#39;m only looking for a decent value for nrounds; We won&amp;#39;t do full hyperparameter tuning.
# Once you have them, train using xgb.train() or xgboost() to get the final model.

bst.cv &amp;lt;- xgb.cv(data = titanic_train_sparse, 
              label = titanic_train$Survived, 
              params = params_booster,
              nrounds = 300, 
              nfold = 5,
              print_every_n = 20,
              verbose = 2)

# Note, we can also implement early-stopping: early_stopping_rounds = 3, so that if there has been no improvement in test accuracy for a specified number of rounds, the algorithm stops.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the resuts of &lt;code&gt;xgb.cv()&lt;/code&gt;, let’s plot our validation error and training error against the number of round:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_df &amp;lt;- data.frame(TRAINING_ERROR = bst.cv$evaluation_log$train_error_mean, 
                     VALIDATION_ERROR = bst.cv$evaluation_log$test_error_mean, # Don&amp;#39;t confuse this with the test data set. 
                     ITERATION = bst.cv$evaluation_log$iter) %&amp;gt;%
  mutate(MIN = VALIDATION_ERROR == min(VALIDATION_ERROR))

best_nrounds &amp;lt;- res_df %&amp;gt;%
  filter(MIN) %&amp;gt;%
  pull(ITERATION)

res_df_longer &amp;lt;- pivot_longer(data = res_df, 
                              cols = c(TRAINING_ERROR, VALIDATION_ERROR), 
                              names_to = &amp;quot;ERROR_TYPE&amp;quot;,
                              values_to = &amp;quot;ERROR&amp;quot;)

g &amp;lt;- ggplot(res_df_longer, aes(x = ITERATION)) +        # Look @ it overfit.
  geom_line(aes(y = ERROR, group = ERROR_TYPE, colour = ERROR_TYPE)) +
  geom_vline(xintercept = best_nrounds, colour = &amp;quot;green&amp;quot;) +
  geom_label(aes(label = str_interp(&amp;quot;${best_nrounds} iterations gives minimum validation error&amp;quot;), y = 0.2, x = best_nrounds, hjust = 0.1)) +
  labs(
    x = &amp;quot;nrounds&amp;quot;,
    y = &amp;quot;Error&amp;quot;,
    title = &amp;quot;Test &amp;amp; Train Errors&amp;quot;,
    subtitle = str_interp(&amp;quot;Note how the training error keeps decreasing after ${best_nrounds} iterations, but the validation error starts \ncreeping up. This is a sign of overfitting.&amp;quot;)
  ) +
  scale_colour_discrete(&amp;quot;Error Type: &amp;quot;)

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-10-04-an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
This leads us to believe that we should use 9 as the value for &lt;code&gt;nrounds&lt;/code&gt;. Let’s train our XGBoost model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bstSparse &amp;lt;- xgboost(data = titanic_train_sparse, label = titanic_train$Survived, nrounds = best_nrounds, params = params_booster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  train-error:0.207632 
## [2]  train-error:0.209877 
## [3]  train-error:0.189675 
## [4]  train-error:0.173962 
## [5]  train-error:0.163861 
## [6]  train-error:0.166105 
## [7]  train-error:0.166105 
## [8]  train-error:0.162739 
## [9]  train-error:0.156004&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;makpred&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Making Predictions&lt;/h1&gt;
&lt;p&gt;Now that we have our model, we can use it to make predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_test &amp;lt;- read_csv(directoryWhichContaintsTestData)

predictions &amp;lt;- predict(bstSparse, titanic_test_sparse)
titanic_test$Survived = predictions

titanic_test &amp;lt;- titanic_test %&amp;gt;% select(PassengerId, Survived)
titanic_test$Survived = as.numeric(titanic_test$Survived &amp;gt; 0.5) 


# write_csv(titanic_test, &amp;quot;./xg_boost_data/sb.csv&amp;quot;)
# I submitted the above to Kaggle, and got a score of 0.77751 (this is the categorization accuracy)
# No discussion of AUC, precision / recall. One day, I&amp;#39;ll blog about this as well.... Maybe&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;In this blog post we saw how to put together a pipeline to implement XGBoost. We saw how to create a sparse matrix, do cross validation, create the actual model, and finally, make predictions.&lt;/p&gt;
&lt;p&gt;There’s a lot here that I didn’t cover. Things like feature importance, for example. The plan is to write a Part II, Some Day, which goes into more detail. Really, I just intend this to be a handy reference for future me.
If you see any errors in this post, please let me know :). I’ll try fix them.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Statistics the Fun Way (My Notes)</title>
      <link>/2020/09/bayesian-statistics-the-fun-way-my-notes/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/bayesian-statistics-the-fun-way-my-notes/</guid>
      <description>&lt;p&gt;These notes are by no means (or medians) comprehensive. These are just key points in the book that I want to document and ultimately remember. I’m sure I missed some important points, but hey, I probably picked up on some important ones too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch1&#34;&gt;Chapter 1 - Bayesian Thinking and Everyday Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch2&#34;&gt;Chapter 2 - Measuring Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch3&#34;&gt;Chapter 3 - The Logic of Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch4&#34;&gt;Chapter 4 - Creating a Binomial Probability Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch5&#34;&gt;Chapter 5 - The Beta Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch6&#34;&gt;Chapter 6 - Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Distinction made between Frequentists and Bayesians.
&lt;u&gt;Frequentists&lt;/u&gt; - probability represents frequency.
&lt;u&gt;Bayesian&lt;/u&gt; - probability represents how uncertain we are about a piece of information.&lt;/p&gt;
&lt;p&gt;The point is made that for coin tosses, each approach seems reasonable, but for “one-offs”, like elections, Bayesian approach makes more sense; The example of looking at probabilities associated with election results (for a given year) - Viewing the probability from a Bayesian perspective tells you about your uncertainty regarding who will win. From a Frequentist perspective, you’re making a comment about how frequently a candidate wins the 2020 election… which seems weird.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ch1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Bayesian Thinking and Everyday Reasoning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Discussion about the Bayesian process, of updating your beliefes given more data.&lt;/li&gt;
&lt;li&gt;When thinking of hypotheses in Bayesian stats, we are usually concerned about how well they predict the data we observe.&lt;/li&gt;
&lt;li&gt;The true heart of Bayesian analysis: &lt;i&gt;the test of your beliefs is how well they explain the real world.&lt;/i&gt;&lt;/li&gt;
&lt;li&gt;Distinction made between &lt;span class=&#34;math inline&#34;&gt;\(P(D|H,X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(H| D, X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; your hypothesis, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is your data, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is your experience of the world. I’m going to assume that your experience of the world is implied, so really we are talking about the distinction between &lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt;. This point is a little tricky for me to get my head around. In the &lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt; case, we change our beliefs according to the data we gather. I guess it’s kind of like: if &lt;span class=&#34;math inline&#34;&gt;\(P(D|H_1)\)&lt;/span&gt; &amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(P(D|H_2)\)&lt;/span&gt;, then pick &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; as your hypothesis. So, we check how well the data makes sense, given a bunch of hypotheses. Quote the book: “The data we observe is all that is real, so our beliefs ultimately need to shift until they align with the data”. Consider the other formulation: &lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt; - we’re basically saying, “probability of my beliefs (&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;), given the data.” Or, how well does what I observe support what I believe? Seems kind of confirmation bias-y? Not sure.&lt;/li&gt;
&lt;li&gt;So, it’s a question of “How well does what I observe support what I believe (&lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt;)” vs “How well does what I believe support what I observe (&lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt;)”. Great, now I have a headache. But seriously, the latter case seems more amenable to changing beliefs, while the former to changing data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Measuring Uncertainty&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The previous chapter was more conceptual (where we just thumb sucked probabilities, like the probability of seeing an alien is “very low”), but in this chapter, try to actually quantify&lt;/li&gt;
&lt;li&gt;Some axiomatic stuff about probability - like should add to 1, etc…&lt;/li&gt;
&lt;li&gt;Counting outcomes of events - combinatorics.&lt;/li&gt;
&lt;li&gt;Counting outcomes good for things like poker, coin tosses, etc, but what about things like “what’s the probabilty it’ll rain tomorrow?”, “Is that a UFO?”&lt;/li&gt;
&lt;li&gt;Using odds to solve above problem. Say you don’t believe that a certain article exists on Wikipedia, but your annoying friend does. You reckon it’s so unlikely, you’ll give the schmuck \$100 if the article exists, and he’ll give you \$5 if it does. &lt;span class=&#34;math inline&#34;&gt;\(\frac{100}{5} = 20\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(P(H_{no\_article}) = 20 \times P(H_{article})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;$P(H_{no_article}) = 20 (1- P(H_{no_article})) $&lt;/li&gt;
&lt;li&gt;$P(H_{no_article}) = 20 - 20 P(H_{no_article})) $&lt;/li&gt;
&lt;li&gt;so &lt;span class=&#34;math inline&#34;&gt;\(P(H_{no\_article}) = \frac{20}{21}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;In general &lt;span class=&#34;math inline&#34;&gt;\(P(H) = \frac{O(H)}{1 + O(H)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; is odds.&lt;/li&gt;
&lt;li&gt;This chapter explored 2 different twpes of probabilities: those of events and those of beliefs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. The Logic of Uncertainty&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;“AND”, leads to product rule &lt;span class=&#34;math inline&#34;&gt;\(P(A, B) = P(A) \times P(B)\)&lt;/span&gt; (note, there is no discussion of independence yet…)&lt;/li&gt;
&lt;li&gt;“OR”, &lt;span class=&#34;math inline&#34;&gt;\(P(A or B) = P(A) + P(B) - P(A \cap B)\)&lt;/span&gt;, keeping in mind &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B)\)&lt;/span&gt; will be zero when A, B mutually exclusive&lt;/li&gt;
&lt;li&gt;Nice example to illustrate this point. Say you get pulled over by the cops… You need both your registration and insurance card. You’re confident that you’ve got registration, so &lt;span class=&#34;math inline&#34;&gt;\(P(registration) = 0.7\)&lt;/span&gt;, not so confident about having insurance card so: &lt;span class=&#34;math inline&#34;&gt;\(P(insurace) = 0.2\)&lt;/span&gt;.
So, &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}) = 0.3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{ins}) = 0.8\)&lt;/span&gt;. You are worried that &lt;u&gt;either&lt;/u&gt; is missing, so use the sum rule (this is an “OR” case…) Then get: &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}) + P(Missing_{ins}) = 1.1\)&lt;/span&gt;. Great, we fucking broke statistics. But wait… are these events mutually exclusive? Does the occurence of one mean the other cannot occur? Hell no! So, we need to subtract last term… &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}, Missing_{ins})\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0.3 \times 0.8 = 0.24\)&lt;/span&gt;, so final result &lt;span class=&#34;math inline&#34;&gt;\(0.86\)&lt;/span&gt;. Note, when calculating &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}, Missing_{ins})\)&lt;/span&gt; we are assuming that they are independent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Creating a Binomial Probability Distribution&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In this chapter create our first probability distribution.&lt;/li&gt;
&lt;li&gt;That is, a way of describing all possible events and the probability of each one happening.&lt;/li&gt;
&lt;li&gt;The “bi” part refers to 2 possible outcomes. If more than 2, then distribution is called multinomial.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B(k; n, p)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, total number of outcomes we care about, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; total number of trials, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, probability of the event happening.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B(k; n, p) = {n\choose k} \times p^k \times (1-p)^{n-k}\)&lt;/span&gt; - this is the PMF (the pprobability mass function).&lt;/li&gt;
&lt;li&gt;The binomial coefficient part of the above pmf is there to account for the number of ways you could choose k successes from n trials.&lt;/li&gt;
&lt;li&gt;For example, if looking at 2 heads in 5 coin tosses, then &lt;span class=&#34;math inline&#34;&gt;\(5 \choose 2\)&lt;/span&gt; is the number of ways this could happen. Could be HHTTT, HTHTT, etc…&lt;/li&gt;
&lt;li&gt;Exmaple - &lt;i&gt;Gacha Games&lt;/i&gt;. Purchase virtual cards with in-game currency.&lt;/li&gt;
&lt;li&gt;Get a card of Bayes with p = 0.00721, Jaynes with p = 0.00720, etc… want a Jaynes card.&lt;/li&gt;
&lt;li&gt;Cost 1 Bayes Buck to pull a card, can purchase 100 Bayes Bucks for \$10. Willing to buy this if you have an even chance of pulling the card you want, namely Jaynes (p = 0.00720),&lt;/li&gt;
&lt;li&gt;Plug into above PMF : &lt;span class=&#34;math inline&#34;&gt;\({100 \choose 1} \times 0.0072^1 \times (1-0.0072)^{99}\)&lt;/span&gt;. But, this is only for getting exactly 1 Jaynes card.&lt;/li&gt;
&lt;li&gt;We need &lt;span class=&#34;math inline&#34;&gt;\(\sum^{100}_{k = 1} \times 0.0072 \times (1 - 0.0072)^{n-k}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We ain’t gonna do that by hand. If only we had some sort of device that could &lt;em&gt;compute&lt;/em&gt; this for us. A computer of sorts.&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;pbinom()&lt;/code&gt; function, can use &lt;code&gt;pbinom(0, 100, 0.0072, lower.tail = FALSE)&lt;/code&gt; = 0.5145138. SO BUY THE BAYES BUCKS.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pbinom(0, 100, 0.0072, lower.tail = TRUE) + pbinom(0, 100, 0.0072, lower.tail = FALSE) = 1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;lower.tail&lt;/code&gt; is &lt;code&gt;TRUE&lt;/code&gt;, sums up all probs less than or equal to first argument.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;lower.tail&lt;/code&gt; is &lt;code&gt;FALSE&lt;/code&gt;, it sums up the probs scrictly greater than first argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. The Beta Distribution&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Use beta distribution to estimtate the probability of an event for which you’ve already observed a number of trials and the number of successful outcomes.&lt;/li&gt;
&lt;li&gt;For example, you would use it to estimate the probability of flipping a heads when so far you’ve observed 100 tosses of a coint, and 40 of those were heads.&lt;/li&gt;
&lt;li&gt;This chapter also discusses the difference between probability, statistics and inference.&lt;/li&gt;
&lt;li&gt;Division between probability and statistics - with probabilities, you have the exact figure which you use as the probability. With probability, what we’re concerned with is how likely observations are. For example, could be a 0.5 chance of getting heads - what’s the probability of getting 7 heads out of 20?&lt;/li&gt;
&lt;li&gt;In statistics, look at this problem backwards - assuming you observe 7 heads in 20 coin tosses, what is the probability of getting heads in a single coin toss?&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Inference&lt;/i&gt; is the task of figuring out probabilities given the data.&lt;/li&gt;
&lt;li&gt;Example: black box. Put a quarter in, sometimes 2 quarters come out, sometimes it just eats your quarter. What’s the probability of getting 2 quarters? IS it 50-50? Is it something else?&lt;/li&gt;
&lt;li&gt;Try 41 quarters, get 14 wins, 27 losses.&lt;/li&gt;
&lt;li&gt;Do we say &lt;span class=&#34;math inline&#34;&gt;\(H_1: P(two coins) = \frac{1}{2}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(H_2: P(two coins) = \frac{14}{41}\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;We can use the binomial distribution for this:&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(D | H_1) = B(14;41,\frac{1}{2}) \approx 0.016\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(D | H_2) = B(14;41,\frac{14}{41}) \approx 0.130\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;So, &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; is almost 10x more likely, though neither is impossible.&lt;/li&gt;
&lt;li&gt;We could also pick different probabilities to test:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601482159/bayesian_stats_fun_way/bin_at_diff_p_wdgxwv.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doing this at a finer grain:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601483909/bayesian_stats_fun_way/bin_at_diff_p_finer_bzahpc.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Formalize this notion with the beta distribution.&lt;/li&gt;
&lt;li&gt;Beta distribution has a probability density function (pdf) rather than pmf (like binomial), because beta is continuous.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601484058/bayesian_stats_fun_way/beta_tm4fgk.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Beta Distribution&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; - probability of an event. Corresponds to our different hypotheses for the possible probabilities of our black box.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; - How many times we observe an event we care about. Like getting 2 quarters in our black box.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; - How many times the event we care about didn’t happen. Like number of times black box ate a quarter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;denominator - beta function, to normalize.
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601484310/bayesian_stats_fun_way/beta_graph_sb87gj.png&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot shows that it’s very unlikely the black box will return 2 quarters at least half the time, our break even point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we want to quantify, we need to integrate: &lt;code&gt;integrate(function(p) dbeta(p, 14, 27), 0, 0.5)&lt;/code&gt; -&amp;gt; 0.9807613 with absolute error &amp;lt; 5.9e-06&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Absolute error just because computers can’t perfectly calculate integrals, usually leads to a very, negligible error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This result tells us that, given our evidence, there is 0.98 probability that the true probability of getting two coinds out of the black box is less than 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We mostly never know true probabilities for events - that’s why beta distribution is one of the most powerful tools for understanding our data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Conditional Probability&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In this chapter learn how to reason about conditional probabilities - where probs are not independent.&lt;/li&gt;
&lt;li&gt;Also learn about Bayes’ theorem.&lt;/li&gt;
&lt;li&gt;Guillane Barre Syndrome (GBS) as incidence of 2 in 100,000&lt;/li&gt;
&lt;li&gt;This can be increased if you get a flu vaccine so,
&lt;ul&gt;
&lt;li&gt;usually &lt;span class=&#34;math inline&#34;&gt;\(P(GBS) = \frac{2}{100000}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;but &lt;span class=&#34;math inline&#34;&gt;\(P(GBS) = \frac{3}{100000}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Conditional probabilities allow us to demonstrate how information changes our beliefs.&lt;/li&gt;
&lt;li&gt;Consider colour blindness - 4.25% of people are colour blind. Caused by defective gene in X chromosome.&lt;/li&gt;
&lt;li&gt;Males only have 1 X chromosome, so about 16 times more likely to be colour blind.
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{colour blind}) = 0.0425\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{colour blind | female}) = 0.005\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{colour blind | male }) = 0.08\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Assume male/female are split 50-50&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male, colour bline}) = p(\textrm{male}) \times p(\textrm{colour blind} = 0.5 \times 0.0425 = 0.02125\)&lt;/span&gt;… problem… clearer if we try product rule with females.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{female, colour bline}) = p(\textrm{female}) \times p(\textrm{colour blind} = 0.5 \times 0.0425 = 0.02125\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Can’t be right that the two probs are the same?!?!&lt;/li&gt;
&lt;li&gt;This is because we failed failed to account for dependence between gender and colour blindness!!!&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male, colour bline}) = p(\textrm{male}) \times p(\textrm{colour blind | male} = 0.5 \times 0.08 = 0.04\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Thus, generalize the product rule: &lt;span class=&#34;math inline&#34;&gt;\(P(A, B) = P(A) \times P(B|A)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Also, update the sum rule: &lt;span class=&#34;math inline&#34;&gt;\(P(A \textrm{ or } B) = P(A) + P(B) - P(A) \times P(B|A)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;How can we determine &lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male | colour blind})\)&lt;/span&gt; ? Bayes’!&lt;/li&gt;
&lt;li&gt;THe heart of Bayesian statistics is data - and right now we have 1 piece of data: that the person we’re interested in, is indeed colour blind.&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; represent the total propulation of people.&lt;/li&gt;
&lt;li&gt;We know the person is colour blind, so we restrict to poeple who are colour blind, thus, &lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male | colour blind}) = \frac{\textrm{?}}{P(\textrm{colour blind} \times N)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;For the numerator, we want to calculate people who are male &lt;i&gt; and &lt;/i&gt; colour blind: &lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male}) \times P(\textrm{colour blind | male}) \times N\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;So: &lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{male | colour blind}) = \frac{P(\textrm{male}) \times P(\textrm{colour blind | male}) \times N}{P(\textrm{colour blind} \times N)} = 0.941\)&lt;/span&gt; (the N’s cancel)&lt;/li&gt;
&lt;li&gt;In general, Bayes’: &lt;span class=&#34;math inline&#34;&gt;\(P(A | B) = \frac{P(A)P(B|A)}{P(B)} = \frac{P(A \cap B)}{P(B)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Think about it - the &lt;span class=&#34;math inline&#34;&gt;\(P(B)\)&lt;/span&gt; goes in the denominator, because it’s the part of the world we’re restricting to. Like in the colour blind example, we only care about &lt;span class=&#34;math inline&#34;&gt;\(P(\textrm{colour blind})\)&lt;/span&gt; because we &lt;i&gt;condition&lt;/i&gt; on it!&lt;/li&gt;
&lt;li&gt;The key takeaway here is that Bayes’ allows evidence to change the strength of our beliefs. Again, think of the colour blind example. We knew the person was colour blind. Before we knew that, we reckoned a 50% chance of them being male… but after we knew this info? 94.1% chance of them being male. Hot damn!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Demonstrating The Central Limit Theorem in R</title>
      <link>/2020/08/demonstrating-the-central-limit-theorem-in-r/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/demonstrating-the-central-limit-theorem-in-r/</guid>
      <description>&lt;p&gt;In this post we&#39;ll talk about what the Central Limit Theorem is, why it&#39;s important, and how we can see it in action, using R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries used in this article:
library(ggplot2)
library(ggthemes)
library(stringr)

# Theme used for graphing:
theme_set(theme_economist())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Table of Contents&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction - What is the Central Limit Theorem?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why&#34;&gt;Why is it important?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#action&#34;&gt;Let&#39;s see it in action&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction---what-is-the-central-limit-theorem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction - What is the Central Limit Theorem? &lt;a name=&#34;introduction&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The normal distribution is famous and important because many natural phenomena are thus distributed. Heights, weight, and IQ scores are all distributed according to this bell shaped curve:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But the normal distribution (or Gaussian distribution as it is referred to by statisticians without lisps), is important for another reason: the distributions of means are themselves normally distributed! The means of samples, that is. And here&#39;s the kicker - the original population can be distributed in any which way. The original population need not be normally distributed for its sample means to be normally distributed.&lt;/p&gt;
&lt;p&gt;Let&#39;s try make this less abstract. Suppose I have a die (as in the singular of dice). Each time I roll it, I can expect to see a value between &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;. The mean value, that is, the sum of the values multiplied by the probability of seeing each value is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Mean Value = \frac{1}{6} \times 1 + \frac{1}{6} \times 2 + \frac{1}{6} \times 3 + \frac{1}{6} \times 4 + \frac{1}{6} \times 5 + \frac{1}{6} \times 6 = 3.5\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s run this bit of code to check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_value &amp;lt;- sum(seq(1:6) * (1 / 6))
print(mean_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, not very interesting, I admit. Let&#39;s now use two dice. If the first die lands a &lt;code&gt;3&lt;/code&gt;, and the second die lands a &lt;code&gt;1&lt;/code&gt;, then the mean value will be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2} \times 1 + \frac{1}{2} \times 3 = 2\)&lt;/span&gt;, wouldn&#39;t you agree? We can create a matrix which will give us the mean value for any combination of dice rolls (the values in the square brackets, [], refer to the values obtained from each die):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]  1.0  1.5  2.0  2.5  3.0  3.5
## [2,]  1.5  2.0  2.5  3.0  3.5  4.0
## [3,]  2.0  2.5  3.0  3.5  4.0  4.5
## [4,]  2.5  3.0  3.5  4.0  4.5  5.0
## [5,]  3.0  3.5  4.0  4.5  5.0  5.5
## [6,]  3.5  4.0  4.5  5.0  5.5  6.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The above code was modified from &lt;a href=&#34;https://rpubs.com/careybaldwin/346995&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In the above matrix, the average score of the two dice ranges from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;6&lt;/code&gt;. Importantly, we only see the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; &lt;em&gt;once&lt;/em&gt; in the above matrix. This is because the only way to get an average score of &lt;code&gt;1&lt;/code&gt; is to roll a &lt;code&gt;1&lt;/code&gt; on our first die, and to roll a &lt;code&gt;1&lt;/code&gt; on our second die as well. There is no other way to get an average score of &lt;code&gt;1&lt;/code&gt;. Similarly, getting an average score of &lt;code&gt;6&lt;/code&gt; means we have to roll a &lt;code&gt;6&lt;/code&gt; on both dice, yielding an average score of &lt;span class=&#34;math inline&#34;&gt;\(\frac{6 + 6}{2} = 6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Compare this with an average score of &lt;code&gt;1.5&lt;/code&gt;. There are two ways to obtain this score. We could roll a &lt;code&gt;1&lt;/code&gt; on our first die and a &lt;code&gt;2&lt;/code&gt; on our second die, &lt;em&gt;or&lt;/em&gt; we could roll a &lt;code&gt;2&lt;/code&gt; on our first die and a &lt;code&gt;1&lt;/code&gt; on our second die! So, there are more ways to get a &lt;code&gt;1.5&lt;/code&gt; than to get a &lt;code&gt;1&lt;/code&gt; (look at how many times &lt;code&gt;1.5&lt;/code&gt; appears in the above matrix compared to &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In fact, the probability of getting a certain value tends to &amp;quot;swell&amp;quot; as we get closer to &lt;code&gt;3.5&lt;/code&gt; (which is the true mean of the population). Just look at the diagonal entries of the matrix: &lt;code&gt;3.5&lt;/code&gt; appears six times in our matrix, compared to the measly two times &lt;code&gt;1.5&lt;/code&gt; or &lt;code&gt;5.5&lt;/code&gt; appear.&lt;/p&gt;
&lt;p&gt;Ok, let&#39;s hold this thought for now.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-it-important&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Why is it important? &lt;a name=&#34;why&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Hopefully we&#39;re starting to get a feel for what this Central Limit Theorem is trying to tell us. From the above, we know that when we roll a die, the average score over the long run will be &lt;code&gt;3.5&lt;/code&gt;. Even though &lt;code&gt;3.5&lt;/code&gt; isn&#39;t an actual value that appears on the die&#39;s face, over the long run if we took the average of the values from multiple rolls, we&#39;d get very close to &lt;code&gt;3.5&lt;/code&gt;. Let&#39;s try:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;number_of_rolls &amp;lt;- 1000000 # Or, we could manually roll a die 1 million times. Nah, let&amp;#39;s let the computer do it.
mean(sample(1:6, number_of_rolls, replace = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.497842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we looked at the average score when we rolled two dice, we saw that getting a mean score of &lt;code&gt;3.5&lt;/code&gt; was most likely. And this trend continues, as we add more dice. What&#39;s amazing here, is that the underlying distribution is &lt;em&gt;not normal&lt;/em&gt;, it&#39;s &lt;em&gt;uniform&lt;/em&gt;: there&#39;s a &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{6}\)&lt;/span&gt; chance of seeing each face of a particular die. But the means of the samples &lt;em&gt;are&lt;/em&gt; normally distributed. We&#39;ll see this effect in action in the next section, so don&#39;t stress too much if you don&#39;t fully grasp this yet.&lt;/p&gt;
&lt;p&gt;The fact that means are normally distributed lets us understand why we can test for differences in means between groups using t-tests (or Z-tests if we have enough information). Essentially, this is because we can ask if an observed mean is significantly different to what we&#39;d expect to have seen under the null hypothesis. Under the null hypothesis, we make a statement about what the mean of a population is equal to. We then observe a sample mean. Since these sample means are normally distributed we can ask about the probability is of having seen our observed sample mean. If it&#39;s super unlikely to have seen that sample mean, we can reject our null (not accept the alternative, just reject the null).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-see-it-in-action&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Let&#39;s see it in action &lt;a name=&#34;action&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Enough talk, time for code and pictures.&lt;/p&gt;
&lt;p&gt;We know that when rolling a fair single die, we will see a value between 1 and 6 with probability &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{6}\)&lt;/span&gt;. We can simulate rolling dice with this piece of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#roll, as in roll di(c)e
# m = number of times
# n = number of dice

roll &amp;lt;- function(m, n){
  set.seed(1234)
  means &amp;lt;- plyr::ldply(1:m, function(x){
    return(mean(sample(1:6, n, replace = TRUE)))
  }) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can call the function &lt;code&gt;roll()&lt;/code&gt; to simulate rolling a single die &lt;code&gt;10000&lt;/code&gt; times as follows: &lt;code&gt;roll(10000, 1)&lt;/code&gt;. We can then draw a bar graph of our observed values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 1
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  )

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above shouldn&#39;t be too surprising: we observed each value more or less the same proportion of the time.&lt;/p&gt;
&lt;p&gt;Let&#39;s slap a &lt;code&gt;geom_density()&lt;/code&gt; on top of this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- g +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Don&#39;t worry about the density function not going all the way down to &lt;code&gt;0&lt;/code&gt; in the space between the values &lt;code&gt;1, 2, 3, 4, 5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;: this is just due to the curve being smoothed. Really, this is putting a continuous density function over something which should be discrete.&lt;/p&gt;
&lt;p&gt;Technically, we have just produced &lt;code&gt;10000&lt;/code&gt; samples of size &lt;code&gt;1&lt;/code&gt; (we rolled &lt;code&gt;1&lt;/code&gt; die &lt;code&gt;10000&lt;/code&gt; times). Let&#39;s produce &lt;code&gt;10000&lt;/code&gt; samples of size &lt;code&gt;2&lt;/code&gt; (roll &lt;code&gt;2&lt;/code&gt; dice &lt;code&gt;10000&lt;/code&gt; and plot the means of these samples):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 2
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above is starting to look a bit more bell-shaped. And it&#39;s centered around the vertical red line, which has the x-intercept of &lt;code&gt;3.5&lt;/code&gt; - the true population mean.&lt;/p&gt;
&lt;p&gt;Let&#39;s increase our sample size to 4:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 4
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not only is the curve starting to look more normal, but the tails of the distribution are starting to flatten out: that is, the extreme mean values of &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; have become less probable.&lt;/p&gt;
&lt;p&gt;Let&#39;s increase the sample size to 20:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 20
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1.2)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we see the classic bell shaped curve. Notice that the peak of the distribution is taller than in the previous graph (the y-axis runs from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;1.25&lt;/code&gt; in this graph as opposed to &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; in the previous). The extreme mean values around &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; seem &lt;em&gt;very&lt;/em&gt; unlikely. In fact getting a mean score around &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;5&lt;/code&gt; also rarely happens. Seems like the distribution is getting &amp;quot;tighter&amp;quot; around the true mean value of &lt;code&gt;3.5&lt;/code&gt; the larger our sample size grows.&lt;/p&gt;
&lt;p&gt;Let&#39;s go wild and simulate &lt;code&gt;10000&lt;/code&gt; samples of &lt;code&gt;2000&lt;/code&gt; dice being rolled! Hold on to your monocles!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 2000
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice 
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a very thin normal distribution. It&#39;s super tight around &lt;code&gt;3.5&lt;/code&gt;. Doesn&#39;t this make sense though? It&#39;s hard to image rolling &lt;code&gt;2000&lt;/code&gt; dice &lt;code&gt;10000&lt;/code&gt; times. But let&#39;s imagine you and &lt;code&gt;1999&lt;/code&gt; of your closest friends each rolled one die, only once, rather than the &lt;code&gt;10000&lt;/code&gt; times required. Does it not seem likely that if you added up everyone&#39;s value and divided by &lt;code&gt;2000&lt;/code&gt; that you&#39;d get pretty close to &lt;code&gt;3.5&lt;/code&gt;, the true population mean?&lt;/p&gt;
&lt;p&gt;Saying that the sample distribution is super tight around &lt;code&gt;3.5&lt;/code&gt; can be expressed in a more mathematical way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{samplingDistribution} = \frac{\sigma_{population}}{\sqrt{n}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the standard deviation.&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the size of your sample. So if the sample size is very large, say &lt;code&gt;2000&lt;/code&gt;, we can expect the sampling distribution&#39;s standard deviation to be rather small, resulting in &amp;quot;thinner&amp;quot; normal curves. Crucially, the mean of the sampling distribution is the same as the population&#39;s mean. But the standard deviation is not.&lt;/p&gt;
&lt;p&gt;The sample standard deviation &lt;em&gt;is&lt;/em&gt; related to the population standard deviation, though (just look at the numerator in the above). The more the original population varies (that is, the higher its standard deviation), the larger the sample size needs to be before we can get &amp;quot;thinner&amp;quot; normal curves.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;if-youve-made-it-this-far...&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. If you&#39;ve made it this far...&lt;/h1&gt;
&lt;p&gt;Well done - I hope that made some modicum of sense and that it was useful. If you have any suggestions for improvements or (*gasp*) find any errors in the above, &lt;em&gt;please&lt;/em&gt; let me know... it will come much appreciated!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Zulu Stuff</title>
      <link>/zulu-stuff/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/zulu-stuff/</guid>
      <description>


&lt;p&gt;In my attemps to learn isiZulu, I have created some resources to help my future self when he/it inevitably forgets the things he has painstakingly attempted to insert into his skull. You’re welcome.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/1/07/zulu-article-uzoqhubeka-asebenzele-ekhaya-umantashe-osuleleke-ngecovid-19&#34;&gt;Uzoqhubeka asebenzele ekhaya uMantashe osuleleke ngeCovid-19&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/1/07/zulu-article-ucele-uthi-bazoboshwa-abasebenzi-bakahulumeni-abazobanjwa-benza-inkohlakalo&#34;&gt;UCele uthi bazoboshwa abasebenzi bakahulumeni abazobanjwa benza inkohlakalo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hitchhiker&#39;s Guides</title>
      <link>/hitchhikers-guides/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/hitchhikers-guides/</guid>
      <description>


&lt;p&gt;I’ve started preparing these guides across various topics. I’ve done this at some of the places I’ve worked, and they seem pretty useful. I use these as a handy reference to technical topics.&lt;/p&gt;
&lt;p&gt;These are rough notes, not really meant for public consumption (but who even reads this blog?). Parts of these guides will probably end up as blog posts.&lt;/p&gt;
&lt;p&gt;Guides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/1/07/hitchikers-guide-to-inferential-statistics/&#34;&gt;The Hitchhiker’s Guide to Inferential Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/1/07/islr-notes&#34;&gt;ISLR Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/1/07/moment-generating-functions&#34;&gt;Moment Generating Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Work Rules! - Insights from Inside Google That Will Transform How You Live and Lead, Part 1</title>
      <link>/2019/09/work-rules/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/work-rules/</guid>
      <description>&lt;!-- toc --&gt;
&lt;p&gt;Hello Person From The Internet who has decided to come here. Thank you for dropping by.&lt;/p&gt;
&lt;p&gt;Today I’m going to talk about a book I read - &lt;a href=&#34;https://www.amazon.com/gp/product/1455554790/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=1455554790&amp;amp;linkCode=as2&amp;amp;tag=orrymr-20&amp;amp;linkId=35253fa91ca83356f3f4b863eb42b3f1&#34;&gt;Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead&lt;/a&gt; by Laszlo Bock.&lt;/p&gt;
&lt;p&gt;Although this book is intended for people who manage teams, most human beings will be able to get valuable insight out of this worthwhile book. Our author, Laszlo, has been in various roles at some large companies, including a stint as Senior Vice President of People Operations (HR, I believe) at Google (the search engine company - keep an eye out for them).&lt;/p&gt;
&lt;p&gt;I read this book a couple of months ago, and have been procrastinating writing this post ever since. So, I’m relying on my barely legible notes and an unreliable brain (human memory is so capricious, isn’t it?).&lt;/p&gt;
&lt;p&gt;What I shall do, Dear Reader, is go through the book, chapter by chapter, divulging my meticulous notes to you.&lt;/p&gt;
&lt;p&gt;Pull yourself towards yourself, here it comes!&lt;/p&gt;
&lt;div id=&#34;chapter-0-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 0 &amp;amp; 1&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Being part of an environment where people thrive starts with taking responsibility for that environment. I like that. Make a place nice. Don’t only care about output, care about what &lt;em&gt;kind&lt;/em&gt; of place it is. Is it the kind of place where we yell at the new guy because he doesn’t know where the HR office is? No! We don’t do that. Because eventually he’ll find out where the HR office is, and we will get in trouble! Is it the kind of place where we show the new guy around? Yes! Because he might be smarter and more hard working than we are, and he might remember that when he is our overlord.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Act like a founder” - sayeth Bock. Again, this is great. It speaks to taking personal responsibility for the work you put out; having pride and a sense of ownership in what you put out into the world. &lt;strong&gt;Act like a founder.&lt;/strong&gt; You’re spending the better part of your day at the job: own it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Talent flows to “high freedom” companies - although Laszlo does state in a later chapter that companies have been successful both in high and low freedom settings. That’s one of the things I really liked about this book; he often says “this is how we did things, it worked for us like that, but that don’t mean it ain’t gonna work for you in another way, dagnabit”. I’m paraphrasing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Managers serve the team - s/he clears roadblocks. Not literal ones, although, he doesn’t preclude managers from clearing literal roadblocks (but I don’t recommend it).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-2---culture-eats-strategy-for-breakfast&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 2 - Culture Eats Strategy For Breakfast&lt;/h1&gt;
&lt;p&gt;I wonder what culture has for brunch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google has a simple mission statement (organize the world’s information and make it universally accessible and useful). It’s easy to rally behind, but they can never really full accomplish it… so they are compelled to keep going.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Google has a culture of information sharing and trust.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-3---lake-wobegon-where-all-the-hires-are-above-average&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 3 - Lake Wobegon, Where all the Hires are Above Average&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only hire people who are better than you (in some meaningful way).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take your time hiring.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-4---searching-for-the-best&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 4 - Searching For the Best&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We are all biased; we may be biased towards hiring friends.. So try be objective in your hiring (read the book to learn how. What? You expect moi to tell you?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-5---dont-trust-your-gut&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 5 - Don’t Trust Your Gut&lt;/h1&gt;
&lt;p&gt;For me this was a tough one. If my gut says cheeseburger, I’m trusting it. Apparently hiring people and deciding what lunch to get &lt;em&gt;should&lt;/em&gt; involve different cognitive processes. K.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Confirmation_bias&#34;&gt;Confirmation bias&lt;/a&gt; (my personal favourite of all the cognitive biases) - interviewees make a first impression on us… then we spend the rest of the time in the interview trying to confirm our initial impression. Well, some of us do. The more sound of mind amongst us know that if a person makes a first impression as a well put together educated young man, then they are obviously a machete wielding senile woman.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get a disinterested party to interview, for objectivity. Like, get someone from another team.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Have a subordinate interview and get his/her opinion.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aggregate many interviewers’ opinions - wisdom of the crowd.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;defusing-the-exploding-offer-the-farpoint-gambit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defusing the Exploding Offer: The Farpoint Gambit&lt;/h1&gt;
&lt;p&gt;This isn’t a chapter, but I thought it a cool enough idea to merit its own section.&lt;/p&gt;
&lt;p&gt;An exploding offer is an offer of employment which expires after a certain amount of time. This can be quite a short amount of time, intending to pressurize. The gambit is simply to accept the offer… provisionally. What you do is set out a condition of your own which causes the deadline to pass. Once the deadline passes, the credibility of the threat is destroyed.&lt;/p&gt;
&lt;p&gt;Say you have 1 week within which to accept the offer. You say: “Gee, I accept!… But I’d sure like to meet the team I’ll be working with. I can’t this week, because my aunt is getting her tonsils removed, so maybe next week?” Of course, next week is already after the deadline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-6---let-the-inmates-run-the-asylum&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 6 - Let the Inmates Run The Asylum&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Eliminate status symbols - symbols and stories matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;He spends a lot of time advocating for the removal of managers’ power. However, one place he says managers should have power, is in tie breaking. When a team can’t make a decision, it can’t move forward. By having a person break the tie, we can all continue making dog food at the dog food factory. One of managers’ primary responsibilities then, is to BREAK TIES.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;im-tired-and-going-to-sleep&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I’m tired and going to sleep&lt;/h1&gt;
&lt;p&gt;So I will continue this in a part 2, at some point in the future.&lt;/p&gt;
&lt;p&gt;Until then, Dear Reader, never stop learning, reading and being awesome!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>r-pca</title>
      <link>/2019/03/r-pca/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/r-pca/</guid>
      <description>&lt;p&gt;Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca---a-brief-introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. PCA - A Brief Introduction&lt;/h1&gt;
&lt;p&gt;PCA dates back to the early 20th century, and is used to reduce the dimensionality of data (technically, it’s a &lt;em&gt;linear&lt;/em&gt; dimensionality reduction procedure. We won’t get into what that means in this post).&lt;/p&gt;
&lt;p&gt;The reason you might want to do this is because high dimensional data might lie in a low dimensional subspace. We want to “re-state” this data in that lower dimensional space; that is, we want to get rid of those superfluous extra dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Intuition&lt;/h1&gt;
&lt;p&gt;A simple example could be collecting data about houses in your neighbourhood; you collect features such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of windows&lt;/li&gt;
&lt;li&gt;Number of bedrooms&lt;/li&gt;
&lt;li&gt;House price&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each one of these is another dimension in your dataset. Say you mistakenly collect the area of your house both in square meters &lt;em&gt;and&lt;/em&gt; in square feet. You now have 5 dimensional data, but really, it only lies in a 4 dimensional subspace.&lt;/p&gt;
&lt;p&gt;This is obviously a contrived example, but hopefully you get the picture.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Implementation in R&lt;/h1&gt;
&lt;p&gt;Let’s generate data which simulates some marks in two tests, a maths test and a physics test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1337)
mathScore &amp;lt;- runif(100) * 100
physicsScore &amp;lt;- mathScore + rnorm(100, sd = 8)

#Let&amp;#39;s make sure that there are no negative marks:
physicsScore &amp;lt;- pmax(0, physicsScore)

#And let&amp;#39;s make sure that there are no marks greater than 100:
physicsScore &amp;lt;- pmin(100, physicsScore)

allTestResults &amp;lt;- data.frame(math = mathScore, physics = physicsScore)
plot(allTestResults$math, allTestResults$physics, xlab = &amp;quot;Math&amp;quot;, ylab = &amp;quot;Physics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see from the picture above that the data are pretty correlated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(allTestResults$math, allTestResults$physics)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9725437&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe there is some underlying, latent “smartness” dimension upon which the data truly lie. Let us attempt to determine&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_results &amp;lt;- prcomp(allTestResults, center = TRUE) #Don&amp;#39;t need to scale, since both axes already on same scale, but centering is NB

print (pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Standard deviations (1, .., p=2):
## [1] 42.769762  5.045888
## 
## Rotation (n x k) = (2 x 2):
##                PC1        PC2
## math    -0.7050443  0.7091633
## physics -0.7091633 -0.7050443&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pca_results$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s scale the plot:
plot(pca_results$x, xlim = c(-60, 80), ylim = c(-60, 80))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;screeplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pov &amp;lt;- pca_results$sdev^2/sum(pca_results$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Percentage of variance explained by the first principal component: 98.6272283%&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# allTestResults[8, ]
# allTestResults[34, ]
# allTestResults[100, ]
# allTestResults[57, ]
# allTestResults[84, ]
# allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[8, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 8 28.11173 32.83384&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[34, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 34 22.91205 22.28855&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[100, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         math physics
## 100 97.10537     100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[57, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 57 14.12764 7.651867&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[84, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 84 18.4879 15.14439&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 39 29.27543 36.10048&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>r-tsne</title>
      <link>/2019/03/r-tsne/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/r-tsne/</guid>
      <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
library(cluster)
#####

a_1 &amp;lt;- rnorm(200, mean = 20, sd = 2)
a_2 &amp;lt;- rnorm(200, mean = 272, sd = 5)
a_3 &amp;lt;- rnorm(200, mean = 62, sd = 3)
a_4 &amp;lt;- rnorm(200, mean = 600, sd = 9)
a_5 &amp;lt;- rnorm(200, mean = 209, sd = 5)
a_y &amp;lt;- rep(1, 200)


b_1 &amp;lt;- rnorm(200, mean = -20, sd = 32)
b_2 &amp;lt;- rnorm(200, mean = 23, sd = 5)
b_3 &amp;lt;- rnorm(200, mean = -32, sd = 31)
b_4 &amp;lt;- rnorm(200, mean = 5670, sd = 93)
b_5 &amp;lt;- rnorm(200, mean = 19283, sd = 59)
b_y &amp;lt;- rep(0, 200)


data &amp;lt;- data.frame(first = c(a_1, b_1),
                   second = c(a_2, b_2),
                   third = c(a_3, b_3),
                   fourth = c(a_4, b_4),
                   fifth = c(a_5, b_5),
                   class = c(a_y, b_y))



data_gower &amp;lt;- daisy(data %&amp;gt;% select(-class))

set.seed(1337)
tsne_obj_test &amp;lt;- Rtsne(data_gower, is_distance = TRUE)

tsne_data &amp;lt;- tsne_obj_test$Y %&amp;gt;%
  data.frame() %&amp;gt;%
  setNames(c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;))

tsne_data$class &amp;lt;- data$class

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = class))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-tsne_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to H2O Using R</title>
      <link>/2018/12/intro-to-h2o-using-r/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/intro-to-h2o-using-r/</guid>
      <description>&lt;!-- toc --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In this post we discuss the &lt;a href=&#34;https://www.h2o.ai/&#34;&gt;H2O machine learning platform.&lt;/a&gt; We talk about what H2O is, and how to get started with it, using R - we create a Random Forest which we use to classify the Iris Dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-h2o&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. What is H2O?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The definition found on &lt;a href=&#34;https://github.com/h2oai/h2o-3&#34;&gt;H2O’s Github page&lt;/a&gt; is a lot to take in,&lt;/strong&gt; especially if you’re just starting out with H2O: “H2O is an in-memory platform for distributed, scalable machine learning. H2O uses familiar interfaces like R, Python, Scala, Java, JSON and the Flow notebook/web interface, and works seamlessly with big data technologies like Hadoop and Spark.”&lt;/p&gt;
&lt;p&gt;We spend the rest of section 2 as well as section 3 discussing salient points of the above definition.&lt;/p&gt;
&lt;div id=&#34;h2o-is-an-in-memory-platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 H2O is an In-Memory Platform&lt;/h2&gt;
&lt;p&gt;H2O is an “in-memory platform”. &lt;strong&gt;Saying that it’s in-memory means that the data being used is loaded into main memory (RAM).&lt;/strong&gt; Reading from main memory, (also known as primary memory) is typically much faster than secondary memory (such as a hard drive).&lt;/p&gt;
&lt;p&gt;H2O is a “platform.” &lt;strong&gt;A platform is software which can be used to build something - in this case, machine learning models.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Putting this togther we now know that H2O is an in-memory environment for building machine learning models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h2o-is-distributed-and-scalable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 H2O is Distributed and Scalable&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;H2O can be run on a cluster.&lt;/strong&gt; &lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt; is an example of a cluster which can run H2O.&lt;/p&gt;
&lt;p&gt;H2O is said to be distributed because your object can be spread amongst several nodes in your cluster. H2O does this by using a Distributed Key Value (DKV). You can read more about it &lt;a href=&#34;https://www.h2o.ai/blog/h2o-architecture/&#34;&gt;here&lt;/a&gt;, but essentially what this means, is that &lt;strong&gt;any object you create in H2O can be distributed amongst several nodes in the cluster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The key-value part of DKV means that when you load data into H2O, you get back a key into a hashmap containing your (potentially distributed) object.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-h2o-runs-under-the-hood&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. How H2O Runs Under the Hood&lt;/h1&gt;
&lt;p&gt;We spoke earlier about H2O being a platform. &lt;strong&gt;It’s important to distinguish between the R interface for H2O, and H2O itself.&lt;/strong&gt; H2O can exist perfectly fine without R. H2O is just a &lt;a href=&#34;https://en.wikipedia.org/wiki/JAR_(file_format)&#34;&gt;.jar&lt;/a&gt; which can be run on its own. If you don’t know (or particularly care) what a .jar is - just think of it as Java code packaged with all the stuff you need in order to run it.&lt;/p&gt;
&lt;p&gt;When you start H2O, you actually create a server which can respond to &lt;a href=&#34;https://en.wikipedia.org/wiki/Representational_state_transfer&#34;&gt;REST&lt;/a&gt; calls. Again, you don’t really need to know how REST works in order to use H2O. But if you do care, just know that &lt;strong&gt;you can use any HTTP client to speak with an H2O instance.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;R is just a client interfact for H2O. All the R functions you call when working with H2O are actually calling H2O using a REST API (a JSON POST request) under the hood. The Python H2O library, as well as the &lt;a href=&#34;https://www.h2o.ai/blog/introducing-flow/&#34;&gt;Flow UI&lt;/a&gt;, interface with H2O in a similar way. &lt;strong&gt;If this is all very confusing just think about it like this: you use R to send commands to H2O. You could equally well just use Flow or Python to send commands.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-an-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Running An Example&lt;/h1&gt;
&lt;div id=&#34;installing-h2o&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.1 Installing H2O&lt;/h2&gt;
&lt;p&gt;You can install H2O using R: &lt;code&gt;install.packages(&amp;quot;h2o&amp;quot;)&lt;/code&gt;. If you’re having trouble with this, &lt;a href=&#34;http://h2o-release.s3.amazonaws.com/h2o/rel-xia/2/index.html&#34;&gt;have a look here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-h2o-and-loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.2 Starting H2O and Loading Data&lt;/h2&gt;
&lt;p&gt;First we’ll need to load the packages we’ll be using: &lt;code&gt;h2o&lt;/code&gt; and &lt;code&gt;datasets&lt;/code&gt;. We load the latter as we’ll be using the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris Dataset&lt;/a&gt;, which is part of the &lt;code&gt;datasets&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
library(h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Iris Dataset contains attributes of three species of iris flowers.&lt;/p&gt;
&lt;p&gt;Let’s load the iris dataset, and start up our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init(nthreads = -1) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in h2o.clusterInfo(): 
## Your H2O cluster version is too old (1 year, 7 months and 4 days)!
## Please download and install the latest version from http://h2o.ai/download/&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, H2O starts up using 2 cores. By calling &lt;code&gt;h2o.init(nthreads = -1)&lt;/code&gt;, with &lt;code&gt;nthreads = -1&lt;/code&gt;, we use all available cores.&lt;/p&gt;
&lt;p&gt;Edit: it doesn’t default to two cores anymore (as per this tweet from H2O’s chief ML scientist):&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Nice post! BTW, H2O in R no longer defaults to 2 cores, so you can just do &lt;code&gt;h2o.init()&lt;/code&gt; now. :-)
&lt;/p&gt;
— Erin LeDell (&lt;span class=&#34;citation&#34;&gt;@ledell&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/ledell/status/1075444885296168962?ref_src=twsrc%5Etfw&#34;&gt;December 19, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;If &lt;code&gt;h2o.init()&lt;/code&gt; was succesful, you should have an instance of H2O running locally! You can verify this by navigating to &lt;a href=&#34;http://localhost:54321&#34; class=&#34;uri&#34;&gt;http://localhost:54321&lt;/a&gt;. There, you should see the Flow UI.&lt;/p&gt;
&lt;p&gt;The iris dataset is now loaded into R. However, it’s not yet in H2O. Let’s go ahead and load the iris data into our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex &amp;lt;- as.h2o(iris)
h2o.ls()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.ls()&lt;/code&gt; lists the dataframes you have loaded into H2O. Right now, you should see only one: iris.&lt;/p&gt;
&lt;p&gt;Let’s start investigating this dataframe. We can get the summary statistics of the various columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use H2O to plot histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.hist(iris.hex$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2018-12-04-r-h2o-introduction_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can use familiar R syntax to modify your H2O dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- iris.hex$Sepal.Length + 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now run &lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;, we should see this extra variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
## 6          foo real       0     0      0      0 6.3 9.9 7.843333 0.8280661
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3
## 6          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(What I still don’t understand is why we don’t see this extra column from the Flow UI. If anyone knows, please let me know in the comments!)&lt;/p&gt;
&lt;p&gt;But we don’t really need this nonsense column, so let’s get rid of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get our dataframe back into R, from H2O:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_df &amp;lt;- as.data.frame(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.3 Building a Model&lt;/h2&gt;
&lt;p&gt;We’ve got our H2O instance up and running, with some data in it. Let’s go ahead and do some machine learning - let’s implement a Random Forest.&lt;/p&gt;
&lt;p&gt;First off, we’ll split our data into a training set and a test set. I’m not going to explicitly set a validation set, as the algorithm will use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-bag_error&#34;&gt;out of bag error&lt;/a&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits &amp;lt;- h2o.splitFrame(data = iris.hex,
                         ratios = c(0.8),  #partition data into 80% and 20% chunks
                         seed = 198)

train &amp;lt;- splits[[1]]
test &amp;lt;- splits[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.splitFrame()&lt;/code&gt; uses approximate splitting. That is, it won’t split the data into an exact 80%-20% split. Setting the seed allows us to create reproducible results.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;h2o.nrow()&lt;/code&gt; to check the number of rows in our train and test sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in train set: &amp;quot;, h2o.nrow(train)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in train set: 117&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in test set: &amp;quot;, h2o.nrow(test)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in test set: 33&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s call &lt;code&gt;h2o.randomForest()&lt;/code&gt; to create our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf &amp;lt;- h2o.randomForest(x = c(&amp;quot;Sepal.Length&amp;quot;, &amp;quot;Sepal.Width&amp;quot;, &amp;quot;Petal.Length&amp;quot;, &amp;quot;Petal.Width&amp;quot;),
                    y = c(&amp;quot;Species&amp;quot;),
                    training_frame = train,
                    model_id = &amp;quot;our.rf&amp;quot;,
                    seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; specify our independent and dependent variables, respectively. The &lt;code&gt;training_frame&lt;/code&gt; specified the training set, and &lt;code&gt;model_id&lt;/code&gt; is the model name, within H2O (not to be confused with variable &lt;code&gt;rf&lt;/code&gt; in the above code - &lt;code&gt;rf&lt;/code&gt; is the R handle; whereas &lt;code&gt;our.rf&lt;/code&gt; is what H2O calls the model). &lt;code&gt;seed&lt;/code&gt; is used for reproducibility.&lt;/p&gt;
&lt;p&gt;We can get the model details simply by printing out the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(rf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model Details:
## ==============
## 
## H2OMultinomialModel: drf
## Model ID:  our.rf 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth
## 1              50                      150               18940         1
##   max_depth mean_depth min_leaves max_leaves mean_leaves
## 1         7    3.26000          2         12     5.41333
## 
## 
## H2OMultinomialMetrics: drf
## ** Reported on training data. **
## ** Metrics reported on Out-Of-Bag training samples **
## 
## Training Set Metrics: 
## =====================
## 
## Extract training frame with `h2o.getFrame(&amp;quot;RTMP_sid_b2ea_7&amp;quot;)`
## MSE: (Extract with `h2o.mse`) 0.03286954
## RMSE: (Extract with `h2o.rmse`) 0.1812996
## Logloss: (Extract with `h2o.logloss`) 0.09793089
## Mean Per-Class Error: 0.0527027
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;,train = TRUE)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error      Rate
## setosa         40          0         0 0.0000 =  0 / 40
## versicolor      0         38         2 0.0500 =  2 / 40
## virginica       0          4        33 0.1081 =  4 / 37
## Totals         40         42        35 0.0513 = 6 / 117
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;,train = TRUE)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.948718
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That seems pretty good. But let’s see how the model performs on the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_perf1 &amp;lt;- h2o.performance(model = rf, newdata = test)
print(rf_perf1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## H2OMultinomialMetrics: drf
## 
## Test Set Metrics: 
## =====================
## 
## MSE: (Extract with `h2o.mse`) 0.05806405
## RMSE: (Extract with `h2o.rmse`) 0.2409648
## Logloss: (Extract with `h2o.logloss`) 0.1708688
## Mean Per-Class Error: 0.1102564
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa         10          0         0 0.0000 = 0 / 10
## versicolor      0          9         1 0.1000 = 1 / 10
## virginica       0          3        10 0.2308 = 3 / 13
## Totals         10         12        11 0.1212 = 4 / 33
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.878788
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s use our model to make some predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- h2o.predict(rf, test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predict    setosa versicolor  virginica
## 1  setosa 0.9969698          0 0.00303019
## 2  setosa 0.9969698          0 0.00303019
## 3  setosa 0.9969698          0 0.00303019
## 4  setosa 0.9969698          0 0.00303019
## 5  setosa 0.9969698          0 0.00303019
## 6  setosa 0.9969698          0 0.00303019
## 
## [33 rows x 4 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;This post discussed what H2O is, and how to use it from R. The full code used in this post can be found &lt;a href=&#34;./h2o_intro.R&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>