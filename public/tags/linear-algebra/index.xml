<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>linear algebra on orrymr.com</title>
    <link>/tags/linear-algebra/</link>
    <description>Recent content in linear algebra on orrymr.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 18 Nov 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Inverse Matrices - A Primer</title>
      <link>/2018/11/inverse-matrix/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/inverse-matrix/</guid>
      <description>&lt;p&gt;In this post &lt;strong&gt;we talk about the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;inverse matrix&lt;/em&gt;&lt;/strong&gt;. We discuss what it means for a matrix to be &lt;em&gt;invertible&lt;/em&gt;. We then discuss what it means for a matrix &lt;em&gt;not&lt;/em&gt; to be invertible - &lt;em&gt;singular&lt;/em&gt;. Finally, we briefly go through how to find a matrix&amp;rsquo;s inverse.&lt;/p&gt;

&lt;!-- toc --&gt;

&lt;h1 id=&#34;1-invertibility&#34;&gt;1. Invertibility&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s think about a square matrix, $A$. In &lt;a href=&#34;/2017/10/of-matrices-and-men-pt.-1/&#34;&gt;a previous post&lt;/a&gt; we mentioned that &lt;strong&gt;a matrix acts on a vector&lt;/strong&gt;. The matrix does something to that vector.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The inverse of a matrix undoes what the original matrix did.&lt;/strong&gt; The inverse of $A$ is $A^{-1}$. $A^{-1}$ undoes what $A$ did. It&amp;rsquo;s kind of like dividing by a number after multiplying by it.&lt;/p&gt;

&lt;p&gt;If we have a vector $x$, and we multiply it by our matrix $A$, we get $Ax$. If we undo this operation by applying the inverse, we get back to $x = A^{-1}Ax$. The one matrix which always leaves any vector unchanged is the identity matrix, $I$. That is: $Ix = x$ . &lt;strong&gt;This is why $A^{-1}A = I$.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When matrix $A$ has an inverse it is said to be &lt;em&gt;invertible&lt;/em&gt;. However, this is not always the case. &lt;strong&gt;When a matrix is not invertible, it is also known as&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;singular.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;2-non-invertibility-singular-matrices&#34;&gt;2. Non-Invertibility - Singular Matrices&lt;/h1&gt;

&lt;p&gt;Not all matrics are invertible. &lt;strong&gt;Matrices which are not invertible are known as&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;singular.&lt;/em&gt;&lt;/strong&gt; This means that there exists no matrix which can undo what the original matrix just did.&lt;/p&gt;

&lt;p&gt;Say we are working in two dimensions - a plane. &lt;strong&gt;Most $2 \times 2$ matrices will move vectors around the $2 \times 2$ plane.&lt;/strong&gt; They&amp;rsquo;ll either stretch them, or squash them, or even change their direction. But they&amp;rsquo;ll usually use up all of the plane; the output of that matrix multiplication won&amp;rsquo;t just be limited to a single line on that plane. The output of that matrix multiplication can &lt;strong&gt;&lt;em&gt;usually&lt;/em&gt;&lt;/strong&gt; fall anywhere on the plane.&lt;/p&gt;

&lt;p&gt;But &lt;strong&gt;if the output of a $2 \times 2$ matrix always lands on a single line, then there is no way to undo that operation.&lt;/strong&gt; We&amp;rsquo;ve, in effect, lost some information.&lt;/p&gt;

&lt;p&gt;This extends to higher dimensions. &lt;strong&gt;If you cannot select a vector in $\mathbb{R}^n$ so that the output of an $n \times n$ matrix multiplication with that vector can fall anywhere within the $\mathbb{R}^n$ hyperplane, then that $n \times n$ matrix is singular.&lt;/strong&gt; This means that the output of that matrix multiplication is constrained to a lower dimension (even a line, or a point).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It&amp;rsquo;s kind of like how $0$ has no inverse.&lt;/strong&gt; If we have the number $10$, and we multiply it by $5$, we get $50$. If we want to undo this operation, we multiply by $5$&amp;rsquo;s (multiplicative) inverse: $\frac {1} {5}$ and get back $50 \times \frac {1} {5} = 10$. However, if we multiply $10$ by $0$, we get $0$. There really isn&amp;rsquo;t much we can do to $0$ to get back to $10$. This is a tenuous analogy at best, as matrices and numbers are different animals. But, hopefully this helps convey &lt;em&gt;some&lt;/em&gt; intuition.&lt;/p&gt;

&lt;h1 id=&#34;3-testing-for-singularity&#34;&gt;3. Testing for Singularity&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;There are several equivalent conditions which, if met, mean that a matrix is singular.&lt;/strong&gt; Some of these conditions are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The matrix&amp;rsquo;s determinant is 0&lt;/li&gt;
&lt;li&gt;The matrix does not have full rank&lt;/li&gt;
&lt;li&gt;The $n \times n$ matrix does not have all of its pivots.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Discussions about rank and determinants are enough to warrant their own posts (and more), and are beyond the scope of this post. When I do get around to writing those posts, I&amp;rsquo;ll be sure to link them here!&lt;/p&gt;

&lt;p&gt;For now, let&amp;rsquo;s just keep in mind that not all matrices have inverses, and that if the determinant of a matrix is zero, then it doesn&amp;rsquo;t have an inverse.&lt;/p&gt;

&lt;p&gt;But if it does have an inverse, let&amp;rsquo;s try out finding it:&lt;/p&gt;

&lt;h1 id=&#34;4-inverse-of-a-2x2-matrix&#34;&gt;4. Inverse of a 2x2 Matrix&lt;/h1&gt;

&lt;p&gt;A $2 \times 2$ matrix has this form:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  a &amp;amp; b  \newline
  c &amp;amp; d
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;If a $2 \times 2$ matrix is invertible, then its inverse is:&lt;/p&gt;

&lt;p&gt;\[
\frac {1} {ad - bc}
\times
 \begin{bmatrix}
  d &amp;amp; -b  \newline
  -c &amp;amp; a
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;$\frac {1} {ad - bc}$ is the determinant of the matrix. If it&amp;rsquo;s zero, then we&amp;rsquo;d have $\frac {1} {0}$. In other words, the inverse wouldn&amp;rsquo;t exist - our matrix would be singular.&lt;/p&gt;

&lt;p&gt;Consider the matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;According to our formula above, its inverse is:&lt;/p&gt;

&lt;p&gt;\[
\frac {1} {1 \times 7 - 3 \times 2}
\times
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
%
&amp;#61;
%
\frac {1} {1}
\times
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
 %
&amp;#61;
%
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s verify that this is indeed the inverse in two ways.&lt;/p&gt;

&lt;h2 id=&#34;4-1-verification-1-matrix-times-inverse-equals-identity&#34;&gt;4.1 Verification #1 - Matrix times Inverse Equals Identity&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll multiply the original matrix by the inverse. If we get the identity matrix, then we&amp;rsquo;ve done our calculation correctly:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
  \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 \times 7 + 3 \times (-2) &amp;amp; 1 \times (-3) + 3 \times 1  \newline
  2 \times 7 + 7 \times (-2) &amp;amp; 2 \times (-3) + 7 \times 1
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 &amp;amp; 0  \newline
  0 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;h2 id=&#34;4-2-verification-2-inverse-undoes-what-the-original-matrix-did&#34;&gt;4.2 Verification #2 - Inverse Undoes what the Original Matrix Did&lt;/h2&gt;

&lt;p&gt;Next, we&amp;rsquo;ll multiply a vector by the original matrix and then the result of that calculation, by the inverse. &lt;strong&gt;We should get back our original vector.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s pick the vector $ \begin{bmatrix} 1 &amp;amp; 2 \end{bmatrix} ^ {T}$:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
  \begin{bmatrix}
  1 \newline
  2
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 \times 1 + 3 \times 2 \newline
  2 \times 1 + 7 \times 2
 \end{bmatrix}
  %
  &amp;#61;
  %
  \begin{bmatrix}
  7 \newline
  16
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now see if the inverse matrix brings  $ \begin{bmatrix} 7 &amp;amp; 16 \end{bmatrix} ^ {T}$ back to $ \begin{bmatrix} 1 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
  \begin{bmatrix}
  7 \newline
  16
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  7 \times 7 + (-3) \times 16 \newline
  (-2) \times 7 + 1 \times 16
 \end{bmatrix}
  %
  &amp;#61;
  %
  \begin{bmatrix}
  1 \newline
  2
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;So it looks like we&amp;rsquo;ve done our calculations correctly!&lt;/p&gt;

&lt;h1 id=&#34;5-inverse-of-a-larger-matrix&#34;&gt;5. Inverse of a Larger Matrix&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;For larger matrices, we can&amp;rsquo;t compute the inverse in the same way that we did for $2 \times 2$ matrices.&lt;/strong&gt; Here, we&amp;rsquo;ll employ a different strategy using &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_elimination&#34;&gt;Gaussian Elimination&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Say we wish to find the inverse of the 3x3 matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  2 &amp;amp; -1 &amp;amp; 0  \newline
  -1 &amp;amp; 2 &amp;amp; -1 \newline
  0 &amp;amp; -1 &amp;amp; 2
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start off by creating the &lt;em&gt;augmented matrix&lt;/em&gt; $[A \space \space I]$, where $I$ is the identity matrix:&lt;/p&gt;

&lt;p&gt;\[
[A \space \space I]
%
&amp;#61;
%
 \begin{bmatrix}
  2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0  \newline
  -1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We then apply Gaussian Elimination, and end up with the matrix $[I \space \space A^{-1}]$:&lt;/p&gt;

&lt;p&gt;\[
[I \space \space A^{-1}]
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{3} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{1} {4}  \newline
  0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \frac{1} {2} &amp;amp; 1 &amp;amp; \frac{1} {2}  \newline
  0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \frac{1} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{3} {4}&lt;br /&gt;
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Yielding the inverse matrix:&lt;/p&gt;

&lt;p&gt;\[
A^{-1}
%
&amp;#61;
%
 \begin{bmatrix}
  \frac{3} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{1} {4}  \newline
  \frac{1} {2} &amp;amp; 1 &amp;amp; \frac{1} {2}  \newline
  \frac{1} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{3} {4}&lt;br /&gt;
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We can then apply the same sort of calculations that we did for the $2 \times 2$ case, and test that this is indeed the inverse matrix. If you don&amp;rsquo;t believe that this is in fact the inverse, then feel free to do so :) I&amp;rsquo;m going to stop right here, though!&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>18.06</category>
      
            <category>matrices</category>
      
            <category>nullspace</category>
      
            <category>inverse</category>
      
      
    </item>
    
    <item>
      <title>Rank of a Matrix</title>
      <link>/2018/11/rank-of-a-matrix/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/rank-of-a-matrix/</guid>
      <description>&lt;p&gt;The dimension of the column space of a matrix is called its rank.&lt;/p&gt;

&lt;p&gt;rank of a matrix
10K – 100K
Low&lt;/p&gt;

&lt;p&gt;The dimension of the column space of a matrix is called its rank.
- rank of 1 - when the output of the matrix multiplication is a line (for next article)
  - rank of 2 - when all the outputs of the matrix mult land in 2-D, then matrix has a rank of 2.
  - etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;It is the number of linearly independent columns&lt;/p&gt;

&lt;p&gt;It is the number of pivots of a matrix&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>matrices</category>
      
            <category>18.06</category>
      
      
    </item>
    
    <item>
      <title>A Matrix Nullspace (Kernel) Tutorial - Finding the Nullspace</title>
      <link>/2018/03/a-matrix-nullspace-kernel-tutorial-finding-the-nullspace/</link>
      <pubDate>Thu, 01 Mar 2018 20:35:21 +0200</pubDate>
      
      <guid>/2018/03/a-matrix-nullspace-kernel-tutorial-finding-the-nullspace/</guid>
      <description>&lt;p&gt;In this article we describe &lt;strong&gt;what the nullspace is&lt;/strong&gt;. We go on to explain &lt;strong&gt;how to find the nullspace&lt;/strong&gt;.&lt;/p&gt;

&lt;!--toc--&gt;

&lt;h1 id=&#34;nullspace-null-space-or-kernel&#34;&gt;Nullspace, Null Space or Kernel?&lt;/h1&gt;

&lt;p&gt;The &lt;em&gt;nullspace&lt;/em&gt; (or &lt;em&gt;null space&lt;/em&gt;) of a matrix is also known as the &lt;em&gt;kernel&lt;/em&gt; of a matrix. &lt;strong&gt;These terms are interchangeable.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The nullspace is a &lt;strong&gt;set of vectors that when multiplied by a matrix returns $0$&lt;/strong&gt; (well, the zero vector).&lt;/p&gt;

&lt;h1 id=&#34;how-to-find-the-nullspace&#34;&gt;How To Find the Nullspace&lt;/h1&gt;

&lt;p&gt;Have a look at this matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  2 &amp;amp; 2 &amp;amp; 8 &amp;amp; 10 \newline
  3 &amp;amp; 3 &amp;amp; 10 &amp;amp; 13
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re interested in finding its nullspace: that is, all vectors such that when multiplied by our matrix $A$, return the zero vector, $ \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} ^ {T}$.&lt;/p&gt;

&lt;p&gt;I happen to know that $ \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$ is one such vector:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  2 &amp;amp; 2 &amp;amp; 8 &amp;amp; 10 \newline
  3 &amp;amp; 3 &amp;amp; 10 &amp;amp; 13
 \end{bmatrix}
%
\begin{bmatrix}
 -3 \newline
 1 \newline
 -2 \newline
 2
\end{bmatrix}
%
&amp;#61;
%
%
\begin{bmatrix}
 -3 + 1 - 4 + 6 \newline
 -6 + 2 - 16 + 20 \newline
 -9 + 3 - 20 + 26
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 0 \newline
 0 \newline
 0
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;If we set $x = \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$ , then we can write out the above equation more succinctly: $Ax = 0$&lt;/p&gt;

&lt;p&gt;We say that &lt;strong&gt;$x$ is in the nullspace of $A$.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to look at a &lt;strong&gt;systematic way to find all the vectors in the nullspace.&lt;/strong&gt;
We&amp;rsquo;ll do this is by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Using elimination to find our pivots&lt;/li&gt;
&lt;li&gt;Using back substitution to find our solutions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can think of this as our two-step recipe to find our nullspace.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-pivots&#34;&gt;Finding The Pivots&lt;/h2&gt;

&lt;p&gt;This article isn&amp;rsquo;t about how to do Gaussian elimination on a matrix. I&amp;rsquo;m going to assume that you already know how to do this. I hope to write an article about this in the future.&lt;/p&gt;

&lt;p&gt;Suppose we performed elimination on our matrix $A$, which results in our new $U$:&lt;/p&gt;

&lt;p&gt;\[
U
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  0 &amp;amp; 0 &amp;amp; 4 &amp;amp; 4 \newline
  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The cool thing is that $A$ and $U$ have the same nullspace.&lt;/strong&gt; So, if we know that $Ux = 0$, we also know that $Ax = 0$.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s suppose that we didn&amp;rsquo;t know that $x = \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$ was in the nullspace of the matrix $A$. We know that there is some vector $x = \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; x_3 &amp;amp; x_4 \end{bmatrix} ^ {T}$ such that $Ux = 0$ (even if that vector is just $\begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} ^ {T}$).&lt;/p&gt;

&lt;p&gt;Writing this out more explicitly:&lt;/p&gt;

&lt;p&gt;\[
\tag{1}
\label{1}
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  0 &amp;amp; 0 &amp;amp; 4 &amp;amp; 4 \newline
  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
 \end{bmatrix}
 %
 \begin{bmatrix}
  x_1 \newline
  x_2 \newline
  x_3 \newline
  x_4
 \end{bmatrix}
 %
  &amp;#61;
 %
 \begin{bmatrix}
  0 \newline
  0 \newline
  0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;According to our recipe for finding the nullspace, we need to find our pivots. &lt;strong&gt;A pivot is simply the first non-zero element in each row of the matrix.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Looking at $U$, the first row has a pivot in the first column, with value $1$. The second row has a pivot in the third column: $4$. The third row does not have a pivot.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We call $x_1$ and $x_3$&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;pivot variables&lt;/em&gt;&lt;/strong&gt; since columns 1 and 3 contain pivots. &lt;strong&gt;$x_2$ and $x_4$ are called&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;free variables&lt;/em&gt;&lt;/strong&gt; since columns 2 and 4 have no pivots.&lt;/p&gt;

&lt;p&gt;Note that there are 4 variables in total - 2 free variables, and 2 pivot variables. This is because the nullspace vectors are in $\mathbb{R}^4$ in this case, since $A$ (and $U$) are $3 \times 4$ matrices.&lt;/p&gt;

&lt;h2 id=&#34;using-back-substitution-to-find-our-nullspace&#34;&gt;Using Back Substitution to Find our Nullspace&lt;/h2&gt;

&lt;p&gt;Now that we know what our pivots are, we want to find our &lt;em&gt;special solutions&lt;/em&gt;. Special solutions are vectors in our nullspace which will eventually &lt;strong&gt;help us find all the vectors in our nullspace&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We will have as many special solutions as we have free variables.&lt;/strong&gt; In this case, we have 2 free variables, so we will have 2 special solutions. The way we find the special solutions is by &lt;em&gt;back substitution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Before we get to what back substition is, let&amp;rsquo;s rewrite equation $(\ref{1})$ as a set of equations, instead of its matrix form:&lt;/p&gt;

&lt;p&gt;\[
x_1 + x_2 + 2x_3 + 3x_4 = 0
\]&lt;/p&gt;

&lt;p&gt;\[
4x_3 + 4x_4 = 0
\]&lt;/p&gt;

&lt;p&gt;We want to find values for $x_1$, $x_2$, $x_3$ and $x_4$ which satisfy the above set of equations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Since $x_2$ and $x_4$ are free variables, we can give them any values we wish&lt;/strong&gt; - they are &lt;em&gt;free&lt;/em&gt; variables, after all! The simples choice for our free variables is ones or zeroes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s set $x_2 = 1$ and $x_4 = 0$:&lt;/p&gt;

&lt;p&gt;\[
x_1 + (1) + 2x_3 + 3(0) = 0
\]&lt;/p&gt;

&lt;p&gt;\[
4x_3 + 4(0) = 0
\]&lt;/p&gt;

&lt;p&gt;which gives:&lt;/p&gt;

&lt;p&gt;\[
\label{2}
\tag{2}
x_1 + 1 + 2x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;\[
\label{3}
\tag{3}
x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;Great! &lt;strong&gt;We now know that $x_3 = 0$&lt;/strong&gt; (and we have values for $x_2$ and $x_4$ ). All that&amp;rsquo;s left is for us to find $x_1$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now, let&amp;rsquo;s use back substitution to plug $x_3 = 0$ into equation ($\ref{2}$):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\[
x_1 + 1 + 2(0) = 0
\]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;p&gt;\[
x_1 = -1
\]&lt;/p&gt;

&lt;p&gt;We now have our first special solution:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3 \newline
 x_4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -1 \newline
 1 \newline
 0 \newline
 0
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Setting $x_2 = 1$ and $x_4 = 0$ gave us our first special solution. &lt;strong&gt;We need to find our second special solution.&lt;/strong&gt; The easiest way to do this is to set $x_2 = 0$ and $x_4 = 1$. Using the same procedure as we did for the first special solution we find our second special solution:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 -1 \newline
 0 \newline
 -1 \newline
 1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Since we had 2 free variables, we get 2 special solutions:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  -1 \newline
  1 \newline
  0 \newline
  0
\end{bmatrix},
\begin{bmatrix}
  -1 \newline
  0 \newline
  -1 \newline
  1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;These two special solutions are &lt;strong&gt;in the nullspace of $U$ and therefore also $A$&lt;/strong&gt;. This means that if you multiply the matrix $A$ or $U$ by these vectors, you will get the zero vector.&lt;/p&gt;

&lt;p&gt;Crucially - &lt;strong&gt;&lt;em&gt;every combination of our special solutions is in the nullspace&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So, the complete solution is:&lt;/p&gt;

&lt;p&gt;\[
x
%
&amp;#61;
%
 x_2
 %
\begin{bmatrix}
 -1 \newline
 1 \newline
 0 \newline
 0
\end{bmatrix}
%&lt;br /&gt;
+
%
 x_4
 %
\begin{bmatrix}
 -1 \newline
 0 \newline
 -1 \newline
 1
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -x_2 - x_4 \newline
 x_2 \newline
 -x_4 \newline
 x_4
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;What does this actually mean? It means that &lt;strong&gt;we can pick our free variables $x_2$ and $x_4$ however we wish, and we will get a valid solution in response.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example let&amp;rsquo;s pick: $x_2=1$ and $x_4 = 2$. So:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 -x_2 - x_4 \newline
 x_2 \newline
 -x_4 \newline
 x_4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -3 \newline
 1 \newline
 -2 \newline
 2
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;which was my example at the beginning of the article.&lt;/p&gt;

&lt;h2 id=&#34;another-example&#34;&gt;Another Example&lt;/h2&gt;

&lt;p&gt;Say now I wish to find the nullspace of&lt;/p&gt;

&lt;p&gt;\[
U
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 5 &amp;amp; 7 \newline
  0 &amp;amp; 0 &amp;amp; 9 \newline
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The second column contains our free variable. Since there is only &lt;strong&gt;one free variable, there will only be one special solution&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We need to back substitute $x_2 = 1$ (no need to set zeroes this time, as there is only one free variable).&lt;/p&gt;

&lt;p&gt;Then
\[
9x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;gives&lt;/p&gt;

&lt;p&gt;\[
x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;\[
x_1 + 5x_2 = 0
\]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;p&gt;\[
x_1 = -5
\]&lt;/p&gt;

&lt;p&gt;since&lt;/p&gt;

&lt;p&gt;\[
x_2 = 1
\]&lt;/p&gt;

&lt;p&gt;So,
\[
x
%
&amp;#61;
%
x_2
%
 \begin{bmatrix}
  -5 \newline
  1 \newline
  0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The nullspace in this case is a line in $\mathbb{R}^3$. It contains multiples of the special solution.&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>18.06</category>
      
            <category>matrices</category>
      
            <category>nullspace</category>
      
      
    </item>
    
    <item>
      <title>Of Matrices and Men (pt. 2)</title>
      <link>/2017/11/of-matrices-and-men-pt.-2/</link>
      <pubDate>Wed, 22 Nov 2017 20:35:21 +0200</pubDate>
      
      <guid>/2017/11/of-matrices-and-men-pt.-2/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;/2017/10/of-matrices-and-men-pt.-1/&#34;&gt;last post&lt;/a&gt; we saw how a matrix could operate on a vector to produce a result&amp;hellip;&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
1 \newline
3 \newline
5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The matrix used above is a $3 \times 3$ &lt;em&gt;difference matrix&lt;/em&gt;. The general form for this type of operation is:&lt;/p&gt;

&lt;p&gt;\[
Ax = b
\]&lt;/p&gt;

&lt;p&gt;From the above equation, you can see that vector $b$ is produced by applying matrix $A$ to vector $x$. This $Ax = b$ is pretty much the central equation of linear algebra.&lt;/p&gt;

&lt;p&gt;Say you now know what $b$ is and you wish to recover $x$. That is, given $Ax = b$, which is:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 b_1 \newline
 b_2 \newline
 b_3
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We assume that $b$ is known, and we now wish to retrieve $x$.
We can write out $Ax = b$ as a set of linear equations:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}
1 \cdot x_1 + 0 \cdot x_2 + 0 \cdot x_3 = b_1
\end{equation}
\]
\[
\begin{equation}
-1 \cdot x_1 + 1 \cdot x_2 + 0 \cdot x_3 = b_2
\end{equation}
\]
\[
\begin{equation}
0 \cdot x_1 -1 \cdot x_2 + 1 \cdot x_3 = b_3
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;And now solving for the $x_i$s:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}
x_1 = b_1 \qquad
\end{equation}
\]
\[
\begin{equation}
x_2 = b_1 + b_2
\end{equation}
\]
\[
\begin{equation}
\qquad x_3 = b_1 + b_2 + b_3
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;We were able to solve for the $x_i$s so easily because our matrix $A$ is &lt;em&gt;lower triangular&lt;/em&gt;. This just means that all the entries above the diagonal entries are equal to $0$. In general, we can recover a unique $x$, given a $b$, if the matrix which produced $b$ is &lt;em&gt;invertible&lt;/em&gt;. I&amp;rsquo;ll use my next post to discuss invertibility and why it was possible for us to so easily solve for the $x_i$s.
For now, just know that from any vector $b$ produced by the the difference matrix $A$, we can solve for the $x$ which led to it. Let&amp;rsquo;s try
\[
b = \begin{bmatrix} b_1 \newline b_2 \newline b_3 \end{bmatrix} = \begin{bmatrix} 1 \newline 3 \newline 5 \end{bmatrix}
\]
This will give us
\[
x = \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} = \begin{bmatrix} b_1 \qquad \qquad \newline b_1 + b_2 \qquad \newline b_1 + b_2 +b_3 \end{bmatrix}= \begin{bmatrix} 1 \newline 1 + 3 \newline 1 + 3 + 5 \end{bmatrix} = \begin{bmatrix} 1 \newline 4 \newline 9 \end{bmatrix}
\]
This is in fact correct, and should come as no surprise to anybody who has endured high school algebra.&lt;/p&gt;

&lt;p&gt;The cool part comes now.&lt;/p&gt;

&lt;p&gt;From the above, we can write $ \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} $ as follows:
\[
 \begin{bmatrix} b_1 \qquad \qquad \newline b_1 +b_2 \qquad \newline b_1 + b_2 +b_3 \end{bmatrix} = \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix}
\begin{bmatrix} b_1 \newline b_2 \newline b_3 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t worry too much about exactly &lt;em&gt;how&lt;/em&gt; we found the matrix \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix} (which we&amp;rsquo;ll call $S$), just convince yourself that it is, in fact, the correct matrix to have here. The neat thing about this matrix, is that it&amp;rsquo;s actually a &lt;em&gt;sum matrix&lt;/em&gt;; as opposed to the &lt;em&gt;difference matrix&lt;/em&gt;, which is its inverse. There is a nice duality or symmetry going on here. The inverse of the difference matrix is the sum matrix.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take the result of our initial $Ax = b$, which is
\[
b = \begin{bmatrix}
 1 \newline
 3 \newline
 5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s then apply the sum matrix to it:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 1 \newline
 3 \newline
 5
\end{bmatrix}
%
&amp;#61;
%
%
\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We get back our original $x = \begin{bmatrix} 1 \newline 4 \newline 9 \end{bmatrix}$.&lt;/p&gt;

&lt;p&gt;So, $Ax = b$, and $Sb = x$.&lt;/p&gt;

&lt;p&gt;Pretty cool, right?&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>18.06</category>
      
            <category>matrices</category>
      
      
    </item>
    
    <item>
      <title>Of Matrices and Men (pt. 1)</title>
      <link>/2017/10/of-matrices-and-men-pt.-1/</link>
      <pubDate>Sun, 22 Oct 2017 20:35:21 +0200</pubDate>
      
      <guid>/2017/10/of-matrices-and-men-pt.-1/</guid>
      <description>&lt;p&gt;One of the most crucial changes in my viewpoint came when I learned that matrices, much like functions, &lt;em&gt;act&lt;/em&gt; on inputs. Until then, a matrix was just a collection of numbers and matrix multiplication was just a mechanical set of rules used to produce another collection of numbers. I knew that the result of matrix multiplication could be a scalar, a vector or another matrix&amp;hellip; but I abandoned any further investigations.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m going to assume that the reader is familiar with the mechanics of matrix multiplication. What I&amp;rsquo;ll be talking about is how a matrix affects a vector; in this case, how a matrix transforms a vector into a different vector and that the transformation is encoded in the matrix itself.&lt;/p&gt;

&lt;p&gt;An example Prof. Strang uses in his lectures and book is the &lt;em&gt;difference matrix&lt;/em&gt;. It looks like this:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
&lt;br&gt;
Now, let&amp;rsquo;s say we have a vector: $ \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} \in  \mathbb{R}^3$.
Let&amp;rsquo;s say we wish to multiply the above matrix with this vector:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We could look at the matrix as the collection of vectors $ \begin{bmatrix} 1 \newline -1 \newline 0 \end{bmatrix}$, $ \begin{bmatrix} 0 \newline 1 \newline -1 \end{bmatrix}$ and $ \begin{bmatrix} 0 \newline 0 \newline 1 \end{bmatrix}$ and then take the linear combination of columns:
\[
x_1
%
\begin{bmatrix}
1 \newline
-1 \newline
0
\end{bmatrix}
%
+
%
x_2
%
\begin{bmatrix}
0 \newline
1 \newline
-1
\end{bmatrix}
%
+
%
x_3
%
\begin{bmatrix}
0 \newline
0 \newline
1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;This does give you the correct result. The numbers $x_1$, $x_2$ and $x_3$ multiply the columns which make up the matrix. But we want to change our point of view. We want to see the matrix as a &lt;em&gt;thing&lt;/em&gt; which somehow transforms the vector. So&amp;hellip; let&amp;rsquo;s finally look at the result of this matrix multiplication:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
x_1 \newline
x_2 - x_1 \newline
x_3 - x_2
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;There it is. Can you see why we called this matrix a difference matrix? The top difference is $x_1 - x_0 = x_1 - 0 = x_1$.
Let&amp;rsquo;s try this with an example. Let&amp;rsquo;s take a vector with $x_1 = 1$, $x_2= 4$ and $x_3 = 9$ that is:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;The result here is:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 1 - 0 \newline
 4 - 1 \newline
 9 - 4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
1 \newline
3 \newline
5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;This matrix will always behave like this - for any vector in $\mathbb{R}^3$.&lt;/p&gt;

&lt;p&gt;Seeing matrix multiplication between a matrix and a vector as a linear combination of vectors (as we did initially) is obviously correct, and yields a correct result. But it&amp;rsquo;s also important to note that matrices are things which operate on vectors. In this case, our difference matrix will take as &amp;ldquo;input&amp;rdquo; a vector and as &amp;ldquo;output&amp;rdquo; produce a new vector which contains the differences of the &amp;ldquo;input&amp;rdquo; vector. (And here I hope to stop my liberal usage of quotes).&lt;/p&gt;

&lt;p&gt;This post discussed obtaining the result of matrix multiplication. Next time I&amp;rsquo;ll talk about how to find out which combination of columns give you a certain result (and if that&amp;rsquo;s even possible). Unlike this post, I hope to write it while not jetlagged (and I apologize if this post is slightly incoherent. I&amp;rsquo;m blaming timezone differences).&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>18.06</category>
      
            <category>matrices</category>
      
      
    </item>
    
    <item>
      <title>Hi There. This is my first post. Or: Linear Algebra, bra?</title>
      <link>/2017/10/hi-there.-this-is-my-first-post.-or-linear-algebra-bra/</link>
      <pubDate>Sun, 01 Oct 2017 17:55:17 +0200</pubDate>
      
      <guid>/2017/10/hi-there.-this-is-my-first-post.-or-linear-algebra-bra/</guid>
      <description>&lt;p&gt;I need a place to document my various ideas as well as the potential results of those ideas.&lt;/p&gt;

&lt;p&gt;I also need a place where I can track my progress; that is, progress on a project I may be working on; or progress on material I&amp;rsquo;m busy studying.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll try document such things here. I figure, this way, if I commit to something, there will be some evidence of that commitment&amp;hellip; If I see evidence of it I can say: &amp;ldquo;Oh yeah, at some point I actually intended to do that!&amp;rdquo; and it&amp;rsquo;ll be harder for me to back out of that commitment. So really, I&amp;rsquo;m hoping that this blog helps keep me on track with my goals.&lt;/p&gt;

&lt;p&gt;Currently, I am busy with &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&#34;&gt;MIT&amp;rsquo;s 18.06 course on linear algebra&lt;/a&gt;. It&amp;rsquo;s an incredible course presented by &lt;a href=&#34;https://en.wikipedia.org/wiki/Gilbert_Strang&#34;&gt;Gilbert Strang&lt;/a&gt;; a man whose passion for mathematics is infectious. As an undergraduate student, I went through the motions of linear algebra mechanically, in order to obtain marks and thus progress to the next year of study. I never really internalized the &lt;em&gt;concepts&lt;/em&gt;. This is something which hindered me later in my academic career. I&amp;rsquo;m interested in machine learning and, as with many subjects, linear algebra is a cornerstone. I chose 18.06 because it doesn&amp;rsquo;t spend too much time on proving theorems; rather it aims to convey intuition and understanding by using examples and exposition. Not that proving can&amp;rsquo;t do that, just that it can be difficult to link up those abstract concepts with concrete applications. Maybe I just like Prof Strang&amp;rsquo;s style of teaching. Either way, I&amp;rsquo;ve commited to 18.06 and I&amp;rsquo;ve gone through about 60% of the material. Right now I&amp;rsquo;m on the part of the course which deals with Eigenvalues and Eigenvectors. For the next while, each post will contain something which I find interesting or particularly illuminating about the section of linear algebra I&amp;rsquo;m busy learning. It could be an example, or a concept. Either way, it&amp;rsquo;ll come from the section I&amp;rsquo;m busy with.&lt;/p&gt;

&lt;p&gt;My original aim was to complete this course by the end of the 2017 year. But, I don&amp;rsquo;t think I&amp;rsquo;ll manage to. I&amp;rsquo;m travelling to the US for two weeks at the end of October, and to Cape Town at the end of December. I won&amp;rsquo;t get much done during those periods. I&amp;rsquo;ll give myself til&amp;rsquo; the end of January 2018 to finish up. There it is. I said it. Now there is proof that I said it.&lt;/p&gt;

&lt;p&gt;Yikes.&lt;/p&gt;</description>
      
            <category>linear algebra</category>
      
            <category>18.06</category>
      
      
    </item>
    
  </channel>
</rss>