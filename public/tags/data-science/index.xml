<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-science on orrymr.com</title>
    <link>/tags/data-science/</link>
    <description>Recent content in data-science on orrymr.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Oct 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An R Pipeline for XGBoost Part I</title>
      <link>/2020/10/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters-part-i/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters-part-i/</guid>
      <description>&lt;p&gt;Contents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgimpl&#34;&gt;XGBoost - An Implementation of Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loadxplor&#34;&gt;Load And Explore The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hyperprm&#34;&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trn&#34;&gt;Training The Model: Or, how I learned to stop overfitting and love the cross-validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#makpred&#34;&gt;Making Predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conc&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/&#34;&gt;XGBoost&lt;/a&gt; is an implementation of a machine learning technique known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;gradient boosting&lt;/a&gt;.
&lt;b&gt;In this blog post, we discuss what XGBoost is, and demonstrate a pipeline for working with it in R.&lt;/b&gt; We won’t go into too much theoretical detail. Rather, we’ll focus on application.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xgimpl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; XGBoost - An Implementation of Gradient Boosting&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;Gradient boosting is part of a class of machine learning techniques known as ensemble methods.&lt;/b&gt; An ensemble method leverages the output of many &lt;u&gt;weak learners&lt;/u&gt; in order to make a prediction. Typically, these weak learners are implemented as decision trees. While each individual weak learner might not get the answer right, on average, their combined answers should be pretty decent. What makes gradient boosting different from another popular ensemble method - &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random Forest&lt;/a&gt;, is that the construction of the weak learners depend on the previously constructed learners.&lt;/p&gt;
&lt;p&gt;In gradient boosting, each weak learner is chosen iteratively in a greedy manner, so as to minimize the loss function. &lt;b&gt;XGBoost is a highly optimized implementation of gradient boosting.&lt;/b&gt; The original paper describing XGBoost can be found &lt;a href=&#34;https://arxiv.org/pdf/1603.02754.pdf&#34;&gt;here&lt;/a&gt;. Although XGBoost is written in C++, it can be interfaced from R using the &lt;code&gt;xgboost&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;To install the package:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;install.packages(&#34;xgboost&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial we use the following packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(xgboost)
library(ggplot2)
library(ggthemes)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)

theme_set(theme_economist())

set.seed(1234) # For reproducibility.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loadxplor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Load And Explore The Data&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;We will use the &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Dataset&lt;/a&gt; which we get from Kaggle.&lt;/b&gt; Basically, we try predict whether a passenger survived or not (so this is a binary classification problem).&lt;/p&gt;
&lt;p&gt;Let’s load up the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Replace directoryWhichContainsTrainingData and directoryWhichContaintsTestData 
# with your path to your training and test data, respectively.
# For example mine looks like this:
directoryWhichContainsTrainingData &amp;lt;-  &amp;quot;./xg_boost_data/train.csv&amp;quot;
directoryWhichContaintsTestData &amp;lt;- &amp;quot;./xg_boost_data//test.csv&amp;quot;

titanic_train &amp;lt;- read_csv(directoryWhichContainsTrainingData)
titanic_test &amp;lt;- read_csv(directoryWhichContaintsTestData)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of brevity (and my laziness), I’ll only keep some of the features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pclass: this refers to passenger class (1st, 2nd or 3rd)&lt;/li&gt;
&lt;li&gt;Sex&lt;/li&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Embarked: Port of Embarkation - C = Cherbourg, Q = Queenstown, S = Southampton&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll use dplyr’s &lt;code&gt;select()&lt;/code&gt; to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_train &amp;lt;- titanic_train %&amp;gt;%
  select(Survived,
         Pclass,
         Sex,
         Age,
         Embarked)

titanic_test &amp;lt;- titanic_test %&amp;gt;%
  select(Pclass,
         Sex,
         Age,
         Embarked)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at our data after discarding a few features:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train, give.attr = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [891 x 5] (S3: tbl_df/tbl/data.frame)
##  $ Survived: num [1:891] 0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass  : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex     : chr [1:891] &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age     : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...
##  $ Embarked: chr [1:891] &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;b&gt;XGBoost will only take numeric data as input.&lt;/b&gt; Let’s convert our character features to factors, and one-hot encode. Since we’re one-hot encoding, we expect our matrix to be filled with lots of zeroes - in other words, we expect it to be sparse. We will use &lt;code&gt;sparse.model.matrix()&lt;/code&gt; to create a sparse matrix which will be used as input for our model. XGBoost has been written to take advantage of sparse matrices, so we want to make sure that we’re using this feature.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Unfortunately, in using R at least, sparse.model.matrix() will drop rows which contain NA’s&lt;/b&gt; if the global option &lt;code&gt;options(&#39;na.action&#39;)&lt;/code&gt; is set to &lt;code&gt;&#34;na.omit&#34;&lt;/code&gt;. So we use a fix outlined &lt;a href=&#34;https://stackoverflow.com/questions/29732720/sparse-model-matrix-loses-rows-in-r&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;previous_na_action &amp;lt;- options(&amp;#39;na.action&amp;#39;) #store the current na.action
options(na.action=&amp;#39;na.pass&amp;#39;) #change the na.action

titanic_train$Sex &amp;lt;- as.factor(titanic_train$Sex)
titanic_train$Embarked &amp;lt;- as.factor(titanic_train$Embarked)

#create the sparse matrices
titanic_train_sparse &amp;lt;- sparse.model.matrix(Survived~., data = titanic_train)[,-1] 
titanic_test_sparse &amp;lt;- sparse.model.matrix(~., data = titanic_test)[,-1] 

options(na.action=previous_na_action$na.action) #reset the na.action&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could have just used a sentinel value to replace the NA’s.&lt;/p&gt;
&lt;div id=&#34;interacting-with-the-sparse-matrix-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Interacting with the Sparse Matrix Object&lt;/h2&gt;
&lt;p&gt;The data are in the format of a &lt;strong&gt;dgCMatrix&lt;/strong&gt; class - this is the Matrix package’s implementation of sparse matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Formal class &amp;#39;dgCMatrix&amp;#39; [package &amp;quot;Matrix&amp;quot;] with 6 slots
##   ..@ i       : int [1:3080] 0 1 2 3 4 5 6 7 8 9 ...
##   ..@ p       : int [1:6] 0 891 1468 2359 2436 3080
##   ..@ Dim     : int [1:2] 891 5
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:891] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##   .. ..$ : chr [1:5] &amp;quot;Pclass&amp;quot; &amp;quot;Sexmale&amp;quot; &amp;quot;Age&amp;quot; &amp;quot;EmbarkedQ&amp;quot; ...
##   ..@ x       : num [1:3080] 3 1 3 1 3 3 1 3 3 2 ...
##   ..@ factors : list()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the dimension of the matrix directly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 891   5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The names are the features are given by &lt;code&gt;titanic_train_sparse@Dimnames[[2]]&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(titanic_train_sparse@Dimnames[[2]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Pclass&amp;quot;    &amp;quot;Sexmale&amp;quot;   &amp;quot;Age&amp;quot;       &amp;quot;EmbarkedQ&amp;quot; &amp;quot;EmbarkedS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If needed, you can convert this data (back) into a data frame, thusly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data_as_df &amp;lt;- as.data.frame(as.matrix(titanic_train_sparse))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperprm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Hyperparameters&lt;/h1&gt;
&lt;p&gt;Tuning hyperparameters is a vast topic. Without going into too much depth, I’ll outline some of the more commonly used hyperparameters:&lt;/p&gt;
&lt;p&gt;Full reference: &lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/parameter.html&#34; class=&#34;uri&#34;&gt;https://xgboost.readthedocs.io/en/latest/parameter.html&lt;/a&gt;&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-11&#34;&gt;Table 4.1: &lt;/span&gt;Parameters
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Parameter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Explanation
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
eta
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0.3 learning rate / shrinkage. Scales the contribution of each try by a factor of 0 &amp;lt; eta &amp;lt; 1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gamma
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0 minimum loss reduction needed to make another partition in a given tree. larger the value, the more conservative the tree will be (as it will need to make a bigger reduction to split) So, conservative in the sense of willingness to split.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
max_depth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 6 max depth of each tree…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
subsample
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1 (ie, no subsampling) fraction of training samples to use in each “boosting iteration”
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
colsample_bytree
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 (ie, no sampling) Fraction of columns to be used when constructing each tree. This is an idea used in RandomForests
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min_child_weight
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 This is the minimum number of instances that have to been in a node. It’s a regularization parameter So, if it’s set to 10, each leaf has to have at least 10 instances assigned to it. The higher the value, the more conservative the tree will be.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;b&gt;Note that the above hyperparameters are in the case of the weak learner being a tree.&lt;/b&gt; It is possible to have linear models as your weak learners.&lt;/p&gt;
&lt;p&gt;Let’s create the hyper-parameters list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# booster = &amp;#39;gbtree&amp;#39;: Possible to also have linear boosters as your weak learners.
params_booster &amp;lt;- list(booster = &amp;#39;gbtree&amp;#39;, eta = 1, gamma = 0, max.depth = 2, subsample = 1, colsample_bytree = 1, min_child_weight = 1, objective = &amp;quot;binary:logistic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One day, I shall write a blog post about various ways to tune your hyperparameters. But, today is not that day. If you are like me, and believe in serendipitous machine learning, then &lt;b&gt;try a random search of the hyperparameters&lt;/b&gt; (within reason, of course. Don’t set &lt;code&gt;max.depth = 9999999999&lt;/code&gt;. Or do, I’m not telling you how to live your life). You get surprisingly decent results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trn&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Training The Model: Or, how I learned to stop overfitting and love the cross-validation&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;The &lt;code&gt;xgb.train()&lt;/code&gt; and &lt;code&gt;xgboost()&lt;/code&gt; functions are used to train the boosting model&lt;/b&gt;, and both return an object of class xgb.Booster. &lt;code&gt;xgboost()&lt;/code&gt; is a simple wrapper for &lt;code&gt;xgb.train()&lt;/code&gt;. &lt;code&gt;xgb.train()&lt;/code&gt; is an advanced interface for training the xgboost model. We’re going to use &lt;code&gt;xgboost()&lt;/code&gt; to train our model. Yay.&lt;/p&gt;
&lt;p&gt;One of the parameters we set in the &lt;code&gt;xgboost()&lt;/code&gt; function is &lt;code&gt;nrounds&lt;/code&gt; - the maximum number of boosting iterations. So, how many weak learners get added to our ensemble. If we set this parameter too low, we won’t be able to model the complexity of our dataset very well. If we set it too high, we run the risk of overfitting. &lt;b&gt;We always need to be wary of overfitting our model to our training data. &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;This leads us to the &lt;code&gt;xgb.cv()&lt;/code&gt; function. &lt;b&gt;Let’s use xgb.cv() to determine how many rounds we should use for training.&lt;/b&gt; Important to note that xgb.cv() returns an object of type xgb.cv.synchronous, not xgb.Booster. So you won’t be able to call functions like xgb.importance() on it, as xgb.importance() takes object of class xgb.Booster &lt;strong&gt;not&lt;/strong&gt; xgb.cv.synchronous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# NB: keep in mind xgb.cv() is used to select the correct hyperparams.
# Here I&amp;#39;m only looking for a decent value for nrounds; We won&amp;#39;t do full hyperparameter tuning.
# Once you have them, train using xgb.train() or xgboost() to get the final model.

bst.cv &amp;lt;- xgb.cv(data = titanic_train_sparse, 
              label = titanic_train$Survived, 
              params = params_booster,
              nrounds = 300, 
              nfold = 5,
              print_every_n = 20,
              verbose = 2)

# Note, we can also implement early-stopping: early_stopping_rounds = 3, so that if there has been no improvement in test accuracy for a specified number of rounds, the algorithm stops.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the resuts of &lt;code&gt;xgb.cv()&lt;/code&gt;, let’s plot our validation error and training error against the number of round:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_df &amp;lt;- data.frame(TRAINING_ERROR = bst.cv$evaluation_log$train_error_mean, 
                     VALIDATION_ERROR = bst.cv$evaluation_log$test_error_mean, # Don&amp;#39;t confuse this with the test data set. 
                     ITERATION = bst.cv$evaluation_log$iter) %&amp;gt;%
  mutate(MIN = VALIDATION_ERROR == min(VALIDATION_ERROR))

best_nrounds &amp;lt;- res_df %&amp;gt;%
  filter(MIN) %&amp;gt;%
  pull(ITERATION)

res_df_longer &amp;lt;- pivot_longer(data = res_df, 
                              cols = c(TRAINING_ERROR, VALIDATION_ERROR), 
                              names_to = &amp;quot;ERROR_TYPE&amp;quot;,
                              values_to = &amp;quot;ERROR&amp;quot;)

g &amp;lt;- ggplot(res_df_longer, aes(x = ITERATION)) +        # Look @ it overfit.
  geom_line(aes(y = ERROR, group = ERROR_TYPE, colour = ERROR_TYPE)) +
  geom_vline(xintercept = best_nrounds, colour = &amp;quot;green&amp;quot;) +
  geom_label(aes(label = str_interp(&amp;quot;${best_nrounds} iterations gives minimum validation error&amp;quot;), y = 0.2, x = best_nrounds, hjust = 0.1)) +
  labs(
    x = &amp;quot;nrounds&amp;quot;,
    y = &amp;quot;Error&amp;quot;,
    title = &amp;quot;Test &amp;amp; Train Errors&amp;quot;,
    subtitle = str_interp(&amp;quot;Note how the training error keeps decreasing after ${best_nrounds} iterations, but the validation error starts \ncreeping up. This is a sign of overfitting.&amp;quot;)
  ) +
  scale_colour_discrete(&amp;quot;Error Type: &amp;quot;)

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-10-04-an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
This leads us to believe that we should use 9 as the value for &lt;code&gt;nrounds&lt;/code&gt;. Let’s train our XGBoost model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bstSparse &amp;lt;- xgboost(data = titanic_train_sparse, label = titanic_train$Survived, nrounds = best_nrounds, params = params_booster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  train-error:0.207632 
## [2]  train-error:0.209877 
## [3]  train-error:0.189675 
## [4]  train-error:0.173962 
## [5]  train-error:0.163861 
## [6]  train-error:0.166105 
## [7]  train-error:0.166105 
## [8]  train-error:0.162739 
## [9]  train-error:0.156004&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;makpred&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Making Predictions&lt;/h1&gt;
&lt;p&gt;Now that we have our model, we can use it to make predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_test &amp;lt;- read_csv(directoryWhichContaintsTestData)

predictions &amp;lt;- predict(bstSparse, titanic_test_sparse)
titanic_test$Survived = predictions

titanic_test &amp;lt;- titanic_test %&amp;gt;% select(PassengerId, Survived)
titanic_test$Survived = as.numeric(titanic_test$Survived &amp;gt; 0.5) 


# write_csv(titanic_test, &amp;quot;./xg_boost_data/sb.csv&amp;quot;)
# I submitted the above to Kaggle, and got a score of 0.77751 (this is the categorization accuracy)
# No discussion of AUC, precision / recall. One day, I&amp;#39;ll blog about this as well.... Maybe&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;In this blog post we saw how to put together a pipeline to implement XGBoost. We saw how to create a sparse matrix, do cross validation, create the actual model, and finally, make predictions.&lt;/p&gt;
&lt;p&gt;There’s a lot here that I didn’t cover. Things like feature importance, for example. The plan is to write a Part II, Some Day, which goes into more detail. Really, I just intend this to be a handy reference for future me.
If you see any errors in this post, please let me know :). I’ll try fix them.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>data-science</category>
      
            <category>xgboost</category>
      
      
    </item>
    
    <item>
      <title>r-pca</title>
      <link>/2019/03/r-pca/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/r-pca/</guid>
      <description>&lt;p&gt;Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca---a-brief-introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. PCA - A Brief Introduction&lt;/h1&gt;
&lt;p&gt;PCA dates back to the early 20th century, and is used to reduce the dimensionality of data (technically, it’s a &lt;em&gt;linear&lt;/em&gt; dimensionality reduction procedure. We won’t get into what that means in this post).&lt;/p&gt;
&lt;p&gt;The reason you might want to do this is because high dimensional data might lie in a low dimensional subspace. We want to “re-state” this data in that lower dimensional space; that is, we want to get rid of those superfluous extra dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Intuition&lt;/h1&gt;
&lt;p&gt;A simple example could be collecting data about houses in your neighbourhood; you collect features such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of windows&lt;/li&gt;
&lt;li&gt;Number of bedrooms&lt;/li&gt;
&lt;li&gt;House price&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each one of these is another dimension in your dataset. Say you mistakenly collect the area of your house both in square meters &lt;em&gt;and&lt;/em&gt; in square feet. You now have 5 dimensional data, but really, it only lies in a 4 dimensional subspace.&lt;/p&gt;
&lt;p&gt;This is obviously a contrived example, but hopefully you get the picture.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Implementation in R&lt;/h1&gt;
&lt;p&gt;Let’s generate data which simulates some marks in two tests, a maths test and a physics test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1337)
mathScore &amp;lt;- runif(100) * 100
physicsScore &amp;lt;- mathScore + rnorm(100, sd = 8)

#Let&amp;#39;s make sure that there are no negative marks:
physicsScore &amp;lt;- pmax(0, physicsScore)

#And let&amp;#39;s make sure that there are no marks greater than 100:
physicsScore &amp;lt;- pmin(100, physicsScore)

allTestResults &amp;lt;- data.frame(math = mathScore, physics = physicsScore)
plot(allTestResults$math, allTestResults$physics, xlab = &amp;quot;Math&amp;quot;, ylab = &amp;quot;Physics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see from the picture above that the data are pretty correlated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(allTestResults$math, allTestResults$physics)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9725437&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe there is some underlying, latent “smartness” dimension upon which the data truly lie. Let us attempt to determine&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_results &amp;lt;- prcomp(allTestResults, center = TRUE) #Don&amp;#39;t need to scale, since both axes already on same scale, but centering is NB

print (pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Standard deviations (1, .., p=2):
## [1] 42.769762  5.045888
## 
## Rotation (n x k) = (2 x 2):
##                PC1        PC2
## math    -0.7050443  0.7091633
## physics -0.7091633 -0.7050443&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pca_results$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s scale the plot:
plot(pca_results$x, xlim = c(-60, 80), ylim = c(-60, 80))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;screeplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pov &amp;lt;- pca_results$sdev^2/sum(pca_results$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Percentage of variance explained by the first principal component: 98.6272283%&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# allTestResults[8, ]
# allTestResults[34, ]
# allTestResults[100, ]
# allTestResults[57, ]
# allTestResults[84, ]
# allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[8, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 8 28.11173 32.83384&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[34, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 34 22.91205 22.28855&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[100, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         math physics
## 100 97.10537     100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[57, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 57 14.12764 7.651867&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[84, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 84 18.4879 15.14439&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 39 29.27543 36.10048&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>pca</category>
      
            <category>data-science</category>
      
      
    </item>
    
    <item>
      <title>An Introduction to H2O Using R</title>
      <link>/2018/12/intro-to-h2o-using-r/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/intro-to-h2o-using-r/</guid>
      <description>&lt;!-- toc --&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In this post we discuss the &lt;a href=&#34;https://www.h2o.ai/&#34;&gt;H2O machine learning platform.&lt;/a&gt; We talk about what H2O is, and how to get started with it, using R - we create a Random Forest which we use to classify the Iris Dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-h2o&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. What is H2O?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The definition found on &lt;a href=&#34;https://github.com/h2oai/h2o-3&#34;&gt;H2O’s Github page&lt;/a&gt; is a lot to take in,&lt;/strong&gt; especially if you’re just starting out with H2O: “H2O is an in-memory platform for distributed, scalable machine learning. H2O uses familiar interfaces like R, Python, Scala, Java, JSON and the Flow notebook/web interface, and works seamlessly with big data technologies like Hadoop and Spark.”&lt;/p&gt;
&lt;p&gt;We spend the rest of section 2 as well as section 3 discussing salient points of the above definition.&lt;/p&gt;
&lt;div id=&#34;h2o-is-an-in-memory-platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 H2O is an In-Memory Platform&lt;/h2&gt;
&lt;p&gt;H2O is an “in-memory platform”. &lt;strong&gt;Saying that it’s in-memory means that the data being used is loaded into main memory (RAM).&lt;/strong&gt; Reading from main memory, (also known as primary memory) is typically much faster than secondary memory (such as a hard drive).&lt;/p&gt;
&lt;p&gt;H2O is a “platform.” &lt;strong&gt;A platform is software which can be used to build something - in this case, machine learning models.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Putting this togther we now know that H2O is an in-memory environment for building machine learning models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h2o-is-distributed-and-scalable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 H2O is Distributed and Scalable&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;H2O can be run on a cluster.&lt;/strong&gt; &lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt; is an example of a cluster which can run H2O.&lt;/p&gt;
&lt;p&gt;H2O is said to be distributed because your object can be spread amongst several nodes in your cluster. H2O does this by using a Distributed Key Value (DKV). You can read more about it &lt;a href=&#34;https://www.h2o.ai/blog/h2o-architecture/&#34;&gt;here&lt;/a&gt;, but essentially what this means, is that &lt;strong&gt;any object you create in H2O can be distributed amongst several nodes in the cluster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The key-value part of DKV means that when you load data into H2O, you get back a key into a hashmap containing your (potentially distributed) object.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-h2o-runs-under-the-hood&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. How H2O Runs Under the Hood&lt;/h1&gt;
&lt;p&gt;We spoke earlier about H2O being a platform. &lt;strong&gt;It’s important to distinguish between the R interface for H2O, and H2O itself.&lt;/strong&gt; H2O can exist perfectly fine without R. H2O is just a &lt;a href=&#34;https://en.wikipedia.org/wiki/JAR_(file_format)&#34;&gt;.jar&lt;/a&gt; which can be run on its own. If you don’t know (or particularly care) what a .jar is - just think of it as Java code packaged with all the stuff you need in order to run it.&lt;/p&gt;
&lt;p&gt;When you start H2O, you actually create a server which can respond to &lt;a href=&#34;https://en.wikipedia.org/wiki/Representational_state_transfer&#34;&gt;REST&lt;/a&gt; calls. Again, you don’t really need to know how REST works in order to use H2O. But if you do care, just know that &lt;strong&gt;you can use any HTTP client to speak with an H2O instance.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;R is just a client interfact for H2O. All the R functions you call when working with H2O are actually calling H2O using a REST API (a JSON POST request) under the hood. The Python H2O library, as well as the &lt;a href=&#34;https://www.h2o.ai/blog/introducing-flow/&#34;&gt;Flow UI&lt;/a&gt;, interface with H2O in a similar way. &lt;strong&gt;If this is all very confusing just think about it like this: you use R to send commands to H2O. You could equally well just use Flow or Python to send commands.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-an-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Running An Example&lt;/h1&gt;
&lt;div id=&#34;installing-h2o&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.1 Installing H2O&lt;/h2&gt;
&lt;p&gt;You can install H2O using R: &lt;code&gt;install.packages(&amp;quot;h2o&amp;quot;)&lt;/code&gt;. If you’re having trouble with this, &lt;a href=&#34;http://h2o-release.s3.amazonaws.com/h2o/rel-xia/2/index.html&#34;&gt;have a look here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-h2o-and-loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.2 Starting H2O and Loading Data&lt;/h2&gt;
&lt;p&gt;First we’ll need to load the packages we’ll be using: &lt;code&gt;h2o&lt;/code&gt; and &lt;code&gt;datasets&lt;/code&gt;. We load the latter as we’ll be using the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris Dataset&lt;/a&gt;, which is part of the &lt;code&gt;datasets&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
library(h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Iris Dataset contains attributes of three species of iris flowers.&lt;/p&gt;
&lt;p&gt;Let’s load the iris dataset, and start up our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init(nthreads = -1) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in h2o.clusterInfo(): 
## Your H2O cluster version is too old (1 year, 7 months and 4 days)!
## Please download and install the latest version from http://h2o.ai/download/&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, H2O starts up using 2 cores. By calling &lt;code&gt;h2o.init(nthreads = -1)&lt;/code&gt;, with &lt;code&gt;nthreads = -1&lt;/code&gt;, we use all available cores.&lt;/p&gt;
&lt;p&gt;Edit: it doesn’t default to two cores anymore (as per this tweet from H2O’s chief ML scientist):&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Nice post! BTW, H2O in R no longer defaults to 2 cores, so you can just do &lt;code&gt;h2o.init()&lt;/code&gt; now. :-)
&lt;/p&gt;
— Erin LeDell (&lt;span class=&#34;citation&#34;&gt;@ledell&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/ledell/status/1075444885296168962?ref_src=twsrc%5Etfw&#34;&gt;December 19, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;If &lt;code&gt;h2o.init()&lt;/code&gt; was succesful, you should have an instance of H2O running locally! You can verify this by navigating to &lt;a href=&#34;http://localhost:54321&#34; class=&#34;uri&#34;&gt;http://localhost:54321&lt;/a&gt;. There, you should see the Flow UI.&lt;/p&gt;
&lt;p&gt;The iris dataset is now loaded into R. However, it’s not yet in H2O. Let’s go ahead and load the iris data into our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex &amp;lt;- as.h2o(iris)
h2o.ls()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.ls()&lt;/code&gt; lists the dataframes you have loaded into H2O. Right now, you should see only one: iris.&lt;/p&gt;
&lt;p&gt;Let’s start investigating this dataframe. We can get the summary statistics of the various columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use H2O to plot histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.hist(iris.hex$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/2018-12-04-r-h2o-introduction_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can use familiar R syntax to modify your H2O dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- iris.hex$Sepal.Length + 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now run &lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;, we should see this extra variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
## 6          foo real       0     0      0      0 6.3 9.9 7.843333 0.8280661
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3
## 6          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(What I still don’t understand is why we don’t see this extra column from the Flow UI. If anyone knows, please let me know in the comments!)&lt;/p&gt;
&lt;p&gt;But we don’t really need this nonsense column, so let’s get rid of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get our dataframe back into R, from H2O:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_df &amp;lt;- as.data.frame(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.3 Building a Model&lt;/h2&gt;
&lt;p&gt;We’ve got our H2O instance up and running, with some data in it. Let’s go ahead and do some machine learning - let’s implement a Random Forest.&lt;/p&gt;
&lt;p&gt;First off, we’ll split our data into a training set and a test set. I’m not going to explicitly set a validation set, as the algorithm will use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-bag_error&#34;&gt;out of bag error&lt;/a&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits &amp;lt;- h2o.splitFrame(data = iris.hex,
                         ratios = c(0.8),  #partition data into 80% and 20% chunks
                         seed = 198)

train &amp;lt;- splits[[1]]
test &amp;lt;- splits[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.splitFrame()&lt;/code&gt; uses approximate splitting. That is, it won’t split the data into an exact 80%-20% split. Setting the seed allows us to create reproducible results.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;h2o.nrow()&lt;/code&gt; to check the number of rows in our train and test sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in train set: &amp;quot;, h2o.nrow(train)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in train set: 117&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in test set: &amp;quot;, h2o.nrow(test)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in test set: 33&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s call &lt;code&gt;h2o.randomForest()&lt;/code&gt; to create our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf &amp;lt;- h2o.randomForest(x = c(&amp;quot;Sepal.Length&amp;quot;, &amp;quot;Sepal.Width&amp;quot;, &amp;quot;Petal.Length&amp;quot;, &amp;quot;Petal.Width&amp;quot;),
                    y = c(&amp;quot;Species&amp;quot;),
                    training_frame = train,
                    model_id = &amp;quot;our.rf&amp;quot;,
                    seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; specify our independent and dependent variables, respectively. The &lt;code&gt;training_frame&lt;/code&gt; specified the training set, and &lt;code&gt;model_id&lt;/code&gt; is the model name, within H2O (not to be confused with variable &lt;code&gt;rf&lt;/code&gt; in the above code - &lt;code&gt;rf&lt;/code&gt; is the R handle; whereas &lt;code&gt;our.rf&lt;/code&gt; is what H2O calls the model). &lt;code&gt;seed&lt;/code&gt; is used for reproducibility.&lt;/p&gt;
&lt;p&gt;We can get the model details simply by printing out the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(rf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model Details:
## ==============
## 
## H2OMultinomialModel: drf
## Model ID:  our.rf 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth
## 1              50                      150               18940         1
##   max_depth mean_depth min_leaves max_leaves mean_leaves
## 1         7    3.26000          2         12     5.41333
## 
## 
## H2OMultinomialMetrics: drf
## ** Reported on training data. **
## ** Metrics reported on Out-Of-Bag training samples **
## 
## Training Set Metrics: 
## =====================
## 
## Extract training frame with `h2o.getFrame(&amp;quot;RTMP_sid_b2ea_7&amp;quot;)`
## MSE: (Extract with `h2o.mse`) 0.03286954
## RMSE: (Extract with `h2o.rmse`) 0.1812996
## Logloss: (Extract with `h2o.logloss`) 0.09793089
## Mean Per-Class Error: 0.0527027
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;,train = TRUE)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error      Rate
## setosa         40          0         0 0.0000 =  0 / 40
## versicolor      0         38         2 0.0500 =  2 / 40
## virginica       0          4        33 0.1081 =  4 / 37
## Totals         40         42        35 0.0513 = 6 / 117
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;,train = TRUE)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.948718
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That seems pretty good. But let’s see how the model performs on the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_perf1 &amp;lt;- h2o.performance(model = rf, newdata = test)
print(rf_perf1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## H2OMultinomialMetrics: drf
## 
## Test Set Metrics: 
## =====================
## 
## MSE: (Extract with `h2o.mse`) 0.05806405
## RMSE: (Extract with `h2o.rmse`) 0.2409648
## Logloss: (Extract with `h2o.logloss`) 0.1708688
## Mean Per-Class Error: 0.1102564
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa         10          0         0 0.0000 = 0 / 10
## versicolor      0          9         1 0.1000 = 1 / 10
## virginica       0          3        10 0.2308 = 3 / 13
## Totals         10         12        11 0.1212 = 4 / 33
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.878788
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s use our model to make some predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- h2o.predict(rf, test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predict    setosa versicolor  virginica
## 1  setosa 0.9969698          0 0.00303019
## 2  setosa 0.9969698          0 0.00303019
## 3  setosa 0.9969698          0 0.00303019
## 4  setosa 0.9969698          0 0.00303019
## 5  setosa 0.9969698          0 0.00303019
## 6  setosa 0.9969698          0 0.00303019
## 
## [33 rows x 4 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;This post discussed what H2O is, and how to use it from R. The full code used in this post can be found &lt;a href=&#34;./h2o_intro.R&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>h2o</category>
      
            <category>data-science</category>
      
            <category>machine-learning</category>
      
            <category>R</category>
      
            <category>data-science</category>
      
      
    </item>
    
  </channel>
</rss>