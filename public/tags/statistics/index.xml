<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on orrymr.com</title>
    <link>/tags/statistics/</link>
    <description>Recent content in statistics on orrymr.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 16 Sep 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bayesian Statistics the Fun Way (My Notes)</title>
      <link>/2020/09/bayesian-statistics-the-fun-way-my-notes/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/bayesian-statistics-the-fun-way-my-notes/</guid>
      <description>&lt;p&gt;These notes are by no means (or medians) comprehensive. These are just key points in the book that I want to document and ultimately remember. I’m sure I missed some important points, but hey, I probably picked up on some important ones too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch1&#34;&gt;Chapter 1 - Bayesian Thinking and Everyday Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch2&#34;&gt;Chapter 2 - Measuring Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch3&#34;&gt;Chapter 3 - The Logic of Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch4&#34;&gt;Chapter 4 - Creating a Binomial Probability Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch5&#34;&gt;Chapter 5 - The Beta Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch6&#34;&gt;Chapter 6 - Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Distinction made between Frequentists and Bayesians.
&lt;u&gt;Frequentists&lt;/u&gt; - probability represents frequency.
&lt;u&gt;Bayesian&lt;/u&gt; - probability represents how uncertain we are about a piece of information.&lt;/p&gt;
&lt;p&gt;The point is made that for coin tosses, each approach seems reasonable, but for “one-offs”, like elections, Bayesian approach makes more sense; The example of looking at probabilities associated with election results (for a given year) - Viewing the probability from a Bayesian perspective tells you about your uncertainty regarding who will win. From a Frequentist perspective, you’re making a comment about how frequently a candidate wins the 2020 election… which seems weird.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ch1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Bayesian Thinking and Everyday Reasoning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Discussion about the Bayesian process, of updating your beliefes given more data.&lt;/li&gt;
&lt;li&gt;When thinking of hypotheses in Bayesian stats, we are usually concerned about how well they predict the data we observe.&lt;/li&gt;
&lt;li&gt;The true heart of Bayesian analysis: &lt;i&gt;the test of your beliefs is how well they explain the real world.&lt;/i&gt;&lt;/li&gt;
&lt;li&gt;Distinction made between &lt;span class=&#34;math inline&#34;&gt;\(P(D|H,X)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(H| D, X)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; your hypothesis, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is your data, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is your experience of the world. I’m going to assume that your experience of the world is implied, so really we are talking about the distinction between &lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt;. This point is a little tricky for me to get my head around. In the &lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt; case, we change our beliefs according to the data we gather. I guess it’s kind of like: if &lt;span class=&#34;math inline&#34;&gt;\(P(D|H_1)\)&lt;/span&gt; &amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(P(D|H_2)\)&lt;/span&gt;, then pick &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; as your hypothesis. So, we check how well the data makes sense, given a bunch of hypotheses. Quote the book: “The data we observe is all that is real, so our beliefs ultimately need to shift until they align with the data”. Consider the other formulation: &lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt; - we’re basically saying, “probability of my beliefs (&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;), given the data.” Or, how well does what I observe support what I believe? Seems kind of confirmation bias-y? Not sure.&lt;/li&gt;
&lt;li&gt;So, it’s a question of “How well does what I observe support what I believe (&lt;span class=&#34;math inline&#34;&gt;\(P(H|D)\)&lt;/span&gt;)” vs “How well does what I believe support what I observe (&lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt;)”. Great, now I have a headache. But seriously, the latter case seems more amenable to changing beliefs, while the former to changing data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Measuring Uncertainty&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The previous chapter was more conceptual (where we just thumb sucked probabilities, like the probability of seeing an alien is “very low”), but in this chapter, try to actually quantify&lt;/li&gt;
&lt;li&gt;Some axiomatic stuff about probability - like should add to 1, etc…&lt;/li&gt;
&lt;li&gt;Counting outcomes of events - combinatorics.&lt;/li&gt;
&lt;li&gt;Counting outcomes good for things like poker, coin tosses, etc, but what about things like “what’s the probabilty it’ll rain tomorrow?”, “Is that a UFO?”&lt;/li&gt;
&lt;li&gt;Using odds to solve above problem. Say you don’t believe that a certain article exists on Wikipedia, but your annoying friend does. You reckon it’s so unlikely, you’ll give the schmuck \$100 if the article exists, and he’ll give you \$5 if it does. &lt;span class=&#34;math inline&#34;&gt;\(\frac{100}{5} = 20\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(P(H_{no\_article}) = 20 \times P(H_{article})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;$P(H_{no_article}) = 20 (1- P(H_{no_article})) $&lt;/li&gt;
&lt;li&gt;$P(H_{no_article}) = 20 - 20 P(H_{no_article})) $&lt;/li&gt;
&lt;li&gt;so &lt;span class=&#34;math inline&#34;&gt;\(P(H_{no\_article}) = \frac{20}{21}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;In general &lt;span class=&#34;math inline&#34;&gt;\(P(H) = \frac{O(H)}{1 + O(H)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; is odds.&lt;/li&gt;
&lt;li&gt;This chapter explored 2 different twpes of probabilities: those of events and those of beliefs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. The Logic of Uncertainty&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;“AND”, leads to product rule &lt;span class=&#34;math inline&#34;&gt;\(P(A, B) = P(A) \times P(B)\)&lt;/span&gt; (note, there is no discussion of independence yet…)&lt;/li&gt;
&lt;li&gt;“OR”, &lt;span class=&#34;math inline&#34;&gt;\(P(A or B) = P(A) + P(B) - P(A \cap B)\)&lt;/span&gt;, keeping in mind &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B)\)&lt;/span&gt; will be zero when A, B mutually exclusive&lt;/li&gt;
&lt;li&gt;Nice example to illustrate this point. Say you get pulled over by the cops… You need both your registration and insurance card. You’re confident that you’ve got registration, so &lt;span class=&#34;math inline&#34;&gt;\(P(registration) = 0.7\)&lt;/span&gt;, not so confident about having insurance card so: &lt;span class=&#34;math inline&#34;&gt;\(P(insurace) = 0.2\)&lt;/span&gt;.
So, &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}) = 0.3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{ins}) = 0.8\)&lt;/span&gt;. You are worried that &lt;u&gt;either&lt;/u&gt; is missing, so use the sum rule (this is an “OR” case…) Then get: &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}) + P(Missing_{ins}) = 1.1\)&lt;/span&gt;. Great, we fucking broke statistics. But wait… are these events mutually exclusive? Does the occurence of one mean the other cannot occur? Hell no! So, we need to subtract last term… &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}, Missing_{ins})\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0.3 \times 0.8 = 0.24\)&lt;/span&gt;, so final result &lt;span class=&#34;math inline&#34;&gt;\(0.86\)&lt;/span&gt;. Note, when calculating &lt;span class=&#34;math inline&#34;&gt;\(P(Missing_{reg}, Missing_{ins})\)&lt;/span&gt; we are assuming that they are independent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Creating a Binomial Probability Distribution&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In this chapter create our first probability distribution.&lt;/li&gt;
&lt;li&gt;That is, a way of describing all possible events and the probability of each one happening.&lt;/li&gt;
&lt;li&gt;The “bi” part refers to 2 possible outcomes. If more than 2, then distribution is called multinomial.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B(k; n, p)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, total number of outcomes we care about, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; total number of trials, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, probability of the event happening.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B(k; n, p) = {n\choose k} \times p^k \times (1-p)^{n-k}\)&lt;/span&gt; - this is the PMF (the pprobability mass function).&lt;/li&gt;
&lt;li&gt;The binomial coefficient part of the above pmf is there to account for the number of ways you could choose k successes from n trials.&lt;/li&gt;
&lt;li&gt;For example, if looking at 2 heads in 5 coin tosses, then &lt;span class=&#34;math inline&#34;&gt;\(5 \choose 2\)&lt;/span&gt; is the number of ways this could happen. Could be HHTTT, HTHTT, etc…&lt;/li&gt;
&lt;li&gt;Exmaple - &lt;i&gt;Gacha Games&lt;/i&gt;. Purchase virtual cards with in-game currency.&lt;/li&gt;
&lt;li&gt;Get a card of Bayes with p = 0.00721, Jaynes with p = 0.00720, etc… want a Jaynes card.&lt;/li&gt;
&lt;li&gt;Cost 1 Bayes Buck to pull a card, can purchase 100 Bayes Bucks for \$10. Willing to buy this if you have an even chance of pulling the card you want, namely Jaynes (p = 0.00720),&lt;/li&gt;
&lt;li&gt;Plug into above PMF : &lt;span class=&#34;math inline&#34;&gt;\({100 \choose 1} \times 0.0072^1 \times (1-0.0072)^{99}\)&lt;/span&gt;. But, this is only for getting exactly 1 Jaynes card.&lt;/li&gt;
&lt;li&gt;We need &lt;span class=&#34;math inline&#34;&gt;\(\sum^{100}_{k = 1} \times 0.0072 \times (1 - 0.0072)^{n-k}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We ain’t gonna do that by hand. If only we had some sort of device that could &lt;em&gt;compute&lt;/em&gt; this for us. A computer of sorts.&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;pbinom()&lt;/code&gt; function, can use &lt;code&gt;pbinom(0, 100, 0.0072, lower.tail = FALSE)&lt;/code&gt; = 0.5145138. SO BUY THE BAYES BUCKS.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pbinom(0, 100, 0.0072, lower.tail = TRUE) + pbinom(0, 100, 0.0072, lower.tail = FALSE) = 1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;lower.tail&lt;/code&gt; is &lt;code&gt;TRUE&lt;/code&gt;, sums up all probs less than or equal to first argument.&lt;/li&gt;
&lt;li&gt;When &lt;code&gt;lower.tail&lt;/code&gt; is &lt;code&gt;FALSE&lt;/code&gt;, it sums up the probs scrictly greater than first argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. The Beta Distribution&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Use beta distribution to estimtate the probability of an event for which you’ve already observed a number of trials and the number of successful outcomes.&lt;/li&gt;
&lt;li&gt;For example, you would use it to estimate the probability of flipping a heads when so far you’ve observed 100 tosses of a coint, and 40 of those were heads.&lt;/li&gt;
&lt;li&gt;This chapter also discusses the difference between probability, statistics and inference.&lt;/li&gt;
&lt;li&gt;Division between probability and statistics - with probabilities, you have the exact figure which you use as the probability. With probability, what we’re concerned with is how likely observations are. For example, could be a 0.5 chance of getting heads - what’s the probability of getting 7 heads out of 20?&lt;/li&gt;
&lt;li&gt;In statistics, look at this problem backwards - assuming you observe 7 heads in 20 coin tosses, what is the probability of getting heads in a single coin toss?&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Inference&lt;/i&gt; is the task of figuring out probabilities given the data.&lt;/li&gt;
&lt;li&gt;Example: black box. Put a quarter in, sometimes 2 quarters come out, sometimes it just eats your quarter. What’s the probability of getting 2 quarters? IS it 50-50? Is it something else?&lt;/li&gt;
&lt;li&gt;Try 41 quarters, get 14 wins, 27 losses.&lt;/li&gt;
&lt;li&gt;Do we say &lt;span class=&#34;math inline&#34;&gt;\(H_1: P(two coins) = \frac{1}{2}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(H_2: P(two coins) = \frac{14}{41}\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;We can use the binomial distribution for this:&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(D | H_1) = B(14;41,\frac{1}{2}) \approx 0.016\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(D | H_2) = B(14;41,\frac{14}{41}) \approx 0.130\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;So, &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; is almost 10x more likely, though neither is impossible.&lt;/li&gt;
&lt;li&gt;We could also pick different probabilities to test:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601482159/bayesian_stats_fun_way/bin_at_diff_p_wdgxwv.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doing this at a finer grain:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601483909/bayesian_stats_fun_way/bin_at_diff_p_finer_bzahpc.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Formalize this notion with the beta distribution.&lt;/li&gt;
&lt;li&gt;Beta distribution has a probability density function (pdf) rather than pmf (like binomial), because beta is continuous.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601484058/bayesian_stats_fun_way/beta_tm4fgk.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Beta Distribution&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; - probability of an event. Corresponds to our different hypotheses for the possible probabilities of our black box.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; - How many times we observe an event we care about. Like getting 2 quarters in our black box.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; - How many times the event we care about didn’t happen. Like number of times black box ate a quarter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;denominator - beta function, to normalize.
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1601484310/bayesian_stats_fun_way/beta_graph_sb87gj.png&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot shows that it’s very unlikely the black box will return 2 quarters at least half the time, our break even point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we want to quantify, we need to integrate: &lt;code&gt;integrate(function(p) dbeta(p, 14, 27), 0, 0.5)&lt;/code&gt; -&amp;gt; 0.9807613 with absolute error &amp;lt; 5.9e-06&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Absolute error just because computers can’t perfectly calculate integrals, usually leads to a very, negligible error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This result tells us that, given our evidence, there is 0.98 probability that the true probability of getting two coinds out of the black box is less than 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We mostly never know true probabilities for events - that’s why beta distribution is one of the most powerful tools for understanding our data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ch6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Conditional Probability&lt;/h1&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
      
    </item>
    
    <item>
      <title>Demonstrating The Central Limit Theorem in R</title>
      <link>/2020/08/demonstrating-the-central-limit-theorem-in-r/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/demonstrating-the-central-limit-theorem-in-r/</guid>
      <description>&lt;p&gt;In this post we&#39;ll talk about what the Central Limit Theorem is, why it&#39;s important, and how we can see it in action, using R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries used in this article:
library(ggplot2)
library(ggthemes)
library(stringr)

# Theme used for graphing:
theme_set(theme_economist())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Table of Contents&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction - What is the Central Limit Theorem?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why&#34;&gt;Why is it important?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#action&#34;&gt;Let&#39;s see it in action&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction---what-is-the-central-limit-theorem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction - What is the Central Limit Theorem? &lt;a name=&#34;introduction&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The normal distribution is famous and important because many natural phenomena are thus distributed. Heights, weight, and IQ scores are all distributed according to this bell shaped curve:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But the normal distribution (or Gaussian distribution as it is referred to by statisticians without lisps), is important for another reason: the distributions of means are themselves normally distributed! The means of samples, that is. And here&#39;s the kicker - the original population can be distributed in any which way. The original population need not be normally distributed for its sample means to be normally distributed.&lt;/p&gt;
&lt;p&gt;Let&#39;s try make this less abstract. Suppose I have a die (as in the singular of dice). Each time I roll it, I can expect to see a value between &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;. The mean value, that is, the sum of the values multiplied by the probability of seeing each value is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Mean Value = \frac{1}{6} \times 1 + \frac{1}{6} \times 2 + \frac{1}{6} \times 3 + \frac{1}{6} \times 4 + \frac{1}{6} \times 5 + \frac{1}{6} \times 6 = 3.5\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s run this bit of code to check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_value &amp;lt;- sum(seq(1:6) * (1 / 6))
print(mean_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, not very interesting, I admit. Let&#39;s now use two dice. If the first die lands a &lt;code&gt;3&lt;/code&gt;, and the second die lands a &lt;code&gt;1&lt;/code&gt;, then the mean value will be &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2} \times 1 + \frac{1}{2} \times 3 = 2\)&lt;/span&gt;, wouldn&#39;t you agree? We can create a matrix which will give us the mean value for any combination of dice rolls (the values in the square brackets, [], refer to the values obtained from each die):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]  1.0  1.5  2.0  2.5  3.0  3.5
## [2,]  1.5  2.0  2.5  3.0  3.5  4.0
## [3,]  2.0  2.5  3.0  3.5  4.0  4.5
## [4,]  2.5  3.0  3.5  4.0  4.5  5.0
## [5,]  3.0  3.5  4.0  4.5  5.0  5.5
## [6,]  3.5  4.0  4.5  5.0  5.5  6.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The above code was modified from &lt;a href=&#34;https://rpubs.com/careybaldwin/346995&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In the above matrix, the average score of the two dice ranges from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;6&lt;/code&gt;. Importantly, we only see the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; &lt;em&gt;once&lt;/em&gt; in the above matrix. This is because the only way to get an average score of &lt;code&gt;1&lt;/code&gt; is to roll a &lt;code&gt;1&lt;/code&gt; on our first die, and to roll a &lt;code&gt;1&lt;/code&gt; on our second die as well. There is no other way to get an average score of &lt;code&gt;1&lt;/code&gt;. Similarly, getting an average score of &lt;code&gt;6&lt;/code&gt; means we have to roll a &lt;code&gt;6&lt;/code&gt; on both dice, yielding an average score of &lt;span class=&#34;math inline&#34;&gt;\(\frac{6 + 6}{2} = 6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Compare this with an average score of &lt;code&gt;1.5&lt;/code&gt;. There are two ways to obtain this score. We could roll a &lt;code&gt;1&lt;/code&gt; on our first die and a &lt;code&gt;2&lt;/code&gt; on our second die, &lt;em&gt;or&lt;/em&gt; we could roll a &lt;code&gt;2&lt;/code&gt; on our first die and a &lt;code&gt;1&lt;/code&gt; on our second die! So, there are more ways to get a &lt;code&gt;1.5&lt;/code&gt; than to get a &lt;code&gt;1&lt;/code&gt; (look at how many times &lt;code&gt;1.5&lt;/code&gt; appears in the above matrix compared to &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In fact, the probability of getting a certain value tends to &amp;quot;swell&amp;quot; as we get closer to &lt;code&gt;3.5&lt;/code&gt; (which is the true mean of the population). Just look at the diagonal entries of the matrix: &lt;code&gt;3.5&lt;/code&gt; appears six times in our matrix, compared to the measly two times &lt;code&gt;1.5&lt;/code&gt; or &lt;code&gt;5.5&lt;/code&gt; appear.&lt;/p&gt;
&lt;p&gt;Ok, let&#39;s hold this thought for now.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-it-important&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Why is it important? &lt;a name=&#34;why&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Hopefully we&#39;re starting to get a feel for what this Central Limit Theorem is trying to tell us. From the above, we know that when we roll a die, the average score over the long run will be &lt;code&gt;3.5&lt;/code&gt;. Even though &lt;code&gt;3.5&lt;/code&gt; isn&#39;t an actual value that appears on the die&#39;s face, over the long run if we took the average of the values from multiple rolls, we&#39;d get very close to &lt;code&gt;3.5&lt;/code&gt;. Let&#39;s try:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;number_of_rolls &amp;lt;- 1000000 # Or, we could manually roll a die 1 million times. Nah, let&amp;#39;s let the computer do it.
mean(sample(1:6, number_of_rolls, replace = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.497842&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we looked at the average score when we rolled two dice, we saw that getting a mean score of &lt;code&gt;3.5&lt;/code&gt; was most likely. And this trend continues, as we add more dice. What&#39;s amazing here, is that the underlying distribution is &lt;em&gt;not normal&lt;/em&gt;, it&#39;s &lt;em&gt;uniform&lt;/em&gt;: there&#39;s a &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{6}\)&lt;/span&gt; chance of seeing each face of a particular die. But the means of the samples &lt;em&gt;are&lt;/em&gt; normally distributed. We&#39;ll see this effect in action in the next section, so don&#39;t stress too much if you don&#39;t fully grasp this yet.&lt;/p&gt;
&lt;p&gt;The fact that means are normally distributed lets us understand why we can test for differences in means between groups using t-tests (or Z-tests if we have enough information). Essentially, this is because we can ask if an observed mean is significantly different to what we&#39;d expect to have seen under the null hypothesis. Under the null hypothesis, we make a statement about what the mean of a population is equal to. We then observe a sample mean. Since these sample means are normally distributed we can ask about the probability is of having seen our observed sample mean. If it&#39;s super unlikely to have seen that sample mean, we can reject our null (not accept the alternative, just reject the null).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-see-it-in-action&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Let&#39;s see it in action &lt;a name=&#34;action&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Enough talk, time for code and pictures.&lt;/p&gt;
&lt;p&gt;We know that when rolling a fair single die, we will see a value between 1 and 6 with probability &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{6}\)&lt;/span&gt;. We can simulate rolling dice with this piece of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#roll, as in roll di(c)e
# m = number of times
# n = number of dice

roll &amp;lt;- function(m, n){
  set.seed(1234)
  means &amp;lt;- plyr::ldply(1:m, function(x){
    return(mean(sample(1:6, n, replace = TRUE)))
  }) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can call the function &lt;code&gt;roll()&lt;/code&gt; to simulate rolling a single die &lt;code&gt;10000&lt;/code&gt; times as follows: &lt;code&gt;roll(10000, 1)&lt;/code&gt;. We can then draw a bar graph of our observed values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 1
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  )

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above shouldn&#39;t be too surprising: we observed each value more or less the same proportion of the time.&lt;/p&gt;
&lt;p&gt;Let&#39;s slap a &lt;code&gt;geom_density()&lt;/code&gt; on top of this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- g +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Don&#39;t worry about the density function not going all the way down to &lt;code&gt;0&lt;/code&gt; in the space between the values &lt;code&gt;1, 2, 3, 4, 5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;: this is just due to the curve being smoothed. Really, this is putting a continuous density function over something which should be discrete.&lt;/p&gt;
&lt;p&gt;Technically, we have just produced &lt;code&gt;10000&lt;/code&gt; samples of size &lt;code&gt;1&lt;/code&gt; (we rolled &lt;code&gt;1&lt;/code&gt; die &lt;code&gt;10000&lt;/code&gt; times). Let&#39;s produce &lt;code&gt;10000&lt;/code&gt; samples of size &lt;code&gt;2&lt;/code&gt; (roll &lt;code&gt;2&lt;/code&gt; dice &lt;code&gt;10000&lt;/code&gt; and plot the means of these samples):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 2
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above is starting to look a bit more bell-shaped. And it&#39;s centered around the vertical red line, which has the x-intercept of &lt;code&gt;3.5&lt;/code&gt; - the true population mean.&lt;/p&gt;
&lt;p&gt;Let&#39;s increase our sample size to 4:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 4
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not only is the curve starting to look more normal, but the tails of the distribution are starting to flatten out: that is, the extreme mean values of &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; have become less probable.&lt;/p&gt;
&lt;p&gt;Let&#39;s increase the sample size to 20:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 20
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  lims(
    y = c(0, 1.2)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we see the classic bell shaped curve. Notice that the peak of the distribution is taller than in the previous graph (the y-axis runs from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;1.25&lt;/code&gt; in this graph as opposed to &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; in the previous). The extreme mean values around &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; seem &lt;em&gt;very&lt;/em&gt; unlikely. In fact getting a mean score around &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;5&lt;/code&gt; also rarely happens. Seems like the distribution is getting &amp;quot;tighter&amp;quot; around the true mean value of &lt;code&gt;3.5&lt;/code&gt; the larger our sample size grows.&lt;/p&gt;
&lt;p&gt;Let&#39;s go wild and simulate &lt;code&gt;10000&lt;/code&gt; samples of &lt;code&gt;2000&lt;/code&gt; dice being rolled! Hold on to your monocles!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ &amp;lt;- 2000
m_ &amp;lt;- 10000
g &amp;lt;- ggplot(roll(m = m_, n = n_),
            mapping = aes(x = V1)) +
  geom_vline(xintercept = 3.5, colour = &amp;quot;tomato3&amp;quot;) +
  labs(
    subtitle = str_interp(&amp;#39;Density of means of dice 
                          rolls for ${n_} dice over ${m_} rolls.&amp;#39;),
    x = &amp;#39;Mean Score&amp;#39;,
    y = &amp;#39;Proportion of the time the value was observed&amp;#39;
  ) +
  geom_bar(aes(y = ..prop..), alpha = 0.4) +
  scale_x_continuous(
    breaks = 1:6,
    lim =  c(0, 7)
  ) +
  geom_density()

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-08-04-demonstrating-the-central-limit-theorem-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a very thin normal distribution. It&#39;s super tight around &lt;code&gt;3.5&lt;/code&gt;. Doesn&#39;t this make sense though? It&#39;s hard to image rolling &lt;code&gt;2000&lt;/code&gt; dice &lt;code&gt;10000&lt;/code&gt; times. But let&#39;s imagine you and &lt;code&gt;1999&lt;/code&gt; of your closest friends each rolled one die, only once, rather than the &lt;code&gt;10000&lt;/code&gt; times required. Does it not seem likely that if you added up everyone&#39;s value and divided by &lt;code&gt;2000&lt;/code&gt; that you&#39;d get pretty close to &lt;code&gt;3.5&lt;/code&gt;, the true population mean?&lt;/p&gt;
&lt;p&gt;Saying that the sample distribution is super tight around &lt;code&gt;3.5&lt;/code&gt; can be expressed in a more mathematical way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{samplingDistribution} = \frac{\sigma_{population}}{\sqrt{n}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the standard deviation.&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the size of your sample. So if the sample size is very large, say &lt;code&gt;2000&lt;/code&gt;, we can expect the sampling distribution&#39;s standard deviation to be rather small, resulting in &amp;quot;thinner&amp;quot; normal curves. Crucially, the mean of the sampling distribution is the same as the population&#39;s mean. But the standard deviation is not.&lt;/p&gt;
&lt;p&gt;The sample standard deviation &lt;em&gt;is&lt;/em&gt; related to the population standard deviation, though (just look at the numerator in the above). The more the original population varies (that is, the higher its standard deviation), the larger the sample size needs to be before we can get &amp;quot;thinner&amp;quot; normal curves.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;if-youve-made-it-this-far...&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. If you&#39;ve made it this far...&lt;/h1&gt;
&lt;p&gt;Well done - I hope that made some modicum of sense and that it was useful. If you have any suggestions for improvements or (*gasp*) find any errors in the above, &lt;em&gt;please&lt;/em&gt; let me know... it will come much appreciated!&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
      
    </item>
    
    <item>
      <title>ISLR Notes</title>
      <link>/1/07/islr-notes/</link>
      <pubDate>Tue, 17 Jul 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/07/islr-notes/</guid>
      <description>&lt;p&gt;Just a place for me to keep my notes of the &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34;&gt;Introduction to Statistical Learning&lt;/a&gt; book.&lt;/p&gt;
&lt;p&gt;This is by no means a comprehensive summary; just a list of points I wish to remember.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ch2&#34;&gt;Chapter 2: Statistical Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#inf_pred&#34;&gt;Inference vs Prediction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#acc_int&#34;&gt;Accuracy vs Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Par&#34;&gt;Parametric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non_par&#34;&gt;Non-Parametric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tr_te&#34;&gt;Train vs Test MSE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayes_clas&#34;&gt;Bayes Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#KNN&#34;&gt;KNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch3&#34;&gt;Chapter 3: Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ch4&#34;&gt;Chapter 4: Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;ch2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 2: Statistical Learning&lt;/h1&gt;
&lt;p&gt;Least squares is a popular method of doing regression, but one of many methods.&lt;/p&gt;
&lt;p&gt;The accuracy of &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; as a prediction for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; depends on two quantities, which we will call the &lt;em&gt;reducible error&lt;/em&gt; and the &lt;em&gt;irreducible error&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586180520/islr%20notes/red_ir_ibsdoc.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;We can improve the reducible error by using an more appropriate statistical technique. We can never improve the irreducible error, as it is introduced by the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; term in: &lt;span class=&#34;math inline&#34;&gt;\(Y = f(x) + \epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the true output.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) + \epsilon\)&lt;/span&gt; : we assume this is the function which generated the data. It has 2 components&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(X) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p\)&lt;/span&gt; which we assume is &lt;em&gt;linear&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; which we assume is from a mean 0 Gaussian&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We estimate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. The predictions we make &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; are generated from &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, in other words &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y} = \hat{f}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X_1 + ... + \hat{\beta_p}X_p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is only an estimate for&lt;/p&gt;
&lt;p&gt;So, it obvisouly doesn’t take into account the error term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;…&lt;/p&gt;
&lt;p&gt;There is no free lunch in statistics: no one method dominates all others over all possible data sets.&lt;/p&gt;
&lt;div id=&#34;inf_pred&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference vs Prediction&lt;/h2&gt;
&lt;p&gt;Inference: to do with how the various predictors affect the output. For example, how does TV advertising spend affect sales, how does radio advertising affect, how does newspaper advertising, how does some combination of these affect sales?&lt;/p&gt;
&lt;p&gt;Prediction: Predict the output, like, at a given level of advertising spends, what will the sales be?&lt;/p&gt;
&lt;p&gt;Different models better at different things. If just interested in prediction, then a very complicated black-box model is fine. If interested in inference, simpler and more interpretable may be better.&lt;/p&gt;
&lt;div id=&#34;acc_int&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accuracy vs Interpretability&lt;/h3&gt;
&lt;p&gt;Lasso less flexible, more interpretable: sets some params to 0.&lt;/p&gt;
&lt;p&gt;Generalized additive models (GAMs) extend the linear model to allow for certain non-linear relationships. GAMs are more flexible than linear regression. They are also less interpretable than linear regression.&lt;/p&gt;
&lt;p&gt;Fully non-linear methods such as bagging, boosting, and support vector machines bagging boosting with non-linear kernels, discussed in Chapters 8 and 9, are highly flexible approaches that are harder to interpret.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182241/islr%20notes/flex_zposfv.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parametric-vs-non-parametric&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parametric vs Non-Parametric&lt;/h2&gt;
&lt;p&gt;Statistical learning methods can be characterized as either parametric or non-parametric.&lt;/p&gt;
&lt;div id=&#34;Par&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parametric&lt;/h3&gt;
&lt;p&gt;2 steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select a functional form (like &lt;span class=&#34;math inline&#34;&gt;\(y = mx + c\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;fit / train the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pro: assuming functional form, it may be easier (than non-parametric case) to find params. con: choosing bad functional form is… bad.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non_par&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-Parametric&lt;/h3&gt;
&lt;p&gt;No explict assumptions about functional form.&lt;/p&gt;
&lt;p&gt;pro: avoid danger of choosing a bad functional form con: since non-parametric do not reduce the problem to finding a fixed number of parameters, lots more training data needed.&lt;/p&gt;
&lt;p&gt;Thin-plate spline is an example of non-parametric. Set a “smoothness” parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tr_te&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Train vs Test MSE&lt;/h3&gt;
&lt;p&gt;When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.&lt;/p&gt;
&lt;p&gt;Cross-validation is a method for estimating test MSE using the training data.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182404/islr%20notes/trate_pkpnaf.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-variance-trade-off&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bias Variance Trade-off&lt;/h3&gt;
&lt;p&gt;Expected (since, expected over value of &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x_0)\)&lt;/span&gt; over many number of training sets used to estimate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;) &lt;em&gt;test&lt;/em&gt; MSE of a given &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; can be broken down into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x_0)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Square bias of &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x_0)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The variance of the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last one, corresponds to &lt;em&gt;irreducible&lt;/em&gt; error. We can control the other two somewhat by changing our choice of model.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586182732/islr%20notes/bivar_hvi5il.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this context, &lt;em&gt;variance&lt;/em&gt; refers to how much &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; would change if we used a different training set (Overfitting). &lt;em&gt;Bias&lt;/em&gt; refers to the error we introduce by approximating a (possibly very complicated) real-life problem with a simpler mathemetical model (Underfitting).&lt;/p&gt;
&lt;p&gt;Generally, more flexible methods result in less bias. More flexible statistical methods have higher variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes_clas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. Obviously we don’t know the most likely class!&lt;/p&gt;
&lt;p&gt;The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate.&lt;/p&gt;
&lt;p&gt;The Bayes error rate is analogous to the irreducible error, discussed earlier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;KNN&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KNN&lt;/h2&gt;
&lt;p&gt;KNN classifier identifies the K points in the training data that are closest to &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;. Estimates class probability as proportion of these classes in the K points.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ch3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 3: Linear Regression&lt;/h1&gt;
&lt;p&gt;New terms in this chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interaction Effect&lt;/li&gt;
&lt;li&gt;Residual&lt;/li&gt;
&lt;li&gt;RSS&lt;/li&gt;
&lt;li&gt;population regression line&lt;/li&gt;
&lt;li&gt;least squares line&lt;/li&gt;
&lt;li&gt;biased / unbiased estimators&lt;/li&gt;
&lt;li&gt;standard error&lt;/li&gt;
&lt;li&gt;RSE&lt;/li&gt;
&lt;li&gt;confidence interval&lt;/li&gt;
&lt;li&gt;hypothesis test&lt;/li&gt;
&lt;li&gt;t-statistic&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Total Sum of Squares&lt;/li&gt;
&lt;li&gt;correlation&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; statistic&lt;/li&gt;
&lt;li&gt;forward selection&lt;/li&gt;
&lt;li&gt;variable selection&lt;/li&gt;
&lt;li&gt;synergy / interaction effect&lt;/li&gt;
&lt;li&gt;interaction terms&lt;/li&gt;
&lt;li&gt;prediction intervals&lt;/li&gt;
&lt;li&gt;dummy variable&lt;/li&gt;
&lt;li&gt;hierarchical principle&lt;/li&gt;
&lt;li&gt;polynomial regression&lt;/li&gt;
&lt;li&gt;Residual plots&lt;/li&gt;
&lt;li&gt;heteroscedasticity&lt;/li&gt;
&lt;li&gt;studentized residuals&lt;/li&gt;
&lt;li&gt;high leverage&lt;/li&gt;
&lt;li&gt;leverage statistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This chapter reviews ideas underlying linear regression, as well as least squares approach that is commonly used to fit this model.&lt;/p&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;&lt;u&gt;Simple&lt;/u&gt; linear regression refers to predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; from a &lt;u&gt;single&lt;/u&gt; predictor, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Fit line that is closest to datapoints. most common measure of closeness involved minimizing least squares criterion.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;residual&lt;/em&gt; is the difference between the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th observed response value and the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th response value predicted by the model: &lt;span class=&#34;math inline&#34;&gt;\(e_i = y_i-\hat{y}_i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;residual sum of squares&lt;/em&gt;: RSS &lt;span class=&#34;math inline&#34;&gt;\(= e_1^2 + e_2^2 + ... + e_n^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The least squares approach chooses &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; to minimize the RSS.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586345760/islr%20notes/mion_ylow1m.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The above define the &lt;em&gt;least squares coefficient estimates&lt;/em&gt; for simple linear regression.&lt;/p&gt;
&lt;div id=&#34;assessing-the-accuracy-of-coefficient-estimates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assessing the Accuracy of Coefficient Estimates&lt;/h3&gt;
&lt;p&gt;The model defined by the &lt;em&gt;population regression line&lt;/em&gt; is the &lt;u&gt;best&lt;/u&gt; linear approximation to the relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. It is given by: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The least squares coefficients define the &lt;em&gt;least squares line&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346036/islr%20notes/pop_sq_mlkc0z.png&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;biased-vs-unbiased-estimators&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Biased vs Unbiased Estimators&lt;/h4&gt;
&lt;p&gt;Unbiased estimator does not systematically over or under estimate the true parameter. The sample mean is an unbiased estimator: expect the sample mean to equal popln mean on average.&lt;/p&gt;
&lt;p&gt;Similar to how we try to estimate the population mean using the sample mean, we try use the least squares coefficients to estimate the coefficients of the population least squares line. Apt analogy, as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_1}\)&lt;/span&gt; are &lt;em&gt;unbiased&lt;/em&gt; estimators, like the sample mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-error&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Standard Error&lt;/h4&gt;
&lt;p&gt;Continuing with the example of the sample mean, we’d like to be able to comment on the accuracy of the sample mean as an estimate of the population mean. That’s where &lt;em&gt;standard error&lt;/em&gt; comes in, &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\mu})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\mu})^2\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(Var(\hat{\mu})\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{n}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the standard deviation of each of the realizations of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; (remember, you have this, as it’s in the labeled training data). So, the standard error is a ratio of 2 quantities: the bigger the standard deviation, the bigger the standard error. The more observations you have, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the smaller the standard error. Makes sense, since if the data vary wildly, you need lots of observations to be sure of it. Similarly, if the data is super clustered together, then you probably just need a few obersvations. Of course, the more observations you have, the better (reflected by the denominator of the Standard Error).&lt;/p&gt;
&lt;p&gt;The formula for the standard error holds provided that none of the observations are correlated.&lt;/p&gt;
&lt;p&gt;We can also look at the standard errors of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586346735/islr%20notes/se_b1_seB2_serjac.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = Var(\epsilon)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For these formulas to be strictly valid, we need to assume that the errors &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; for each observation are uncorrelated with common variance.&lt;/p&gt;
&lt;p&gt;Notice in the formula that &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\beta_1})\)&lt;/span&gt; is smaller when the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are more spread out: intuitively we have more leverage to estimate a slope when this is the case.&lt;/p&gt;
&lt;p&gt;We also see that &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{β}_0)\)&lt;/span&gt; would be the same as &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{μ})\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; were zero (in which case &lt;span class=&#34;math inline&#34;&gt;\(\hat{β}_0\)&lt;/span&gt; would be equal to &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In the above formulae, we needed &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; which was equal to &lt;span class=&#34;math inline&#34;&gt;\(Var(\epsilon)\)&lt;/span&gt;. In general, this quantity is not known, but it can be estimated from the data. The estimate for this is the &lt;em&gt;residual standard error&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(RSE = \sqrt{RSS / (n - 2)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Standard errors can be used to compute &lt;em&gt;confidence intervals&lt;/em&gt; - so, you can say with 95% confidence that &lt;span class=&#34;math inline&#34;&gt;\(\hat{β}_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{β}_1\)&lt;/span&gt; lie within a certain range of values.&lt;/p&gt;
&lt;p&gt;Standard errors can also be used to perform &lt;em&gt;hypothesis tests&lt;/em&gt;. The most common hypothesis test involves checking whether there is a relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Mathematically, corresponding to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0 : \beta_1 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1 : \beta_1 \neq 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, the hypothesis test is about &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, but we’ll have to use &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_1}\)&lt;/span&gt;. To test this hypothesis, we need to test whether &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; is far away enough from zero, to conclude that &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is non-zero. But how far away is far enough? This depends on the accuracy of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt;, of course. And we were able to comment about its accuracy using &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\beta}_1)\)&lt;/span&gt;. If we are confident in our estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt;, in other words, &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\beta}_1)\)&lt;/span&gt; is small, then even relatively small values of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; can lead us to conclude that there is a relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. In contrast, if we’re not that confident in the accuracy of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt;, ie &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\beta}_1)\)&lt;/span&gt; is large, then we’ll need some pretty large values of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; to convince us that there is a relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In practice, we use &lt;em&gt;t-statistic&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which measures the number of standard deviations away that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_1}\)&lt;/span&gt; is from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Look at it like this - the t-statistic is the &lt;u&gt;ratio&lt;/u&gt; of the distance of the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta_1}\)&lt;/span&gt; parameter to its hypothesized value (&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; in this case), to its standard error. The value of the t-statistic will lie somewhere on the t-distribution (with &lt;span class=&#34;math inline&#34;&gt;\(n-2\)&lt;/span&gt; degrees of freedom). We want to compute the probability of observing this &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; value, under the assumption of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 0\)&lt;/span&gt;. What is probability of having observed this value of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;? We call this the &lt;em&gt;p-value&lt;/em&gt;. Small p-value means it’s super unlikely to have observed it, so we reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;assessing-the-accuracy-of-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assessing the Accuracy of The Model&lt;/h3&gt;
&lt;p&gt;Once we have rejected &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; above, we need to comment on the accuracy of our model.&lt;/p&gt;
&lt;p&gt;The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; statistic.&lt;/p&gt;
&lt;div id=&#34;rse&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;RSE&lt;/h4&gt;
&lt;p&gt;Recall, we’ve already met the &lt;span class=&#34;math inline&#34;&gt;\(RSE = \sqrt{RSS / (n - 2)}\)&lt;/span&gt;. We used it to estimate the value of the standard deviation of the residuals (which were Gaussian, mean 0, s.d. = ?).&lt;/p&gt;
&lt;p&gt;In the advertising example, RSE was 3.26 (measured in thousands of units). So any prediction could be off by about 3260 units (if it’s within 1 standard deviation away from the line).&lt;/p&gt;
&lt;p&gt;RSE is considered a measure of the lack of fit of the model to the data - is the choice of model, namely &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 + \epsilon\)&lt;/span&gt; a good one?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;RSE is measured in units of Y, &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; takes the form of a proportion. &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is the proportion of the variance explained by the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0 \leq R^2 \leq 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;TSS measures the total variance in the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(TSS = \sum(y_i - \bar{y})^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, it’s the variance in the &lt;u&gt;actual, observed&lt;/u&gt; data.&lt;/p&gt;
&lt;p&gt;RSS measures the variability that is left unexplained after performing the regression.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, it’s the variance left over from the difference between the actual &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Ergo, &lt;span class=&#34;math inline&#34;&gt;\(TSS - RSS\)&lt;/span&gt; measures the amount of variability in the response that is explained by the regression. &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; measures the proportion of variability in in Y that can be explained using X.&lt;/p&gt;
&lt;p&gt;In Table 3.2, the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; was 0.61, and so just under two-thirds of the variability in sales is explained by a linear regression on TV.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is a measure of the &lt;em&gt;linear&lt;/em&gt; relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. &lt;em&gt;Correlation&lt;/em&gt; is also a measure of the linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. In simple linear regression, &lt;span class=&#34;math inline&#34;&gt;\(r^2 = R^2\)&lt;/span&gt;. In mutliple linear regression, though, &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is different: The concept of correlation is between 2 variables, namely &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in this case. When we have &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, ... X_n\)&lt;/span&gt;, the concept of correlation does not extend to these predictors and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, as its no longer between 2 variables (it’s between &lt;span class=&#34;math inline&#34;&gt;\(n + 1\)&lt;/span&gt;). But &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; still plays this same role in multiple correlation setting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-info-on-lm-from-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional Info on lm() from R:&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;More than one predictor.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1x_1 + \beta_2x2 + ... + \beta_px_p + \epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;estimating-regression-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating Regression Coefficients&lt;/h3&gt;
&lt;p&gt;Can use least squares (like in simple linear regression, but requires matrix algebra).&lt;/p&gt;
&lt;p&gt;NB: simple and multiple regression coefficients can be quite different:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427329/islr%20notes/mult_wefkis.png&#34; alt=&#34;Multiple Regression Coefficients&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Multiple Regression Coefficients&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/TV_yuqrlo.png&#34; alt=&#34;TV&#34; /&gt; &lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427525/islr%20notes/newspaper_idieyi.png&#34; alt=&#34;Newspaper&#34; /&gt; &lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427524/islr%20notes/radio_fyczo7.png&#34; alt=&#34;Radio&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Look at coefficient of Newspaper in multiple (-0.001) and simple (0.055) case. Difference stems from the fact that in the simple linear regression case, the slope term represents the average effect of a \$1000 increase in newspaper advertising, ignoring other predictors. In multiple regression setting, the coefficient for newspaper represents the average effect of increasing newspaper spending by \$1000 while holding TV and radio fixed.&lt;/p&gt;
&lt;p&gt;How can make sense for multiple regression to suggest no relationship between sales and newspaper, where in simple regression case it does?&lt;/p&gt;
&lt;p&gt;Look at the correlation matrix:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427957/islr%20notes/correlation_j2lazp.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Notice that the correlation between radio and newspaper is 0.35. Reveals tendency to spend more on newspaper in markets where more is spent on radio advertising.&lt;/p&gt;
&lt;p&gt;If multiple regression is correct, then newspaper has no &lt;em&gt;direct&lt;/em&gt; impact on sales, but radio does. In markets where more spent on radio, sales will be higher AND, as correlation matrix shows, newspaper spend will also be higher. Therefore, in simple linear regression, only looking at sales vs newspaper, we observe that higher newspaper spend tends to be associated with higher sales. Even though it doesn’t actually affect sales!!! Newspaper “takes the credit” in the absence of radio on sales.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-there-a-relationship-between-the-response-and-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Is there a relationship Between The Response and Predictors?&lt;/h3&gt;
&lt;p&gt;F-statistic&lt;/p&gt;
&lt;p&gt;The hypothesis being tested here is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0 : \beta_1 = \beta_2 = ... = \beta_p = 0\)&lt;/span&gt; $H_a : $ at least one &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; is non-zero&lt;/p&gt;
&lt;p&gt;So, we check whether or not &lt;em&gt;all&lt;/em&gt; coefficients are zero.&lt;/p&gt;
&lt;p&gt;Test using the F-statistic.&lt;/p&gt;
&lt;p&gt;When there is no relationship, we expect the F-stat to take a value close to 1. If &lt;span class=&#34;math inline&#34;&gt;\(H_a\)&lt;/span&gt; is true, expect it to be larger than 1. How much larger? Depends on values of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; -&amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; large, (lots of data), then needs to be just a little larger. If &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; small, (not lots of data), &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; needs to be bigger.&lt;/p&gt;
&lt;p&gt;Each predictor gets its own t-statistic and p-value as well, so why also look at F-stat? Suppose &lt;span class=&#34;math inline&#34;&gt;\(p = 100\)&lt;/span&gt;, then there will be 100 t-statistics, one for each coefficient. In this situation, 5% (so, 5 of them), will fall below the 0.05 threshold by chance. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(p &amp;gt; n\)&lt;/span&gt; can’t use least squares.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deciding-on-important-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Deciding on Important Variables&lt;/h3&gt;
&lt;p&gt;We could look at the individual p-values of the t-statistics, but as discussed, if p is large we are likely to make some false discoveries.&lt;/p&gt;
&lt;p&gt;How choose best model? These statistics can be used to judge quality of the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mallow’s &lt;span class=&#34;math inline&#34;&gt;\(C_p\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Akaike Information criterion (AIC),&lt;/li&gt;
&lt;li&gt;Bayesian information criterion (BIC)&lt;/li&gt;
&lt;li&gt;adjusted &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If got p vars, can fit &lt;span class=&#34;math inline&#34;&gt;\(2^p\)&lt;/span&gt; models, so will have to get these statistcs for &lt;span class=&#34;math inline&#34;&gt;\(2^p\)&lt;/span&gt; which is a &lt;em&gt;shitload&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;variable selection&lt;/em&gt; -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Forward Selection&lt;/li&gt;
&lt;li&gt;Backward Selection&lt;/li&gt;
&lt;li&gt;Mixed Selection&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Fit&lt;/h3&gt;
&lt;p&gt;Two of the most common numerical measures of model fit are the RSE and &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;, the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression.&lt;/p&gt;
&lt;p&gt;Recall that in simple regression, &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is the square of the correlation of the response and the variable: &lt;span class=&#34;math inline&#34;&gt;\(Cor(X, Y)^2\)&lt;/span&gt;. In multiple linear regression, it turns out that it equals &lt;span class=&#34;math inline&#34;&gt;\(Cor(Y, \hat{Y} )^2\)&lt;/span&gt;, the square of the correlation between the response and the fitted linear model.&lt;/p&gt;
&lt;p&gt;Plotting the data: overestimates when only TV or radio is spent on, under-estimates when budget split between the two media. This non-linear relationship cannot be modeled accurately using linear regression. –&amp;gt; &lt;em&gt;synergy / interaction effect&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586527766/islr%20notes/quad_yefb2i.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictions&lt;/h3&gt;
&lt;p&gt;We can compute a confidence interval in order to determine how close &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; will be to &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Prediction intervals to see how much &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; will vary from &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Prediction intervals are always wider than confidence intervals because they incorporate both reducible and irreducible error. Confidence intervals only look at reducible error.&lt;/p&gt;
&lt;p&gt;Irreducible error arises from the fact that X doesn’t completely determine &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (the error term).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X_1 + ... + \hat{\beta_p}X_p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is only an estimate for&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(X) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, it obvisouly doesn’t take into account the error term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) + \epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ch4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 4: Classification&lt;/h1&gt;
&lt;p&gt;New Terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;logistic function&lt;/li&gt;
&lt;li&gt;maximum likelihood&lt;/li&gt;
&lt;li&gt;odds&lt;/li&gt;
&lt;li&gt;log odds / logit&lt;/li&gt;
&lt;li&gt;Confounding&lt;/li&gt;
&lt;li&gt;prior&lt;/li&gt;
&lt;li&gt;discriminant function&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;why-not-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why not Linear Regression?&lt;/h2&gt;
&lt;p&gt;Choice of encoding affects model.&lt;/p&gt;
&lt;p&gt;For example, suppose you’re trying to predict &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586866723/islr%20notes/choice_enc_d22ahc.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This coding implies an ordering of the outcomes (in linear regression setting).&lt;/p&gt;
&lt;p&gt;If you had to code this differently, the different coding would create a different model. So… Don’t use Linear Regression!&lt;/p&gt;
&lt;p&gt;2 variable case is better:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586866898/islr%20notes/2var_h4wtsr.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;But, could get estimates outside the &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; range making them hard to interpret as probabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1586427957/islr%20notes/correlation_j2lazp.png&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;the-logistic-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Logistic Model&lt;/h3&gt;
&lt;p&gt;To avoid this problem, we must model p(X) using a function that gives outputs between 0 and 1 for all values of X. &lt;strong&gt;Many functions meet this description.&lt;/strong&gt; We use the logistic function in this model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To fit the model, use &lt;em&gt;Maximum Likelihood&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Can manipulate the above equation to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(X)/[1−p(X)]\)&lt;/span&gt; is called the odds, and can take on any value odds between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Take the logarithm of both sides to get the log-odds:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\log(\frac{p(X)}{1 - p(X)}) = \beta_0 + \beta_1X\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let hand side is called the &lt;em&gt;log-odds&lt;/em&gt; or &lt;em&gt;logit&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Recall from Chapter 3 that in a linear regression model, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; gives the average change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; associated with a one-unit increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. In contrast, in a logistic regression model, increasing &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by one unit changes the &lt;em&gt;log odds&lt;/em&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-the-regression-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating the Regression Coefficients&lt;/h3&gt;
&lt;p&gt;In Chapter 3, we used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the model, the more general method of maximum likelihood is preferred, since it has better statistical properties.&lt;/p&gt;
&lt;p&gt;We seek estimates of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; such that the predicted probability &lt;span class=&#34;math inline&#34;&gt;\(\hat{p}(x_i)\)&lt;/span&gt; corresponds as closely as possible to the observed value (for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;). Mathematically, we wish to maximize the &lt;em&gt;likelihood function&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The idea: Maximize the probability of seeing the data, by tuning the coefficients.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587822265/islr%20notes/ml_bdp4ix.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The estimates &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; are chosen to &lt;em&gt;maximize&lt;/em&gt; the above likelihood function.&lt;/p&gt;
&lt;p&gt;In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.&lt;/p&gt;
&lt;p&gt;Let’s look at a table of coefficients in this setting (we try to predict a default based on balance of an individual):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587822570/islr%20notes/logistic_results_zxcf30.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Notice how the log odds are affected by the coefficient, not the direct probability.&lt;/p&gt;
&lt;p&gt;Note that we use a Z-statistic, not a t-statistic. - In logistic and poisson regression but not in regression with gaussian errors, we know the expected variance and don’t have to estimate it separately. (&lt;a href=&#34;https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;Just plug in the values… for example, a balance of $1000:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587822888/islr%20notes/estimateing_aitwd2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Can use indicator variables, like with linear regression. For example, say we wish to predict a default based on whether or not a persion is a student:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587823064/islr%20notes/student_ezszfx.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple Logistic Regression&lt;/h3&gt;
&lt;p&gt;Using multiple predictors:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587823175/islr%20notes/multiple_reg_gtzxsz.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;So, we now try predict default based on whether or not a person is a student, &lt;strong&gt;and&lt;/strong&gt; their balance.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587823583/islr%20notes/mutliple_reg_table_vjgfgc.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Previously, the coefficient of student was positive (when it was the sole predictor). Now, it is negative. How can this be?&lt;/p&gt;
&lt;p&gt;Look at this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://res.cloudinary.com/da1gwmlvj/image/upload/v1587825057/islr%20notes/diffs_iicmbf.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Basically what the left hand graph is showing, is that for a given level of balance, a student is less risky. But there are more students with higher balances (the right hand side shows this). So, in the absence of balance data, the indicator variable “student” acts as a sort of proxy for it. In general this is known as confounding.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-for-2-response-classes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logistic for &amp;gt;2 Response Classes&lt;/h3&gt;
&lt;p&gt;There are extensions to the 2 class classification for logistic regression (using softmax), but this book discusses Linear Discriminant Analyis instead.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-discriminant-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;In logisitc regression we model &lt;span class=&#34;math inline&#34;&gt;\(P(y = k | X = x)\)&lt;/span&gt;, for the 2 class case. IE, we model the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, given the predictor(s), &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In LDA, model the distribution of the predictors &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;em&gt;in each response class&lt;/em&gt;, and then use Bayes’ theorem to flip these into: &lt;span class=&#34;math inline&#34;&gt;\(P(y = k | X = x)\)&lt;/span&gt;. It’s kind of like saying: model the distribution of the predictors in each response class, and then pick the response class where it’s most likely that the given instance of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; came from.&lt;/p&gt;
&lt;p&gt;When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.&lt;/p&gt;
&lt;p&gt;If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
            <category>hitchikers_guide</category>
      
      
    </item>
    
    <item>
      <title>Moment Generating Functions</title>
      <link>/1/07/moment-generating-functions/</link>
      <pubDate>Tue, 17 Jul 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/07/moment-generating-functions/</guid>
      <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tibble)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggthemes&amp;#39; was built under R version 3.5.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_economist())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Mean of a variable is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \sum_{x \in S}xf(x) = x_1f(x_1) + x_2f(x_2) + ... + x_kf(x_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, consider an example where &lt;span class=&#34;math inline&#34;&gt;\(x \in \{1, 2, 3\}\)&lt;/span&gt; and the p.m.f is given by &lt;span class=&#34;math inline&#34;&gt;\(f(1) = 3/6\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(2) = 2/6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(3) = 1/6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The mean of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1 \times \frac{3}{6} + 2 \times \frac{2}{6} + 3 \times \frac{1}{6} = \frac{10}{6}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Or, visually:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmf &amp;lt;- tribble(
  ~x, ~`f(x)`,
  1, (3/6),
  2, (2/6),
  3, (1/6)
)

g &amp;lt;- ggplot(pmf, mapping = aes(x = x, y = `f(x)`)) +
  geom_point() +
  lims(
    y = c(0, 1),
    x = c(0, 5)
  )

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-07-22-moment-generating-functions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the distance of the i-th point from the origin. A moment is just distance x weight. In this case, the weight is proabability.&lt;/p&gt;
&lt;p&gt;A moment - distance (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; probability (&lt;span class=&#34;math inline&#34;&gt;\(f(x_i)\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(x_if(x_i)\)&lt;/span&gt; is a moment having arm length &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sum of such products is the first moment about the origin since the distances are simply to the first power and the lenghts are measured from the origin.&lt;/p&gt;
&lt;div id=&#34;first-moment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First Moment&lt;/h2&gt;
&lt;p&gt;If we had to measure the first moments about the mean, instead we’d get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_{x \in S} (x - \mu)f(x) = \sum_{x \in S}xf(x) - \sum_{x \in S}\mu f(x)\)&lt;/span&gt; $ = &lt;em&gt;{x S}xf(x) -&lt;/em&gt;{x S}f(x) = - 1 = 0$&lt;/p&gt;
&lt;p&gt;First momement about &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is equal to 0.&lt;/p&gt;
&lt;p&gt;In mechanics &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is called the centroid. Last equation shows that if a fulcrum is placed at the point where the mean is, the system would balance because the sum of the mass to the left of the centroid would equal the sum of the mass to the right of the centroid. That is, the sum of the negative moments would equal the sum of the positive moments.&lt;/p&gt;
&lt;p&gt;Negative moment (from previous example):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((x_1 - \mu) \times f(x_1) = (1 - \frac{10}{6}) \times \frac{3}{6} = -\frac{12}{36}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Positive moments:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((x_2 - \mu) \times f(x_2) = (2 - \frac{10}{6}) \times \frac{2}{6} = \frac{4}{36}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\((x_3 - \mu) \times f(x_3) = (3 - \frac{10}{6}) \times \frac{1}{6} = \frac{8}{36}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So they balance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-moment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second Moment&lt;/h2&gt;
&lt;p&gt;Recall, first moment “cancels out”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
            <category>moment generating functions</category>
      
            <category>mgf</category>
      
      
    </item>
    
    <item>
      <title>The Hitchikers Guide to Inferential Statistics</title>
      <link>/1/07/hitchikers-guide-to-inferential-statistics/</link>
      <pubDate>Tue, 17 Jul 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/07/hitchikers-guide-to-inferential-statistics/</guid>
      <description>&lt;div id=&#34;inferential-vs-descriptive-stats.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inferential vs Descriptive stats.&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Descriptive - Purely used to summarize data, like IQR, Mean. Importantly, they do not draw conclusions beyond the data we already have.&lt;/li&gt;
&lt;li&gt;Inferential - Does allow us to make conclusions beyond the data we have.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;Inferential statistics is based on the premise that you can’t prove something is true, but you can disprove something by finding an exception.&lt;/p&gt;
&lt;p&gt;Decide what you are trying to find evidence for (alternative hypothesis), then set up the opposite as null hypothesis, and find evidence to disprove that.&lt;/p&gt;
&lt;p&gt;Suppose you want to show a link between studying and test results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: There is no effect on test result from studying&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: There is an effect on test result from studying&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hypotheses are always about the population parameters (like the mean).&lt;/p&gt;
&lt;p&gt;So the above should be stated mathematically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; =/= &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt; are the means for the “no study” and “study” groups respectively.&lt;/p&gt;
&lt;p&gt;General formula for test statistics: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(Observed Data) - (What We Expect if the null is true)}{AverageVariation}\)&lt;/span&gt;. This formula gets adapted for all the specific cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standardization&lt;/h2&gt;
&lt;p&gt;(Example from &lt;a href=&#34;https://www.youtube.com/watch?v=uAxyI_XfqXk&amp;amp;list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;index=19&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;How can we compare things that aren’t the same?&lt;/p&gt;
&lt;p&gt;Suppose there are two tests: SAT and ACT.&lt;/p&gt;
&lt;p&gt;Both try measure college readiness, but SAT is out of 1600 points and ACT is out of 36. How can you compare scores across these 2 tests?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Centre both distributions around 0 (subtract the mean) Mean of SAT is 1000, and mean of ACT is 21, so subtract those.&lt;/li&gt;
&lt;li&gt;Scales are still wrong, so divide adjusted score by standard deviation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This adjusted score is called a Z-Score (see Definitions). &lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{x - \mu}{\sigma}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note the assumption that both the distribution of SAT and ACT are normally distributed!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal Distribution&lt;/h2&gt;
&lt;p&gt;(Based on &lt;a href=&#34;https://www.youtube.com/watch?v=rBjft49MAO8&amp;amp;list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;index=20&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Normal distribution is important not only because a lot of things are normally distributed, like, height, IQ etc… There are a lot of things like debt, blood pressure which are not normally distributed. But here’s the thing - distributions of means are normally distributed, even if populations aren’t. So, if you sample a lot from a population which is not distributed normally, the mean of those samples will be normally distributed!&lt;/p&gt;
&lt;p&gt;In order to meaningfully compare whether two means are different, we need to know something about their distribution: the sampling distribution.&lt;/p&gt;
&lt;p&gt;This is described formally in the &lt;strong&gt;Central Limit Theorem&lt;/strong&gt; (see definitions).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-6.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-7.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-2-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, the mean of the distribution of sample means is the same as the population mean, its standard deviation is not.&lt;/p&gt;
&lt;p&gt;However, the standard deviation of the distribution of sample means &lt;em&gt;is&lt;/em&gt; related to the standard deviation of the population. But, it’s also related to the sample size. In the above graphs, see how the probability distributions of the graphs get thinner, as the number of dice goes up? The bigger your sample size, the closer your sample means are to the true population mean. So we need to adjust the original population standard deviation somehow to reflect this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{samplingDistribution} = \frac{\sigma_{population}}{\sqrt{n}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The standard deviation of a sampling distribution is called the &lt;em&gt;standard error&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note here that the number of rolls does not affect the shape of the distribution, only how accurately we approximate it. Think about it, the true shape is determined by the number of dice we have. If we have fewer dice, we expect the mean value we get for them to fluctuate more wildly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/post/hhguides/2020-07-17-hitchikers-guide-to-inferential-statistics_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can also use sampling distributions to compare other parameters - proportions, regression coefficients, or standard deviations, which also follow the central limit theorem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;t-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;T-Distribution&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yDEvXB6ApWc&amp;amp;list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;index=21&#34;&gt;They talk about T-dist closer to end of this vid&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Continous, unimodal probability distribution that’s useful to represent sampling distributions. It changes its shape based on how much information there is. It’s got thicker tails for smaller sample sizes (as there is less information) - making the distribution less “tight” around the mean - showing that we’re more uncertain. For bigger and bigger sample sizes the distribution gets thinner tails, and eventually becomes the same as the normal distribution. Usually, for tests about means sample sizes &amp;gt;= 30, it’s usually the same as normal. For proportions, need at least 10 instances from each group in the proportion!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing-applied&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypothesis Testing Applied&lt;/h2&gt;
&lt;p&gt;Cool definition from &lt;a href=&#34;here&#34;&gt;https://youtu.be/bf3egy7TQ2Q?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=281&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Null Hypothesis Significance Testing&lt;/u&gt; is a form of the reductio ad absurdum argument which tries to discredit an idea by assuming the idea is true, and then showing that if you make that assumption, something contradictory happens.&lt;/p&gt;
&lt;p&gt;(Just some examples of reductio ad absurdum outside of stats: The Earth cannot be flat; otherwise, we would find people falling off the edge. There is no smallest positive rational number because, if there were, then it could be divided by two to get a smaller one. It’s like proof by contradiction.)&lt;/p&gt;
&lt;div id=&#34;p-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;P Value&lt;/h3&gt;
&lt;p&gt;A P-Value answers the question of how rare your data is, by telling you the probability of getting data that’s extreme as the data you observed, if the null hypothesis were true. P-value tells you is how extreme your sample is assuming that the null is true.&lt;/p&gt;
&lt;p&gt;P-value = 0.1, then sample is in the top 10% most extreme samples we’d expect to see based on the distribution of sample means.&lt;/p&gt;
&lt;p&gt;A 2-sided p-value will tell us the probability of getting a sample mean as extreme (on either side) as the one we’ve seen, under the null hypothesis. A 1 sided will tell us the probability of getting a sample mean as high (or low) as the one we’ve seen, under the null hypothesis.&lt;/p&gt;
&lt;p&gt;NB: On Common misinterpretation of the a p-value is that it can telly ou the probability that the null hypothesis is true - &lt;a href=&#34;https://youtu.be/PPD8lER8ju4?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=234&#34;&gt;IT CAN’T!&lt;/a&gt; Ronald Fisher said: “In general, tests of significance are based on hypothetical probabilities calculated from their null hypotheses. They do no generally lead to any probability statements about the real word, but to a rational and well-defines measure of reluctance to the acceptance of the hypotheses they test.” So, getting a p-value of 0.1 doesn’t mean there is a 10% chance the null hypothesis is true.&lt;/p&gt;
&lt;p&gt;If P-Value is not lower than alpha, then we fail to reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. Notice that we say fail to rehect, not accept. Null hypothesis testing doesn’t allow us to “accept” or provide evidence that the null is true, instead we’ve only failed to provide evidence that it’s false. Abscense of evidence is not the evidence of abscense. Failint to reject the null doesn’t mean thtere isn’t an effect or relationship, just means we didn’t get enough evidence to say there definitely is one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z---test-vs-t---test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Z - test vs T - test&lt;/h3&gt;
&lt;p&gt;Z - tests commonly done when population standard deviation is known. T - tests commonly done when population standard deviation is unknown. We use t - distribution because the unknown population standard deviation is now a value we must estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-z---test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One Sample Z - Test&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BWJRsY-G8u0&#34;&gt;From here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the population the average IQ is 100 with a s.d. of 15. Give a sample of 30 participants a drug which either makes them smarter, stupider or has no effect. The sample of 30 people has a mean of 140. Did the medication affect intelligence?&lt;/p&gt;
&lt;p&gt;Note here that the population standard deviation and mean is given. Very importantly, we’re testing a mean, the sample mean. The question we’re basically asking is how probable is it that we saw a sample mean of 140? Now, recall, from the above, sample means are distributed normally. And they will be centered around the population mean. And that the sample standard deviation is related to the population standard deviation. But remember, it’s actually the sample mean standard deviation, which gets thinner as we get more observations.&lt;/p&gt;
&lt;p&gt;So, the normal distribution in the one sample Z-test is actually the distribution of sample means, under the null hypothesis. Mkay?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}} = 14.60\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s take an &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt; (which corresponds to Type I error happening 5% of the time - Type I error is the probability of rejecting Null Hypothesis even though the Null is correct). This is a 2 - tailed test, so rehect if Z less than -1.96 or greater than 1.96 :) HERE WE REJECT NULL HYPOTHESIS.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-z--test-for-proportions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One Sample Z- Test for proportions&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dH6igFVoCAw&amp;amp;list=PL568547ACA9211CCA&amp;amp;index=61&#34;&gt;From here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Survery claims 9/10 docs recommend aspirin. Survery, random sample, of 100 docs. 82 claim they recommend aspirin. At alpha = 0.05, do we believe?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p = 0.9\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(H_q\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p \neq 0.9\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The test statistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} = -2.667\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{p} = 0.82\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(p_0 = 0.9\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I think this comes from Bernoulli… Mean of Bernoulli is &lt;span class=&#34;math inline&#34;&gt;\(p_0 = 0.9\)&lt;/span&gt; and variance is &lt;span class=&#34;math inline&#34;&gt;\(p_0(1 - p_0) = 0.9 \times 0.1 = 0.09\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, really I think you can derive the test for proportions from the initial equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Except now, the population mean under the Null Hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;, the sample mean is &lt;span class=&#34;math inline&#34;&gt;\(0.82\)&lt;/span&gt;, the population variance, under the Null is &lt;span class=&#34;math inline&#34;&gt;\(0.9(1 - 0.1)\)&lt;/span&gt;, which makes the sample mean’s s.d. &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{p_0(1-p_0)}{n}}\)&lt;/span&gt;. The thing about the Bernoulli is that if you’ve got the mean, you’ve got the variance.&lt;/p&gt;
&lt;p&gt;Anyway, -2.667 &amp;lt; -1.96 so reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-t---test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One sample t - test&lt;/h3&gt;
&lt;p&gt;Population, average IQ = 100. New medication, either has negative, positive or no effect on intelligence. Sample of 30 participants have taken med, end up with a mean of 140 and s.d. of 20. Did medication affect intelligence?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0; \mu = 100\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(H_1; \mu \neq 100\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Degrees of freedom= &lt;span class=&#34;math inline&#34;&gt;\(n - 1 = 30 - 1 = 29\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Critical values: -2.0452 &amp;amp; 2.0452.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t = \frac{\bar{x} - \mu}{s / \sqrt{n}} = 10.96\)&lt;/span&gt;, reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;t-test---degrees-of-freedom-and-effect-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;T Test - Degrees of Freedom and Effect Size&lt;/h3&gt;
&lt;p&gt;T-Distribution is like Z-Distribution, but has fatter tails.&lt;/p&gt;
&lt;p&gt;Recall, we don’t know the population standard deviation in a T-Test. So, we estimate it using the sample standard deviation. So, we don’t have a perfect normal dist. But with bigger sample sizes, we’re better at estimating standard deviation, so T-Dist changes to look more and more like normal. More info means we’re more accurate. Degrees of Freedom can help us choose that accuracy.&lt;/p&gt;
&lt;p&gt;Number of independent pieces of information we have in our data. If have 3 observations, and we know the mean of those 3 observations, and the value of 2 of the 3 observations, the last observations &lt;em&gt;has to&lt;/em&gt; take on a certain value, for the mean to equal what it does. The last observation is not “Free” to take on any value. By contrast, if you know the mean of 3 observations, and only know the actual value of 1, the last 2 observations are free to take on a whole range of values.&lt;/p&gt;
&lt;p&gt;In terms of t-test: When we calc the mean, we use up 1 of those degrees of freedom, or 1 piece of independent info. Amount of info we have, depends on our sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The more data you have, the more independent info that you have - but every time you make a calc like the mean, you’re using up 1 piece of independent information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-size-and-significance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Effect Size and Significance&lt;/h3&gt;
&lt;p&gt;You can sometimesm find that there is a statistically significant difference in 2 means, which might be what you wanted. But the effect size is so small (compared to random variation), that it may not be worth it. For example, a hair vitamin might cause your hair to grow a statistically significant extra couple of nano-meters per month, but since you hair grows 12 mm per month, and the vitamin is expensive, it may not be &lt;em&gt;practically significant&lt;/em&gt; for you to buy it.&lt;/p&gt;
&lt;p&gt;NB, recall that in your t test statistic, the denominator is scaled by the sample size; so the smaller the sample size, the smaller the scaled standard error: &lt;span class=&#34;math inline&#34;&gt;\(\frac{SE}{\sqrt{n}}\)&lt;/span&gt;, the bigger the t test stat. If the numerator is very small (ie, the effect size if very small), then even if there is an effect, the only way to test it might be to increase the sample size, in order to get your t-test stat to be bigger.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t = \frac{\bar{x_1} - \bar{x_2}}{\sqrt{\frac{S^2_{control}}{n_{control}} + \frac{S^2_{treatment}}{n_{treatment}}}}\)&lt;/span&gt; - in the 2 sample case. See how a small effect size (numerator) need much &lt;span class=&#34;math inline&#34;&gt;\(n_{i}\)&lt;/span&gt;s to pick up significance (that is, for t stat to be bigger)?&lt;/p&gt;
&lt;p&gt;&lt;b&gt;So, P values should always be looked at WITH effect size.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt; &lt;b&gt;We need degrees of freedom to understand why smaller differences between means can be significant if you have a larger sample size! &lt;/b&gt;&lt;/u&gt;. The more info you have, the more accurate your estiamtes are.&lt;/p&gt;
&lt;p&gt;So, fewer degrees of freedom -&amp;gt; the fatter the tails, and the less likely your are to reject the null. More data though, means more degrees of freedom and thinner tails, so the more confident you are in extreme values, and thus more likely to reject the null when seeing them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence Intervals&lt;/h2&gt;
&lt;p&gt;The mean of sample distribution is the center a CI (see definition below). The way we choose the confidence range is related to the distribution of sample means (depends on standard deviation of sample means.).&lt;/p&gt;
&lt;p&gt;The 95% in a 95% confidence interval tells us that if we calculated a confidence interval from 100 different samples, about 95% of them would contain the true population mean. Think about it, if you got 2 different samples of 40 people, both of those would have different sample means and sample standard deviations, and thus different confidence intervals. It is possible the CI we’ve created doesn’t contain the true mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes’ Theorem&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(A \cap B)}{P(B)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Updating beliefs.&lt;/p&gt;
&lt;p&gt;Suppose your sister tells you she has a friend. At this point, 50-50 chance that it’s male / female. Suppose your sister also tells you that the friend has breast cancer. Then &lt;span class=&#34;math inline&#34;&gt;\(P(male | BREAST CANCER) = \frac{P(BREAST CANCER | male)P(male)}{P(BREAST CANCER)}\)&lt;/span&gt; Suppose you know &lt;span class=&#34;math inline&#34;&gt;\(P(BREAST CANCER | male) and P(BREAST CANCER)\)&lt;/span&gt; from gov’t statistics, then you can calc: &lt;span class=&#34;math inline&#34;&gt;\(P(male | BREAST CANCER) = \frac{P(0.001)P(0.5)}{P(0.063)} = 0.79%\)&lt;/span&gt;, so pretty low.&lt;/p&gt;
&lt;p&gt;So, in the above, we updated our belief based on new information. Rearrange the formula slightly to see this more clearly: $P(A|B) = P(A) $ &lt;span class=&#34;math inline&#34;&gt;\(P(male | BREAST CANCER) = \frac{P(0.001)}{P(0.063)}P(A)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Updating beliefs is core to Bayesian stats.&lt;/p&gt;
&lt;p&gt;Bayes’ Factor… Say you’re trying to figure out if someone is a star wars fan. Say your &lt;em&gt;prior&lt;/em&gt; belief is that 60% of people are Star Wars fans. ie, &lt;span class=&#34;math inline&#34;&gt;\(P(fan) = 0.6\)&lt;/span&gt; Suppose also, that the probabilty of watching a new star wars movie that’s just come out is &lt;span class=&#34;math inline&#34;&gt;\(P(movie | fan) = 0.99\)&lt;/span&gt;, as fans would rush to watch it. But suppose also that the probability of watching the movie given that a person is not a fan is &lt;span class=&#34;math inline&#34;&gt;\(P(movie | not fan) = 0.5\)&lt;/span&gt;. This is because some people are just curious about it, (like me), or dragged by friends / family. So &lt;span class=&#34;math inline&#34;&gt;\(\frac{P(movie | fan)}{P(movie | not fan)} = \frac{0.99}{0.5} = 1.98\)&lt;/span&gt; is the odds that a person is a fan, if they tell you that they’ve watched the movie. It’s what we’ve learned about they hypothesis from the data (in this case the data is someone telling you they’ve watched the movie). We cam multiply our &lt;em&gt;prior&lt;/em&gt; with this factor to get our posterior.&lt;/p&gt;
&lt;p&gt;We can repeat this process; make the new posterior a prior for updating beliefs. In the example before, let’s say that there are 60% star wars fans and 40% not - this is your prior. So, odds you’re a fan is &lt;span class=&#34;math inline&#34;&gt;\(\frac{0.6}{0.4} = 1.5\)&lt;/span&gt;. But we can update this with the data: &lt;span class=&#34;math inline&#34;&gt;\(\frac{0.6}{0.4} \times \frac{0.99}{0.5} = 1.5 \times 1.98 = 2.97\)&lt;/span&gt;; so the odds have increased.&lt;/p&gt;
&lt;div id=&#34;bayesian-hypothesis-testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian Hypothesis Testing&lt;/h3&gt;
&lt;p&gt;Compare the probabilities of the 2 hypotheses: from the above, compare the probabilities of the probability that a person is a fan, given they’d seen the movie. So, compare &lt;span class=&#34;math inline&#34;&gt;\(P(fan | seenmovie)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(not fan | seenmovie)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So look that the ratio: &lt;span class=&#34;math inline&#34;&gt;\(\frac{P(fan | seenmovie)}{P(not fan | seenmovie)}\)&lt;/span&gt;. We look at the ratio of 2 instances of Bayes’ thm; since both the numerator and the denominator are posterior probs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;criticism-of-bayes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Criticism of Bayes’&lt;/h3&gt;
&lt;p&gt;In Bayesian hypothesis testing (also in just applying Bayes’ thm), the result is influenced by your prior &lt;em&gt;belief&lt;/em&gt;. If you throw in a different number for your belief, then you get different results.&lt;/p&gt;
&lt;p&gt;One of the main uses of statistics is science whis is supposed to be relatively “objective” and not influenced by opinion, yet here’s a method that included beliefs in its calculation.&lt;/p&gt;
&lt;p&gt;In some research, the researchers publish the Bayes Factor, for just this reason - it’s free of the prior. You could then use their calculated Bayes factor to update your own prior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian Inference&lt;/h3&gt;
&lt;p&gt;Using Bayesian methods to analyze questions allows us to “inject” &lt;em&gt;prior beliefs&lt;/em&gt; into the analysis.&lt;/p&gt;
&lt;p&gt;Using Null Hypothesis significance testing, we can ask whether means of groups are likely different, or not (reject, or fail to reject null hypothesis - recall, you can only fail to reject the null hypothesis, that is, you don’t state that the means are the same, just that you haven’t seen enough evidence to show they aren’t). With Bayesian methods, you can assign a probability to the question of whether or not the means are different. It’s not just a binary outcome… (I think…)&lt;/p&gt;
&lt;p&gt;Bayesian A/B Testing: &lt;a href=&#34;https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing&#34; class=&#34;uri&#34;&gt;https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing&lt;/a&gt; ^^ In the above he also mentions why to use instead of using t-tests for differences of means. “Classical statistics tells us Significance, but what we’re really after is Magnitude!” - sayeth Count Bayesie.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi Square Tests&lt;/h2&gt;
&lt;p&gt;Model fits? Compare expectations vs observations.&lt;/p&gt;
&lt;p&gt;Discrete. Contingency tables.&lt;/p&gt;
&lt;p&gt;Test = &lt;span class=&#34;math inline&#34;&gt;\(\frac{Observed - ExpecetedUnderNullHypothesis}{AverageVariation}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What’s Average Variation?&lt;/p&gt;
&lt;p&gt;If we just take observed minus expected, we always get to 0, so we square the answers before adding.&lt;/p&gt;
&lt;p&gt;Chi sq, degrees of freedom - all the counts, minus 1 (same argument as t-test d.o.f). Can find p-val.&lt;/p&gt;
&lt;p&gt;Types of Chisq test:&lt;/p&gt;
&lt;div id=&#34;goodness-of-fit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Goodness of fit&lt;/h3&gt;
&lt;p&gt;Only has 1 row. Expected frequency must be &amp;gt; 5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-of-independence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test of independence&lt;/h3&gt;
&lt;p&gt;Tests whether being a member of 1 category is independent of being in another.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-of-homogeneity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test of homogeneity&lt;/h3&gt;
&lt;p&gt;Tests whether it’s likely thatn 2 samples come from the same population.&lt;/p&gt;
&lt;p&gt;d.o.f = (r-1)(c-1).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;glm---general-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GLM - General Linear Model&lt;/h2&gt;
&lt;p&gt;Allow us to create many different models, like the regression model.&lt;/p&gt;
&lt;p&gt;GLM overview: Data = Model + Error.&lt;/p&gt;
&lt;p&gt;ie, &lt;span class=&#34;math inline&#34;&gt;\(y = b + mx\)&lt;/span&gt;. (error is b; model part is mx)&lt;/p&gt;
&lt;p&gt;Error is a deviation in model.&lt;/p&gt;
&lt;div id=&#34;linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;Assumption is that data is linear. Regression line is the line that’s as close as possible to all data points at once. Ie, minimizes Sum of squared distances from line.&lt;/p&gt;
&lt;p&gt;Residual plot - plot the errors.&lt;/p&gt;
&lt;p&gt;F-Test -&amp;gt; quantify how well we think our data fit a distribution, like the null distribution.&lt;/p&gt;
&lt;p&gt;Recall, in general, a Test Stat = &lt;span class=&#34;math inline&#34;&gt;\(\frac{ObservedData - WhatWeExpectIfTheNullIsTrue}{AverageVariation}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;F-stat = &lt;span class=&#34;math inline&#34;&gt;\(\frac{ObservedModel - ModelIfNullIsTrue}{AverageVariation}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Consider total sum of squares: &lt;span class=&#34;math inline&#34;&gt;\(\sum(y_i -\bar{y}) ^ 2\)&lt;/span&gt; &amp;lt;- total variation in the dataset. We want to know how much of that variation is accounted for in our model, and how much is just error. (good explanation)[&lt;a href=&#34;https://youtu.be/WWqE7YHR4Jc?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=424&#34; class=&#34;uri&#34;&gt;https://youtu.be/WWqE7YHR4Jc?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=424&lt;/a&gt;] - but the idea is, that from the data we calculate some slope coefficient, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;, let’s say &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = 6.468\)&lt;/span&gt;. We want to know what the difference would be if we assumed no relationship, ie &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F-stat = \frac{SSR}{Average Variation}\)&lt;/span&gt; - numerator, sum of squares for regression, is the difference between the models using the calculated slope, and a slope of 0. Bottom part is SSE.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;F statistic allows us to directly compare the amount of variation can and cannot explain&lt;/b&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t^2 = F\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anova&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ANOVA&lt;/h3&gt;
&lt;p&gt;Test the difference between multiple groups.&lt;/p&gt;
&lt;p&gt;Similar to regression, except use a categorical to predict an outcome. Like using degree studied for vs salary.&lt;/p&gt;
&lt;p&gt;Note, Anova can tell you that there is a statistically significant difference between the groups, but not where it is. So follow up with multiple t-tests (Bonferroni).&lt;/p&gt;
&lt;p&gt;The ANOVA makes an assumption about the world which it tests: that the best prediction for a variable is the mean rating of the group it belongs to!&lt;/p&gt;
&lt;p&gt;RA Fisher.&lt;/p&gt;
&lt;p&gt;ANOVAS can include more than one grouping variable - &lt;a href=&#34;https://www.youtube.com/watch?v=wo1xlefg5KI&amp;amp;list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=0s&#34;&gt;&lt;em&gt;intersectional groups&lt;/em&gt;.&lt;/a&gt; Factorial ANOVA. So, for example, say you are wondering if there is a difference in means of car prices as they are grouped by both Manufacturer and Colour… &lt;span class=&#34;math inline&#34;&gt;\(PRICE = BASELINE + \beta{MAN} \ times MANUFACTURER + \beta{COL} \times COLOUR + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;definitions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Definitions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Bayes’ Factor&lt;/u&gt; - Represents the amount of info we’ve learned about our hypotheses from the data.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Central Limit Theorem&lt;/u&gt; - The distribution of sample means for an independent random variable will get closer and closer to a normal distribution as the size of the sample gets bigger and bigger, even if the original population distribution isn’t normal. Think of independet die rolls, like in &lt;a href=&#34;https://youtu.be/rBjft49MAO8?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&amp;amp;t=349&#34;&gt;here&lt;/a&gt;. Twenty dice are 20 independent random variables.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Confidence Interval&lt;/u&gt; - An &lt;b&gt;estimated range of values&lt;/b&gt; that seem reasonable based on what we’ve observed. Its center is still the &lt;b&gt;sample mean&lt;/b&gt;, but we’ve got some room on either side for out uncertainty.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Degrees of freedom &lt;/u&gt; Number of independent pieces of information we have -&lt;u&gt;Effect Size &lt;/u&gt; - How big the effect we observed was, compared to random variation&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Hypthesis Testing&lt;/u&gt; -&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Inference&lt;/u&gt; - the process of drawing conclusions about population parameters based on a sample taken from the population.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;P-Value&lt;/u&gt; - probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Point Estimate &lt;/u&gt; - a single value given as an estimate of a parameter of a population. Like the mean.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Regression Line &lt;/u&gt; - Regression line is the line that’s as close as possible to all data points at once.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Sampling distribution&lt;/u&gt; - the probability distribution of a sample statistic (like the mean, or variance).&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Standard Error&lt;/u&gt; - of a statistic (usually an estimate of a parameter - like the mean) is the standard deviation of its sampling distribution. For example, Standard error of the mean is &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Statistical Power &lt;/u&gt; - &lt;span class=&#34;math inline&#34;&gt;\(1 - \beta\)&lt;/span&gt; - chance of detecting an effect if there is one.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Test Statistic &lt;/u&gt; - Allow us to quantify how close things are to our expectations or theories&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Type I error&lt;/u&gt; - &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; - probability of saying null hypothesis is wrong, when it is correct&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Type II error&lt;/u&gt; - &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; - probability of not saying the null hypothesis is wrong, even though it is wrong. Note, you’re not actually saying it’s true, just that it’s not wrong.&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Z-Distrubtion&lt;/u&gt; - Standardized normal distribution&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Z-Score&lt;/u&gt; - A Z-score is a numerical measurement that describes a value’s relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean. If a Z-score is 0, it indicates that the data point’s score is identical to the mean score. A Z-score of 1.0 would indicate a value that is one standard deviation from the mean. &lt;strong&gt;Z scores allow us to compare things that are not on the same scale as long as they are normally distributed&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
      
            <category>hitchikers_guide</category>
      
            <category>statistics</category>
      
      
    </item>
    
  </channel>
</rss>