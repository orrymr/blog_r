<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on orrymr.com</title>
    <link>/post/</link>
    <description>Recent content in Posts on orrymr.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 04 Oct 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An R Pipeline for XGBoost (and a discussion about hyperparameters)</title>
      <link>/post/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/bsTable/bootstrapTable.min.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-explore-the-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Load And Explore The Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interacting-with-the-sparse-matrix-object&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Interacting with the Sparse Matrix Object&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hyperparameters&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-the-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Training The Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;XGBoost is …. I will use this post to consolidate my learnings thus far about XGBoost.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;load-and-explore-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Load And Explore The Data&lt;/h1&gt;
&lt;p&gt;We will use the &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Titanic Dataset&lt;/a&gt; which we get from Kaggle. Basically, we try predict whether a passenger survived or not (so this is a binary classification problem).&lt;/p&gt;
&lt;p&gt;Let’s load up the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_train &amp;lt;- read_csv(directoryWhichContainsTrainingData)
titanic_test &amp;lt;- read_csv(directoryWhichContaintsTestData)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of brevity, I’ll only keep some of the features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pclass&lt;/li&gt;
&lt;li&gt;Sex&lt;/li&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Embarked&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll use dplyr’s select() to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;titanic_train &amp;lt;- titanic_train %&amp;gt;%
  select(Survived,
         Pclass,
         Sex,
         Age,
         Embarked)

titanic_test &amp;lt;- titanic_test %&amp;gt;%
  select(Pclass,
         Sex,
         Age,
         Embarked)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at our data after discarding a few features:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train, give.attr = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;spec_tbl_df&amp;#39;, &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;: 891 obs. of  5 variables:
##  $ Survived: num  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass  : num  3 1 3 1 3 3 1 3 3 2 ...
##  $ Sex     : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age     : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ Embarked: chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;XGBoost will only take numeric data as input. Let’s convert our character features to factors, and one-hot encode. We will use sparse.model.matrix() to create a sparse matrix which will be used as input for our model. XGBoost has been written to take advantage of sparse matrices, so we want to make sure that we’re using this feature.&lt;/p&gt;
&lt;p&gt;Unfortunately, in using R at least, sparse.model.matrix() will drop rows which contain NA’s if the global option &lt;code&gt;options(&#39;na.action&#39;)&lt;/code&gt; is set to &lt;code&gt;&amp;quot;na.omit&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So we use a fix outlined &lt;a href=&#34;https://stackoverflow.com/questions/29732720/sparse-model-matrix-loses-rows-in-r&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;previous_na_action &amp;lt;- options(&amp;#39;na.action&amp;#39;)
options(na.action=&amp;#39;na.pass&amp;#39;)

titanic_train$Sex &amp;lt;- as.factor(titanic_train$Sex)
titanic_train$Embarked &amp;lt;- as.factor(titanic_train$Embarked)

titanic_train_sparse &amp;lt;- sparse.model.matrix(Survived~., data = titanic_train)[,-1]

options(na.action=previous_na_action$na.action)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could have just used a sentinel value to replace the NA’s.&lt;/p&gt;
&lt;div id=&#34;interacting-with-the-sparse-matrix-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Interacting with the Sparse Matrix Object&lt;/h2&gt;
&lt;p&gt;The data are in the format of a &lt;strong&gt;dgCMatrix&lt;/strong&gt; class - this is the Matrix package’s implementation of sparse matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Formal class &amp;#39;dgCMatrix&amp;#39; [package &amp;quot;Matrix&amp;quot;] with 6 slots
##   ..@ i       : int [1:3080] 0 1 2 3 4 5 6 7 8 9 ...
##   ..@ p       : int [1:6] 0 891 1468 2359 2436 3080
##   ..@ Dim     : int [1:2] 891 5
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:891] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##   .. ..$ : chr [1:5] &amp;quot;Pclass&amp;quot; &amp;quot;Sexmale&amp;quot; &amp;quot;Age&amp;quot; &amp;quot;EmbarkedQ&amp;quot; ...
##   ..@ x       : num [1:3080] 3 1 3 1 3 3 1 3 3 2 ...
##   ..@ factors : list()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check the dimension of the matrix directly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(titanic_train_sparse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 891   5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The names are the features are given by &lt;code&gt;titanic_train_sparse@Dimnames[[2]]&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(titanic_train_sparse@Dimnames[[2]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Pclass&amp;quot;    &amp;quot;Sexmale&amp;quot;   &amp;quot;Age&amp;quot;       &amp;quot;EmbarkedQ&amp;quot; &amp;quot;EmbarkedS&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If needed, you can convert this data (back) into a dataframe, thusly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data_as_df &amp;lt;- as.data.frame(as.matrix(titanic_train_sparse))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperparameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Hyperparameters&lt;/h1&gt;
&lt;p&gt;Tuning hyperparameters is a vast topic. Without going into too much depth, I’ll outline some of the more commonly used hyperparameters:&lt;/p&gt;
&lt;p&gt;Full reference: &lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/parameter.html&#34; class=&#34;uri&#34;&gt;https://xgboost.readthedocs.io/en/latest/parameter.html&lt;/a&gt; ### Tree booster params…####&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-10&#34;&gt;Table 3.1: &lt;/span&gt;Parameters
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Parameter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Explanation
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
eta
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0.3 learning rate / shrinkage. Scales the contribution of each try by a factor of 0 &amp;lt; eta &amp;lt; 1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gamma
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 0 minimum loss reduction needed to make another partition in a given tree. larger the value, the more conservative the tree will be (as it will need to make a bigger reduction to split) So, conservative in the sense of willingness to split.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
max_depth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 6 max depth of each tree…
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
subsample
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1 (ie, no subsampling) fraction of training samples to use in each “boosting iteration”
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
colsample_bytree
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 (ie, no sampling) Fraction of columns to be used when constructing each tree. This is an idea used in RandomForests
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min_child_weight
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
default = 1 This is the minimum number of instances that have to been in a node. It’s a regularization parameter So, if it’s set to 10, each leaf has to have at least 10 instances assigned to it. The higher the value, the more conservative the tree will be.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Full reference: https://xgboost.readthedocs.io/en/latest/parameter.html
# ### Tree booster params...####
# eta:                              default = 0.3
#                                   learning rate / shrinkage. Scales the contribution of each try by a factor of 0 &amp;lt; eta &amp;lt; 1
# gamma:                            default = 0
#                                   minimum loss reduction needed to make another partition in a given tree.
#                                   larger the value, the more conservative the tree will be (as it will need to make a bigger reduction to split)
#                                   So, conservative in the sense of willingness to split.
# max_depth:                        default = 6
#                                   max depth of each tree...
# subsample:                        1 (ie, no subsampling)
#                                   fraction of training samples to use in each &amp;quot;boosting iteration&amp;quot;
# colsample_bytree:     default = 1 (ie, no sampling)
#                       Fraction of columns to be used when constructing each tree. This is an idea used in RandomForests
# min_child_weight:     default = 1
#                       This is the minimum number of instances that have to been in a node. It&amp;#39;s a regularization parameter
#                       So, if it&amp;#39;s set to 10, each leaf has to have at least 10 instances assigned to it.
#                       The higher the value, the more conservative the tree will be.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(I’ve left it as commented code, as I like to past this into my scripts as a quick reference.)&lt;/p&gt;
&lt;p&gt;Let’s create the hyper-parameters list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params_booster &amp;lt;- list(
  booster = &amp;#39;gbtree&amp;#39;, # Possible to also have linear boosters as your weak learners.
  eta = 1, 
  gamma = 0,
  max.depth = 2, 
  subsample = 1, 
  colsample_bytree = 1,
  min_child_weight = 1, 
  objective = &amp;quot;binary:logistic&amp;quot;
)

bstSparse &amp;lt;- xgboost(data = titanic_train_sparse, 
                     label = titanic_train$Survived, 
                     nrounds = 100,  
                     params = params_booster)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;training-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Training The Model&lt;/h1&gt;
&lt;p&gt;The xgb.train() and xgboost() functions are used to train the boosting model, and both return an object of class xgb.Booster. Before we do that, let’s first use xgb.cv() to get some understanding of our performance before we evaluate against our final hold our test set. Important to not that xgb.cv() returns an object of type xgb.cv.synchronous, not xgb.Booster. So you won’t be able to call functions like xgb.importance() on it, as xgb.importance() takes object of class xgb.Booster &lt;strong&gt;not&lt;/strong&gt; xgb.cv.synchronous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# NB: keep in mind xgb.cv() is used to select the correct hyperparams.
# Once you have them, train using xgb.train() or xgboost() to get the final model.

bst.cv &amp;lt;- xgb.cv(data = titanic_train_sparse, 
              label = titanic_train$Survived, 
              params = params_booster,
              nrounds = 300, 
              nfold = 5,
              print_every_n = 20,
              verbose = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, we can also implement early-stopping: early_stopping_rounds = 3, so that if there has been no improvement in test accuracy for a specified number of rounds, the algorithm stops.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_df &amp;lt;- data.frame(tr = bst.cv$evaluation_log$train_error_mean, 
                     val = bst.cv$evaluation_log$test_error_mean,
                     iter = bst.cv$evaluation_log$iter)

g &amp;lt;- ggplot(res_df, aes(x=iter)) +        # Look @ it overfit.
  geom_line(aes(y=tr), color = &amp;quot;blue&amp;quot;) +
  geom_line(aes(y=val), colour = &amp;quot;green&amp;quot;)

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-04-an-r-pipeline-for-xgboost-and-a-discussion-about-hyperparameters_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Work Rules! - Insights from Inside Google That Will Transform How You Live and Lead, Part 1</title>
      <link>/post/work-rules/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/work-rules/</guid>
      <description>


&lt;p&gt;Hello Person From The Internet who has decided to come here. Thank you for dropping by.&lt;/p&gt;
&lt;p&gt;Today I’m going to talk about a book I read - &lt;a href=&#34;https://www.amazon.com/gp/product/1455554790/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=1455554790&amp;amp;linkCode=as2&amp;amp;tag=orrymr-20&amp;amp;linkId=35253fa91ca83356f3f4b863eb42b3f1&#34;&gt;Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead&lt;/a&gt; by Laszlo Bock.&lt;/p&gt;
&lt;p&gt;Although this book is intended for people who manage teams, most human beings will be able to get valuable insight out of this worthwhile book. Our author, Laszlo, has been in various roles at some large companies, including a stint as Senior Vice President of People Operations (HR, I believe) at Google (the search engine company - keep an eye out for them).&lt;/p&gt;
&lt;p&gt;I read this book a couple of months ago, and have been procrastinating writing this post ever since. So, I’m relying on my barely legible notes and an unreliable brain (human memory is so capricious, isn’t it?).&lt;/p&gt;
&lt;p&gt;What I shall do, Dear Reader, is go through the book, chapter by chapter, divulging my meticulous notes to you.&lt;/p&gt;
&lt;p&gt;Pull yourself towards yourself, here it comes!&lt;/p&gt;
&lt;div id=&#34;chapter-0-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 0 &amp;amp; 1&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Being part of an environment where people thrive starts with taking responsibility for that environment. I like that. Make a place nice. Don’t only care about output, care about what &lt;em&gt;kind&lt;/em&gt; of place it is. Is it the kind of place where we yell at the new guy because he doesn’t know where the HR office is? No! We don’t do that. Because eventually he’ll find out where the HR office is, and we will get in trouble! Is it the kind of place where we show the new guy around? Yes! Because he might be smarter and more hard working than we are, and he might remember that when he is our overlord.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Act like a founder” - sayeth Bock. Again, this is great. It speaks to taking personal responsibility for the work you put out; having pride and a sense of ownership in what you put out into the world. &lt;strong&gt;Act like a founder.&lt;/strong&gt; You’re spending the better part of your day at the job: own it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Talent flows to “high freedom” companies - although Laszlo does state in a later chapter that companies have been successful both in high and low freedom settings. That’s one of the things I really liked about this book; he often says “this is how we did things, it worked for us like that, but that don’t mean it ain’t gonna work for you in another way, dagnabit”. I’m paraphrasing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Managers serve the team - s/he clears roadblocks. Not literal ones, although, he doesn’t preclude managers from clearing literal roadblocks (but I don’t recommend it).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-2---culture-eats-strategy-for-breakfast&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 2 - Culture Eats Strategy For Breakfast&lt;/h1&gt;
&lt;p&gt;I wonder what culture has for brunch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google has a simple mission statement (organize the world’s information and make it universally accessible and useful). It’s easy to rally behind, but they can never really full accomplish it… so they are compelled to keep going.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Google has a culture of information sharing and trust.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-3---lake-wobegon-where-all-the-hires-are-above-average&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 3 - Lake Wobegon, Where all the Hires are Above Average&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only hire people who are better than you (in some meaningful way).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take your time hiring.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-4---searching-for-the-best&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 4 - Searching For the Best&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We are all biased; we may be biased towards hiring friends.. So try be objective in your hiring (read the book to learn how. What? You expect moi to tell you?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-5---dont-trust-your-gut&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 5 - Don’t Trust Your Gut&lt;/h1&gt;
&lt;p&gt;For me this was a tough one. If my gut says cheeseburger, I’m trusting it. Apparently hiring people and deciding what lunch to get &lt;em&gt;should&lt;/em&gt; involve different cognitive processes. K.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Confirmation_bias&#34;&gt;Confirmation bias&lt;/a&gt; (my personal favourite of all the cognitive biases) - interviewees make a first impression on us… then we spend the rest of the time in the interview trying to confirm our initial impression. Well, some of us do. The more sound of mind amongst us know that if a person makes a first impression as a well put together educated young man, then they are obviously a machete wielding senile woman.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get a disinterested party to interview, for objectivity. Like, get someone from another team.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Have a subordinate interview and get his/her opinion.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aggregate many interviewers’ opinions - wisdom of the crowd.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;defusing-the-exploding-offer-the-farpoint-gambit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defusing the Exploding Offer: The Farpoint Gambit&lt;/h1&gt;
&lt;p&gt;This isn’t a chapter, but I thought it a cool enough idea to merit its own section.&lt;/p&gt;
&lt;p&gt;An exploding offer is an offer of employment which expires after a certain amount of time. This can be quite a short amount of time, intending to pressurize. The gambit is simply to accept the offer… provisionally. What you do is set out a condition of your own which causes the deadline to pass. Once the deadline passes, the credibility of the threat is destroyed.&lt;/p&gt;
&lt;p&gt;Say you have 1 week within which to accept the offer. You say: “Gee, I accept!… But I’d sure like to meet the team I’ll be working with. I can’t this week, because my aunt is getting her tonsils removed, so maybe next week?” Of course, next week is already after the deadline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chapter-6---let-the-inmates-run-the-asylum&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chapter 6 - Let the Inmates Run The Asylum&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Eliminate status symbols - symbols and stories matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;He spends a lot of time advocating for the removal of managers’ power. However, one place he says managers should have power, is in tie breaking. When a team can’t make a decision, it can’t move forward. By having a person break the tie, we can all continue making dog food at the dog food factory. One of managers’ primary responsibilities then, is to BREAK TIES.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;im-tired-and-going-to-sleep&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I’m tired and going to sleep&lt;/h1&gt;
&lt;p&gt;So I will continue this in a part 2, at some point in the future.&lt;/p&gt;
&lt;p&gt;Until then, Dear Reader, never stop learning, reading and being awesome!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hitchhiker&#39;s Guide to Data Science</title>
      <link>/post/hitchhiker-s-guide-to-data-science/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hitchhiker-s-guide-to-data-science/</guid>
      <description>&lt;p&gt;&lt;em&gt;MapReduce&lt;/em&gt;:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>r-pca</title>
      <link>/post/r-pca/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/r-pca/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Principal Components Analysis (PCA) is a commonly used dimensionality reduction algorithm. In this post, we show how to perform PCA in R. This post will only briefly touch on the theory behind PCA; instead, we focus on intuition, implementation and interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca---a-brief-introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. PCA - A Brief Introduction&lt;/h1&gt;
&lt;p&gt;PCA dates back to the early 20th century, and is used to reduce the dimensionality of data (technically, it’s a &lt;em&gt;linear&lt;/em&gt; dimensionality reduction procedure. We won’t get into what that means in this post).&lt;/p&gt;
&lt;p&gt;The reason you might want to do this is because high dimensional data might lie in a low dimensional subspace. We want to “re-state” this data in that lower dimensional space; that is, we want to get rid of those superfluous extra dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Intuition&lt;/h1&gt;
&lt;p&gt;A simple example could be collecting data about houses in your neighbourhood; you collect features such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of windows&lt;/li&gt;
&lt;li&gt;Number of bedrooms&lt;/li&gt;
&lt;li&gt;House price&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each one of these is another dimension in your dataset. Say you mistakenly collect the area of your house both in square meters &lt;em&gt;and&lt;/em&gt; in square feet. You now have 5 dimensional data, but really, it only lies in a 4 dimensional subspace.&lt;/p&gt;
&lt;p&gt;This is obviously a contrived example, but hopefully you get the picture.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Implementation in R&lt;/h1&gt;
&lt;p&gt;Let’s generate data which simulates some marks in two tests, a maths test and a physics test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1337)
mathScore &amp;lt;- runif(100) * 100
physicsScore &amp;lt;- mathScore + rnorm(100, sd = 8)

#Let&amp;#39;s make sure that there are no negative marks:
physicsScore &amp;lt;- pmax(0, physicsScore)

#And let&amp;#39;s make sure that there are no marks greater than 100:
physicsScore &amp;lt;- pmin(100, physicsScore)

allTestResults &amp;lt;- data.frame(math = mathScore, physics = physicsScore)
plot(allTestResults$math, allTestResults$physics, xlab = &amp;quot;Math&amp;quot;, ylab = &amp;quot;Physics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see from the picture above that the data are pretty correlated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(allTestResults$math, allTestResults$physics)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9725437&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe there is some underlying, latent “smartness” dimension upon which the data truly lie. Let us attempt to determine&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_results &amp;lt;- prcomp(allTestResults, center = TRUE) #Don&amp;#39;t need to scale, since both axes already on same scale, but centering is NB

print (pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Standard deviations (1, .., p=2):
## [1] 42.769762  5.045888
## 
## Rotation (n x k) = (2 x 2):
##                PC1        PC2
## math    -0.7050443  0.7091633
## physics -0.7091633 -0.7050443&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pca_results$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s scale the plot:
plot(pca_results$x, xlim = c(-60, 80), ylim = c(-60, 80))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;screeplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pov &amp;lt;- pca_results$sdev^2/sum(pca_results$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Percentage of variance explained by the first principal component: 98.6272283%&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(pca_results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-pca_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# allTestResults[8, ]
# allTestResults[34, ]
# allTestResults[100, ]
# allTestResults[57, ]
# allTestResults[84, ]
# allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[8, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 8 28.11173 32.83384&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[34, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 34 22.91205 22.28855&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[100, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         math physics
## 100 97.10537     100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[57, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 57 14.12764 7.651867&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[84, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       math  physics
## 84 18.4879 15.14439&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allTestResults[39, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        math  physics
## 39 29.27543 36.10048&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>r-tsne</title>
      <link>/post/r-tsne/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/r-tsne/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
library(cluster)
#####

a_1 &amp;lt;- rnorm(200, mean = 20, sd = 2)
a_2 &amp;lt;- rnorm(200, mean = 272, sd = 5)
a_3 &amp;lt;- rnorm(200, mean = 62, sd = 3)
a_4 &amp;lt;- rnorm(200, mean = 600, sd = 9)
a_5 &amp;lt;- rnorm(200, mean = 209, sd = 5)
a_y &amp;lt;- rep(1, 200)


b_1 &amp;lt;- rnorm(200, mean = -20, sd = 32)
b_2 &amp;lt;- rnorm(200, mean = 23, sd = 5)
b_3 &amp;lt;- rnorm(200, mean = -32, sd = 31)
b_4 &amp;lt;- rnorm(200, mean = 5670, sd = 93)
b_5 &amp;lt;- rnorm(200, mean = 19283, sd = 59)
b_y &amp;lt;- rep(0, 200)


data &amp;lt;- data.frame(first = c(a_1, b_1),
                   second = c(a_2, b_2),
                   third = c(a_3, b_3),
                   fourth = c(a_4, b_4),
                   fifth = c(a_5, b_5),
                   class = c(a_y, b_y))



data_gower &amp;lt;- daisy(data %&amp;gt;% select(-class))

set.seed(1337)
tsne_obj_test &amp;lt;- Rtsne(data_gower, is_distance = TRUE)

tsne_data &amp;lt;- tsne_obj_test$Y %&amp;gt;%
  data.frame() %&amp;gt;%
  setNames(c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;))

tsne_data$class &amp;lt;- data$class

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = class))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-04-r-tsne_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to H2O Using R</title>
      <link>/post/intro-to-h2o-using-r/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/intro-to-h2o-using-r/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In this post we discuss the &lt;a href=&#34;https://www.h2o.ai/&#34;&gt;H2O machine learning platform.&lt;/a&gt; We talk about what H2O is, and how to get started with it, using R - we create a Random Forest which we use to classify the Iris Dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-h2o&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. What is H2O?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The definition found on &lt;a href=&#34;https://github.com/h2oai/h2o-3&#34;&gt;H2O’s Github page&lt;/a&gt; is a lot to take in,&lt;/strong&gt; especially if you’re just starting out with H2O: “H2O is an in-memory platform for distributed, scalable machine learning. H2O uses familiar interfaces like R, Python, Scala, Java, JSON and the Flow notebook/web interface, and works seamlessly with big data technologies like Hadoop and Spark.”&lt;/p&gt;
&lt;p&gt;We spend the rest of section 2 as well as section 3 discussing salient points of the above definition.&lt;/p&gt;
&lt;div id=&#34;h2o-is-an-in-memory-platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 H2O is an In-Memory Platform&lt;/h2&gt;
&lt;p&gt;H2O is an “in-memory platform”. &lt;strong&gt;Saying that it’s in-memory means that the data being used is loaded into main memory (RAM).&lt;/strong&gt; Reading from main memory, (also known as primary memory) is typically much faster than secondary memory (such as a hard drive).&lt;/p&gt;
&lt;p&gt;H2O is a “platform.” &lt;strong&gt;A platform is software which can be used to build something - in this case, machine learning models.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Putting this togther we now know that H2O is an in-memory environment for building machine learning models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;h2o-is-distributed-and-scalable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 H2O is Distributed and Scalable&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;H2O can be run on a cluster.&lt;/strong&gt; &lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt; is an example of a cluster which can run H2O.&lt;/p&gt;
&lt;p&gt;H2O is said to be distributed because your object can be spread amongst several nodes in your cluster. H2O does this by using a Distributed Key Value (DKV). You can read more about it &lt;a href=&#34;https://www.h2o.ai/blog/h2o-architecture/&#34;&gt;here&lt;/a&gt;, but essentially what this means, is that &lt;strong&gt;any object you create in H2O can be distributed amongst several nodes in the cluster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The key-value part of DKV means that when you load data into H2O, you get back a key into a hashmap containing your (potentially distributed) object.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-h2o-runs-under-the-hood&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. How H2O Runs Under the Hood&lt;/h1&gt;
&lt;p&gt;We spoke earlier about H2O being a platform. &lt;strong&gt;It’s important to distinguish between the R interface for H2O, and H2O itself.&lt;/strong&gt; H2O can exist perfectly fine without R. H2O is just a &lt;a href=&#34;https://en.wikipedia.org/wiki/JAR_(file_format)&#34;&gt;.jar&lt;/a&gt; which can be run on its own. If you don’t know (or particularly care) what a .jar is - just think of it as Java code packaged with all the stuff you need in order to run it.&lt;/p&gt;
&lt;p&gt;When you start H2O, you actually create a server which can respond to &lt;a href=&#34;https://en.wikipedia.org/wiki/Representational_state_transfer&#34;&gt;REST&lt;/a&gt; calls. Again, you don’t really need to know how REST works in order to use H2O. But if you do care, just know that &lt;strong&gt;you can use any HTTP client to speak with an H2O instance.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;R is just a client interfact for H2O. All the R functions you call when working with H2O are actually calling H2O using a REST API (a JSON POST request) under the hood. The Python H2O library, as well as the &lt;a href=&#34;https://www.h2o.ai/blog/introducing-flow/&#34;&gt;Flow UI&lt;/a&gt;, interface with H2O in a similar way. &lt;strong&gt;If this is all very confusing just think about it like this: you use R to send commands to H2O. You could equally well just use Flow or Python to send commands.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-an-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Running An Example&lt;/h1&gt;
&lt;div id=&#34;installing-h2o&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.1 Installing H2O&lt;/h2&gt;
&lt;p&gt;You can install H2O using R: &lt;code&gt;install.packages(&amp;quot;h2o&amp;quot;)&lt;/code&gt;. If you’re having trouble with this, &lt;a href=&#34;http://h2o-release.s3.amazonaws.com/h2o/rel-xia/2/index.html&#34;&gt;have a look here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;starting-h2o-and-loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.2 Starting H2O and Loading Data&lt;/h2&gt;
&lt;p&gt;First we’ll need to load the packages we’ll be using: &lt;code&gt;h2o&lt;/code&gt; and &lt;code&gt;datasets&lt;/code&gt;. We load the latter as we’ll be using the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Iris Dataset&lt;/a&gt;, which is part of the &lt;code&gt;datasets&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
library(h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Iris Dataset contains attributes of three species of iris flowers.&lt;/p&gt;
&lt;p&gt;Let’s load the iris dataset, and start up our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init(nthreads = -1) 
data(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, H2O starts up using 2 cores. By calling &lt;code&gt;h2o.init(nthreads = -1)&lt;/code&gt;, with &lt;code&gt;nthreads = -1&lt;/code&gt;, we use all available cores.&lt;/p&gt;
&lt;p&gt;Edit: it doesn’t default to two cores anymore (as per this tweet from H2O’s chief ML scientist):&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Nice post! BTW, H2O in R no longer defaults to 2 cores, so you can just do &lt;code&gt;h2o.init()&lt;/code&gt; now. :-)
&lt;/p&gt;
— Erin LeDell (&lt;span class=&#34;citation&#34;&gt;@ledell&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/ledell/status/1075444885296168962?ref_src=twsrc%5Etfw&#34;&gt;December 19, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;If &lt;code&gt;h2o.init()&lt;/code&gt; was succesful, you should have an instance of H2O running locally! You can verify this by navigating to &lt;a href=&#34;http://localhost:54321&#34; class=&#34;uri&#34;&gt;http://localhost:54321&lt;/a&gt;. There, you should see the Flow UI.&lt;/p&gt;
&lt;p&gt;The iris dataset is now loaded into R. However, it’s not yet in H2O. Let’s go ahead and load the iris data into our H2O instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex &amp;lt;- as.h2o(iris)
h2o.ls()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.ls()&lt;/code&gt; lists the dataframes you have loaded into H2O. Right now, you should see only one: iris.&lt;/p&gt;
&lt;p&gt;Let’s start investigating this dataframe. We can get the summary statistics of the various columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use H2O to plot histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.hist(iris.hex$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-04-r-h2o-introduction_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can use familiar R syntax to modify your H2O dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- iris.hex$Sepal.Length + 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now run &lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;, we should see this extra variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.describe(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Label Type Missing Zeros PosInf NegInf Min Max     Mean     Sigma
## 1 Sepal.Length real       0     0      0      0 4.3 7.9 5.843333 0.8280661
## 2  Sepal.Width real       0     0      0      0 2.0 4.4 3.057333 0.4358663
## 3 Petal.Length real       0     0      0      0 1.0 6.9 3.758000 1.7652982
## 4  Petal.Width real       0     0      0      0 0.1 2.5 1.199333 0.7622377
## 5      Species enum       0    50      0      0 0.0 2.0       NA        NA
## 6          foo real       0     0      0      0 6.3 9.9 7.843333 0.8280661
##   Cardinality
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5           3
## 6          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(What I still don’t understand is why we don’t see this extra column from the Flow UI. If anyone knows, please let me know in the comments!)&lt;/p&gt;
&lt;p&gt;But we don’t really need this nonsense column, so let’s get rid of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris.hex$foo &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get our dataframe back into R, from H2O:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_df &amp;lt;- as.data.frame(iris.hex)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.3 Building a Model&lt;/h2&gt;
&lt;p&gt;We’ve got our H2O instance up and running, with some data in it. Let’s go ahead and do some machine learning - let’s implement a Random Forest.&lt;/p&gt;
&lt;p&gt;First off, we’ll split our data into a training set and a test set. I’m not going to explicitly set a validation set, as the algorithm will use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-bag_error&#34;&gt;out of bag error&lt;/a&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits &amp;lt;- h2o.splitFrame(data = iris.hex,
                         ratios = c(0.8),  #partition data into 80% and 20% chunks
                         seed = 198)

train &amp;lt;- splits[[1]]
test &amp;lt;- splits[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;h2o.splitFrame()&lt;/code&gt; uses approximate splitting. That is, it won’t split the data into an exact 80%-20% split. Setting the seed allows us to create reproducible results.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;h2o.nrow()&lt;/code&gt; to check the number of rows in our train and test sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in train set: &amp;quot;, h2o.nrow(train)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in train set: 117&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;Number of rows in test set: &amp;quot;, h2o.nrow(test)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of rows in test set: 33&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s call &lt;code&gt;h2o.randomForest()&lt;/code&gt; to create our model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf &amp;lt;- h2o.randomForest(x = c(&amp;quot;Sepal.Length&amp;quot;, &amp;quot;Sepal.Width&amp;quot;, &amp;quot;Petal.Length&amp;quot;, &amp;quot;Petal.Width&amp;quot;),
                    y = c(&amp;quot;Species&amp;quot;),
                    training_frame = train,
                    model_id = &amp;quot;our.rf&amp;quot;,
                    seed = 1234)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; specify our independent and dependent variables, respectively. The &lt;code&gt;training_frame&lt;/code&gt; specified the training set, and &lt;code&gt;model_id&lt;/code&gt; is the model name, within H2O (not to be confused with variable &lt;code&gt;rf&lt;/code&gt; in the above code - &lt;code&gt;rf&lt;/code&gt; is the R handle; whereas &lt;code&gt;our.rf&lt;/code&gt; is what H2O calls the model). &lt;code&gt;seed&lt;/code&gt; is used for reproducibility.&lt;/p&gt;
&lt;p&gt;We can get the model details simply by printing out the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(rf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model Details:
## ==============
## 
## H2OMultinomialModel: drf
## Model ID:  our.rf 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth
## 1              50                      150               18941         1
##   max_depth mean_depth min_leaves max_leaves mean_leaves
## 1         7    3.26000          2         12     5.41333
## 
## 
## H2OMultinomialMetrics: drf
## ** Reported on training data. **
## ** Metrics reported on Out-Of-Bag training samples **
## 
## Training Set Metrics: 
## =====================
## 
## Extract training frame with `h2o.getFrame(&amp;quot;RTMP_sid_8ebc_7&amp;quot;)`
## MSE: (Extract with `h2o.mse`) 0.03286954
## RMSE: (Extract with `h2o.rmse`) 0.1812996
## Logloss: (Extract with `h2o.logloss`) 0.09793089
## Mean Per-Class Error: 0.0527027
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;,train = TRUE)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error      Rate
## setosa         40          0         0 0.0000 =  0 / 40
## versicolor      0         38         2 0.0500 =  2 / 40
## virginica       0          4        33 0.1081 =  4 / 37
## Totals         40         42        35 0.0513 = 6 / 117
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;,train = TRUE)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.948718
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That seems pretty good. But let’s see how the model performs on the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_perf1 &amp;lt;- h2o.performance(model = rf, newdata = test)
print(rf_perf1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## H2OMultinomialMetrics: drf
## 
## Test Set Metrics: 
## =====================
## 
## MSE: (Extract with `h2o.mse`) 0.05806405
## RMSE: (Extract with `h2o.rmse`) 0.2409648
## Logloss: (Extract with `h2o.logloss`) 0.1708688
## Mean Per-Class Error: 0.1102564
## Confusion Matrix: Extract with `h2o.confusionMatrix(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa         10          0         0 0.0000 = 0 / 10
## versicolor      0          9         1 0.1000 = 1 / 10
## virginica       0          3        10 0.2308 = 3 / 13
## Totals         10         12        11 0.1212 = 4 / 33
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&amp;lt;model&amp;gt;, &amp;lt;data&amp;gt;)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.878788
## 2 2  1.000000
## 3 3  1.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, let’s use our model to make some predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- h2o.predict(rf, test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   predict    setosa versicolor  virginica
## 1  setosa 0.9969698          0 0.00303019
## 2  setosa 0.9969698          0 0.00303019
## 3  setosa 0.9969698          0 0.00303019
## 4  setosa 0.9969698          0 0.00303019
## 5  setosa 0.9969698          0 0.00303019
## 6  setosa 0.9969698          0 0.00303019
## 
## [33 rows x 4 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;This post discussed what H2O is, and how to use it from R. The full code used in this post can be found &lt;a href=&#34;./h2o_intro.R&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inverse Matrices - A Primer</title>
      <link>/post/inverse-matrix/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/inverse-matrix/</guid>
      <description>

&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;In this post &lt;strong&gt;we talk about the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;inverse matrix&lt;/em&gt;&lt;/strong&gt;. We discuss what it means for a matrix to be &lt;em&gt;invertible&lt;/em&gt;. We then discuss what it means for a matrix &lt;em&gt;not&lt;/em&gt; to be invertible - &lt;em&gt;singular&lt;/em&gt;. Finally, we briefly go through how to find a matrix&amp;rsquo;s inverse.&lt;/p&gt;

&lt;h1 id=&#34;2-invertibility&#34;&gt;2. Invertibility&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s think about a square matrix, &lt;code&gt;$A$&lt;/code&gt;. In &lt;a href=&#34;/post/second-post/&#34;&gt;a previous post&lt;/a&gt; we mentioned that &lt;strong&gt;a matrix acts on a vector&lt;/strong&gt;. The matrix does something to that vector.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The inverse of a matrix undoes what the original matrix did.&lt;/strong&gt; The inverse of &lt;code&gt;$A$&lt;/code&gt; is &lt;code&gt;$A^{-1}$&lt;/code&gt;. &lt;code&gt;$A^{-1}$&lt;/code&gt; undoes what &lt;code&gt;$A$&lt;/code&gt; did. It&amp;rsquo;s kind of like dividing by a number after multiplying by it.&lt;/p&gt;

&lt;p&gt;If we have a vector &lt;code&gt;$x$&lt;/code&gt;, and we multiply it by our matrix &lt;code&gt;$A$&lt;/code&gt;, we get &lt;code&gt;$Ax$&lt;/code&gt;. If we undo this operation by applying the inverse, we get back to &lt;code&gt;$x = A^{-1}Ax$&lt;/code&gt;. The one matrix which always leaves any vector unchanged is the identity matrix, &lt;code&gt;$I$&lt;/code&gt;. That is: &lt;code&gt;$Ix = x$&lt;/code&gt; . &lt;strong&gt;This is why &lt;code&gt;$A^{-1}A = I$&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When matrix &lt;code&gt;$A$&lt;/code&gt; has an inverse it is said to be &lt;em&gt;invertible&lt;/em&gt;. However, this is not always the case. &lt;strong&gt;When a matrix is not invertible, it is also known as&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;singular.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;3-non-invertibility-singular-matrices&#34;&gt;3. Non-Invertibility - Singular Matrices&lt;/h1&gt;

&lt;p&gt;Not all matrics are invertible. &lt;strong&gt;Matrices which are not invertible are known as&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;singular.&lt;/em&gt;&lt;/strong&gt; This means that there exists no matrix which can undo what the original matrix just did.&lt;/p&gt;

&lt;p&gt;Say we are working in two dimensions - a plane. &lt;strong&gt;Most &lt;code&gt;$2 \times 2$&lt;/code&gt; matrices will move vectors around the &lt;code&gt;$2 \times 2$&lt;/code&gt; plane.&lt;/strong&gt; They&amp;rsquo;ll either stretch them, or squash them, or even change their direction. But they&amp;rsquo;ll usually use up all of the plane; the output of that matrix multiplication won&amp;rsquo;t just be limited to a single line on that plane. The output of that matrix multiplication can &lt;strong&gt;&lt;em&gt;usually&lt;/em&gt;&lt;/strong&gt; fall anywhere on the plane.&lt;/p&gt;

&lt;p&gt;But &lt;strong&gt;if the output of a &lt;code&gt;$2 \times 2$&lt;/code&gt; matrix always lands on a single line, then there is no way to undo that operation.&lt;/strong&gt; We&amp;rsquo;ve, in effect, lost some information.&lt;/p&gt;

&lt;p&gt;This extends to higher dimensions. &lt;strong&gt;If you cannot select a vector in &lt;code&gt;$\mathbb{R}^n$&lt;/code&gt; so that the output of an &lt;code&gt;$n \times n$&lt;/code&gt; matrix multiplication with that vector can fall anywhere within the &lt;code&gt;$\mathbb{R}^n$&lt;/code&gt; hyperplane, then that &lt;code&gt;$n \times n$&lt;/code&gt; matrix is singular.&lt;/strong&gt; This means that the output of that matrix multiplication is constrained to a lower dimension (even a line, or a point).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It&amp;rsquo;s kind of like how &lt;code&gt;$0$&lt;/code&gt; has no inverse.&lt;/strong&gt; If we have the number &lt;code&gt;$10$&lt;/code&gt;, and we multiply it by &lt;code&gt;$5$&lt;/code&gt;, we get &lt;code&gt;$50$&lt;/code&gt;. If we want to undo this operation, we multiply by &lt;code&gt;$5$&lt;/code&gt;&amp;rsquo;s (multiplicative) inverse: &lt;code&gt;$\frac {1} {5}$&lt;/code&gt; and get back &lt;code&gt;$50 \times \frac {1} {5} = 10$&lt;/code&gt;. However, if we multiply &lt;code&gt;$10$&lt;/code&gt; by &lt;code&gt;$0$&lt;/code&gt;, we get &lt;code&gt;$0$&lt;/code&gt;. There really isn&amp;rsquo;t much we can do to &lt;code&gt;$0$&lt;/code&gt; to get back to &lt;code&gt;$10$&lt;/code&gt;. This is a tenuous analogy at best, as matrices and numbers are different animals. But, hopefully this helps convey &lt;em&gt;some&lt;/em&gt; intuition.&lt;/p&gt;

&lt;h1 id=&#34;4-testing-for-singularity&#34;&gt;4. Testing for Singularity&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;There are several equivalent conditions which, if met, mean that a matrix is singular.&lt;/strong&gt; Some of these conditions are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The matrix&amp;rsquo;s determinant is 0&lt;/li&gt;
&lt;li&gt;The matrix does not have full rank&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;$n \times n$&lt;/code&gt; matrix does not have all of its pivots.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Discussions about rank and determinants are enough to warrant their own posts (and more), and are beyond the scope of this post. When I do get around to writing those posts, I&amp;rsquo;ll be sure to link them here!&lt;/p&gt;

&lt;p&gt;For now, let&amp;rsquo;s just keep in mind that not all matrices have inverses, and that if the determinant of a matrix is zero, then it doesn&amp;rsquo;t have an inverse.&lt;/p&gt;

&lt;p&gt;But if it does have an inverse, let&amp;rsquo;s try out finding it:&lt;/p&gt;

&lt;h1 id=&#34;5-inverse-of-a-2x2-matrix&#34;&gt;5. Inverse of a 2x2 Matrix&lt;/h1&gt;

&lt;p&gt;A &lt;code&gt;$2 \times 2$&lt;/code&gt; matrix has this form:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  a &amp;amp; b  \newline
  c &amp;amp; d
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;If a &lt;code&gt;$2 \times 2$&lt;/code&gt; matrix is invertible, then its inverse is:&lt;/p&gt;

&lt;p&gt;\[
\frac {1} {ad - bc}
\times
 \begin{bmatrix}
  d &amp;amp; -b  \newline
  -c &amp;amp; a
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$\frac {1} {ad - bc}$&lt;/code&gt; is the determinant of the matrix. If it&amp;rsquo;s zero, then we&amp;rsquo;d have &lt;code&gt;$\frac {1} {0}$&lt;/code&gt;. In other words, the inverse wouldn&amp;rsquo;t exist - our matrix would be singular.&lt;/p&gt;

&lt;p&gt;Consider the matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;According to our formula above, its inverse is:&lt;/p&gt;

&lt;p&gt;\[
\frac {1} {1 \times 7 - 3 \times 2}
\times
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
%
&amp;#61;
%
\frac {1} {1}
\times
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
 %
&amp;#61;
%
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s verify that this is indeed the inverse in two ways.&lt;/p&gt;

&lt;h2 id=&#34;5-1-verification-1-matrix-times-inverse-equals-identity&#34;&gt;5.1 Verification #1 - Matrix times Inverse Equals Identity&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll multiply the original matrix by the inverse. If we get the identity matrix, then we&amp;rsquo;ve done our calculation correctly:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
  \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 \times 7 + 3 \times (-2) &amp;amp; 1 \times (-3) + 3 \times 1  \newline
  2 \times 7 + 7 \times (-2) &amp;amp; 2 \times (-3) + 7 \times 1
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 &amp;amp; 0  \newline
  0 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;h2 id=&#34;5-2-verification-2-inverse-undoes-what-the-original-matrix-did&#34;&gt;5.2 Verification #2 - Inverse Undoes what the Original Matrix Did&lt;/h2&gt;

&lt;p&gt;Next, we&amp;rsquo;ll multiply a vector by the original matrix and then the result of that calculation, by the inverse. &lt;strong&gt;We should get back our original vector.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s pick the vector &lt;code&gt;$ \begin{bmatrix} 1 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  1 &amp;amp; 3  \newline
  2 &amp;amp; 7
 \end{bmatrix}
  \begin{bmatrix}
  1 \newline
  2
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  1 \times 1 + 3 \times 2 \newline
  2 \times 1 + 7 \times 2
 \end{bmatrix}
  %
  &amp;#61;
  %
  \begin{bmatrix}
  7 \newline
  16
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now see if the inverse matrix brings  &lt;code&gt;$ \begin{bmatrix} 7 &amp;amp; 16 \end{bmatrix} ^ {T}$&lt;/code&gt; back to &lt;code&gt;$ \begin{bmatrix} 1 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;\[
 \begin{bmatrix}
  7 &amp;amp; -3  \newline
  -2 &amp;amp; 1
 \end{bmatrix}
  \begin{bmatrix}
  7 \newline
  16
 \end{bmatrix}
  %
  &amp;#61;
  %
 \begin{bmatrix}
  7 \times 7 + (-3) \times 16 \newline
  (-2) \times 7 + 1 \times 16
 \end{bmatrix}
  %
  &amp;#61;
  %
  \begin{bmatrix}
  1 \newline
  2
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;So it looks like we&amp;rsquo;ve done our calculations correctly!&lt;/p&gt;

&lt;h1 id=&#34;6-inverse-of-a-larger-matrix&#34;&gt;6. Inverse of a Larger Matrix&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;For larger matrices, we can&amp;rsquo;t compute the inverse in the same way that we did for &lt;code&gt;$2 \times 2$&lt;/code&gt; matrices.&lt;/strong&gt; Here, we&amp;rsquo;ll employ a different strategy using &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_elimination&#34;&gt;Gaussian Elimination&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Say we wish to find the inverse of the 3x3 matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  2 &amp;amp; -1 &amp;amp; 0  \newline
  -1 &amp;amp; 2 &amp;amp; -1 \newline
  0 &amp;amp; -1 &amp;amp; 2
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start off by creating the &lt;em&gt;augmented matrix&lt;/em&gt; &lt;code&gt;$[A \space \space I]$&lt;/code&gt;, where &lt;code&gt;$I$&lt;/code&gt; is the identity matrix:&lt;/p&gt;

&lt;p&gt;\[
[A \space \space I]
%
&amp;#61;
%
 \begin{bmatrix}
  2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0  \newline
  -1 &amp;amp; 2 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We then apply Gaussian Elimination, and end up with the matrix &lt;code&gt;$[I \space \space A^{-1}]$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;\[
[I \space \space A^{-1}]
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{3} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{1} {4}  \newline
  0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \frac{1} {2} &amp;amp; 1 &amp;amp; \frac{1} {2}  \newline
  0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \frac{1} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{3} {4}&lt;br /&gt;
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Yielding the inverse matrix:&lt;/p&gt;

&lt;p&gt;\[
A^{-1}
%
&amp;#61;
%
 \begin{bmatrix}
  \frac{3} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{1} {4}  \newline
  \frac{1} {2} &amp;amp; 1 &amp;amp; \frac{1} {2}  \newline
  \frac{1} {4} &amp;amp; \frac{1} {2} &amp;amp; \frac{3} {4}&lt;br /&gt;
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We can then apply the same sort of calculations that we did for the &lt;code&gt;$2 \times 2$&lt;/code&gt; case, and test that this is indeed the inverse matrix. If you don&amp;rsquo;t believe that this is in fact the inverse, then feel free to do so :) I&amp;rsquo;m going to stop right here, though!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rank of a Matrix</title>
      <link>/post/rank-of-a-matrix/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/rank-of-a-matrix/</guid>
      <description>&lt;p&gt;rank of a matrix
10K – 100K
Low&lt;/p&gt;

&lt;p&gt;The dimension of the column space of a matrix is called its rank.
- rank of 1 - when the output of the matrix multiplication is a line (for next article)
  - rank of 2 - when all the outputs of the matrix mult land in 2-D, then matrix has a rank of 2.
  - etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;It is the number of linearly independent columns&lt;/p&gt;

&lt;p&gt;It is the number of pivots of a matrix&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Matrix Nullspace (Kernel) Tutorial - Finding the Nullspace</title>
      <link>/post/playing-in-the-nullspace/</link>
      <pubDate>Thu, 01 Mar 2018 20:35:21 +0200</pubDate>
      
      <guid>/post/playing-in-the-nullspace/</guid>
      <description>

&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;In this article we describe &lt;strong&gt;what the nullspace is&lt;/strong&gt;. We go on to explain &lt;strong&gt;how to find the nullspace&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&#34;2-nullspace-null-space-or-kernel&#34;&gt;2. Nullspace, Null Space or Kernel?&lt;/h1&gt;

&lt;p&gt;The &lt;em&gt;nullspace&lt;/em&gt; (or &lt;em&gt;null space&lt;/em&gt;) of a matrix is also known as the &lt;em&gt;kernel&lt;/em&gt; of a matrix. &lt;strong&gt;These terms are interchangeable.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The nullspace is a &lt;strong&gt;set of vectors that when multiplied by a matrix returns &lt;code&gt;$0$&lt;/code&gt;&lt;/strong&gt; (well, the zero vector).&lt;/p&gt;

&lt;h1 id=&#34;3-how-to-find-the-nullspace&#34;&gt;3. How To Find the Nullspace&lt;/h1&gt;

&lt;p&gt;Have a look at this matrix:&lt;/p&gt;

&lt;p&gt;\[
A
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  2 &amp;amp; 2 &amp;amp; 8 &amp;amp; 10 \newline
  3 &amp;amp; 3 &amp;amp; 10 &amp;amp; 13
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re interested in finding its nullspace: that is, all vectors such that when multiplied by our matrix &lt;code&gt;$A$&lt;/code&gt;, return the zero vector, &lt;code&gt;$ \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} ^ {T}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I happen to know that &lt;code&gt;$ \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/code&gt; is one such vector:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  2 &amp;amp; 2 &amp;amp; 8 &amp;amp; 10 \newline
  3 &amp;amp; 3 &amp;amp; 10 &amp;amp; 13
 \end{bmatrix}
%
\begin{bmatrix}
 -3 \newline
 1 \newline
 -2 \newline
 2
\end{bmatrix}
%
&amp;#61;
%
%
\begin{bmatrix}
 -3 + 1 - 4 + 6 \newline
 -6 + 2 - 16 + 20 \newline
 -9 + 3 - 20 + 26
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 0 \newline
 0 \newline
 0
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;If we set &lt;code&gt;$x = \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/code&gt; , then we can write out the above equation more succinctly: &lt;code&gt;$Ax = 0$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We say that &lt;strong&gt;&lt;code&gt;$x$&lt;/code&gt; is in the nullspace of &lt;code&gt;$A$&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to look at a &lt;strong&gt;systematic way to find all the vectors in the nullspace.&lt;/strong&gt;
We&amp;rsquo;ll do this is by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Using elimination to find our pivots&lt;/li&gt;
&lt;li&gt;Using back substitution to find our solutions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can think of this as our two-step recipe to find our nullspace.&lt;/p&gt;

&lt;h2 id=&#34;3-1-finding-the-pivots&#34;&gt;3.1 Finding The Pivots&lt;/h2&gt;

&lt;p&gt;This article isn&amp;rsquo;t about how to do Gaussian elimination on a matrix. I&amp;rsquo;m going to assume that you already know how to do this. I hope to write an article about this in the future.&lt;/p&gt;

&lt;p&gt;Suppose we performed elimination on our matrix &lt;code&gt;$A$&lt;/code&gt;, which results in our new &lt;code&gt;$U$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;\[
U
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  0 &amp;amp; 0 &amp;amp; 4 &amp;amp; 4 \newline
  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The cool thing is that &lt;code&gt;$A$&lt;/code&gt; and &lt;code&gt;$U$&lt;/code&gt; have the same nullspace.&lt;/strong&gt; So, if we know that &lt;code&gt;$Ux = 0$&lt;/code&gt;, we also know that &lt;code&gt;$Ax = 0$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s suppose that we didn&amp;rsquo;t know that &lt;code&gt;$x = \begin{bmatrix} -3 &amp;amp; 1 &amp;amp; -2 &amp;amp; 2 \end{bmatrix} ^ {T}$&lt;/code&gt; was in the nullspace of the matrix &lt;code&gt;$A$&lt;/code&gt;. We know that there is some vector &lt;code&gt;$x = \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; x_3 &amp;amp; x_4 \end{bmatrix} ^ {T}$&lt;/code&gt; such that &lt;code&gt;$Ux = 0$&lt;/code&gt; (even if that vector is just &lt;code&gt;$\begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} ^ {T}$&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Writing this out more explicitly:&lt;/p&gt;

&lt;p&gt;\[
\tag{1}
\label{1}
 \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 \newline
  0 &amp;amp; 0 &amp;amp; 4 &amp;amp; 4 \newline
  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
 \end{bmatrix}
 %
 \begin{bmatrix}
  x_1 \newline
  x_2 \newline
  x_3 \newline
  x_4
 \end{bmatrix}
 %
  &amp;#61;
 %
 \begin{bmatrix}
  0 \newline
  0 \newline
  0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;According to our recipe for finding the nullspace, we need to find our pivots. &lt;strong&gt;A pivot is simply the first non-zero element in each row of the matrix.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Looking at &lt;code&gt;$U$&lt;/code&gt;, the first row has a pivot in the first column, with value &lt;code&gt;$1$&lt;/code&gt;. The second row has a pivot in the third column: &lt;code&gt;$4$&lt;/code&gt;. The third row does not have a pivot.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We call &lt;code&gt;$x_1$&lt;/code&gt; and &lt;code&gt;$x_3$&lt;/code&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;pivot variables&lt;/em&gt;&lt;/strong&gt; since columns 1 and 3 contain pivots. &lt;strong&gt;&lt;code&gt;$x_2$&lt;/code&gt; and &lt;code&gt;$x_4$&lt;/code&gt; are called&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;free variables&lt;/em&gt;&lt;/strong&gt; since columns 2 and 4 have no pivots.&lt;/p&gt;

&lt;p&gt;Note that there are 4 variables in total - 2 free variables, and 2 pivot variables. This is because the nullspace vectors are in &lt;code&gt;$\mathbb{R}^4$&lt;/code&gt; in this case, since &lt;code&gt;$A$&lt;/code&gt; (and &lt;code&gt;$U$&lt;/code&gt;) are &lt;code&gt;$3 \times 4$&lt;/code&gt; matrices.&lt;/p&gt;

&lt;h2 id=&#34;3-2-using-back-substitution-to-find-our-nullspace&#34;&gt;3.2 Using Back Substitution to Find our Nullspace&lt;/h2&gt;

&lt;p&gt;Now that we know what our pivots are, we want to find our &lt;em&gt;special solutions&lt;/em&gt;. Special solutions are vectors in our nullspace which will eventually &lt;strong&gt;help us find all the vectors in our nullspace&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We will have as many special solutions as we have free variables.&lt;/strong&gt; In this case, we have 2 free variables, so we will have 2 special solutions. The way we find the special solutions is by &lt;em&gt;back substitution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Before we get to what back substition is, let&amp;rsquo;s rewrite equation &lt;code&gt;$(\ref{1})$&lt;/code&gt; as a set of equations, instead of its matrix form:&lt;/p&gt;

&lt;p&gt;\[
x_1 + x_2 + 2x_3 + 3x_4 = 0
\]&lt;/p&gt;

&lt;p&gt;\[
4x_3 + 4x_4 = 0
\]&lt;/p&gt;

&lt;p&gt;We want to find values for &lt;code&gt;$x_1$&lt;/code&gt;, &lt;code&gt;$x_2$&lt;/code&gt;, &lt;code&gt;$x_3$&lt;/code&gt; and &lt;code&gt;$x_4$&lt;/code&gt; which satisfy the above set of equations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Since &lt;code&gt;$x_2$&lt;/code&gt; and &lt;code&gt;$x_4$&lt;/code&gt; are free variables, we can give them any values we wish&lt;/strong&gt; - they are &lt;em&gt;free&lt;/em&gt; variables, after all! The simples choice for our free variables is ones or zeroes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s set &lt;code&gt;$x_2 = 1$&lt;/code&gt; and &lt;code&gt;$x_4 = 0$&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;\[
x_1 + (1) + 2x_3 + 3(0) = 0
\]&lt;/p&gt;

&lt;p&gt;\[
4x_3 + 4(0) = 0
\]&lt;/p&gt;

&lt;p&gt;which gives:&lt;/p&gt;

&lt;p&gt;\[
\label{2}
\tag{2}
x_1 + 1 + 2x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;\[
\label{3}
\tag{3}
x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;Great! &lt;strong&gt;We now know that &lt;code&gt;$x_3 = 0$&lt;/code&gt;&lt;/strong&gt; (and we have values for &lt;code&gt;$x_2$&lt;/code&gt; and &lt;code&gt;$x_4$&lt;/code&gt; ). All that&amp;rsquo;s left is for us to find &lt;code&gt;$x_1$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now, let&amp;rsquo;s use back substitution to plug &lt;code&gt;$x_3 = 0$&lt;/code&gt; into equation (&lt;code&gt;$\ref{2}$&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\[
x_1 + 1 + 2(0) = 0
\]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;p&gt;\[
x_1 = -1
\]&lt;/p&gt;

&lt;p&gt;We now have our first special solution:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3 \newline
 x_4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -1 \newline
 1 \newline
 0 \newline
 0
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Setting &lt;code&gt;$x_2 = 1$&lt;/code&gt; and &lt;code&gt;$x_4 = 0$&lt;/code&gt; gave us our first special solution. &lt;strong&gt;We need to find our second special solution.&lt;/strong&gt; The easiest way to do this is to set &lt;code&gt;$x_2 = 0$&lt;/code&gt; and &lt;code&gt;$x_4 = 1$&lt;/code&gt;. Using the same procedure as we did for the first special solution we find our second special solution:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 -1 \newline
 0 \newline
 -1 \newline
 1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Since we had 2 free variables, we get 2 special solutions:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  -1 \newline
  1 \newline
  0 \newline
  0
\end{bmatrix},
\begin{bmatrix}
  -1 \newline
  0 \newline
  -1 \newline
  1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;These two special solutions are &lt;strong&gt;in the nullspace of &lt;code&gt;$U$&lt;/code&gt; and therefore also &lt;code&gt;$A$&lt;/code&gt;&lt;/strong&gt;. This means that if you multiply the matrix &lt;code&gt;$A$&lt;/code&gt; or &lt;code&gt;$U$&lt;/code&gt; by these vectors, you will get the zero vector.&lt;/p&gt;

&lt;p&gt;Crucially - &lt;strong&gt;&lt;em&gt;every combination of our special solutions is in the nullspace&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So, the complete solution is:&lt;/p&gt;

&lt;p&gt;\[
x
%
&amp;#61;
%
 x_2
 %
\begin{bmatrix}
 -1 \newline
 1 \newline
 0 \newline
 0
\end{bmatrix}
%&lt;br /&gt;
+
%
 x_4
 %
\begin{bmatrix}
 -1 \newline
 0 \newline
 -1 \newline
 1
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -x_2 - x_4 \newline
 x_2 \newline
 -x_4 \newline
 x_4
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;What does this actually mean? It means that &lt;strong&gt;we can pick our free variables &lt;code&gt;$x_2$&lt;/code&gt; and &lt;code&gt;$x_4$&lt;/code&gt; however we wish, and we will get a valid solution in response.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For example let&amp;rsquo;s pick: &lt;code&gt;$x_2=1$&lt;/code&gt; and &lt;code&gt;$x_4 = 2$&lt;/code&gt;. So:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 -x_2 - x_4 \newline
 x_2 \newline
 -x_4 \newline
 x_4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 -3 \newline
 1 \newline
 -2 \newline
 2
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;which was my example at the beginning of the article.&lt;/p&gt;

&lt;h2 id=&#34;4-another-example&#34;&gt;4 Another Example&lt;/h2&gt;

&lt;p&gt;Say now I wish to find the nullspace of&lt;/p&gt;

&lt;p&gt;\[
U
%
&amp;#61;
%
 \begin{bmatrix}
  1 &amp;amp; 5 &amp;amp; 7 \newline
  0 &amp;amp; 0 &amp;amp; 9 \newline
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The second column contains our free variable. Since there is only &lt;strong&gt;one free variable, there will only be one special solution&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We need to back substitute &lt;code&gt;$x_2 = 1$&lt;/code&gt; (no need to set zeroes this time, as there is only one free variable).&lt;/p&gt;

&lt;p&gt;Then
\[
9x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;gives&lt;/p&gt;

&lt;p&gt;\[
x_3 = 0
\]&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;\[
x_1 + 5x_2 = 0
\]&lt;/p&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;p&gt;\[
x_1 = -5
\]&lt;/p&gt;

&lt;p&gt;since&lt;/p&gt;

&lt;p&gt;\[
x_2 = 1
\]&lt;/p&gt;

&lt;p&gt;So,
\[
x
%
&amp;#61;
%
x_2
%
 \begin{bmatrix}
  -5 \newline
  1 \newline
  0
 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The nullspace in this case is a line in &lt;code&gt;$\mathbb{R}^3$&lt;/code&gt;. It contains multiples of the special solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of Matrices and Men (pt. 2)</title>
      <link>/post/of-matrices-and-men-part-2/</link>
      <pubDate>Wed, 22 Nov 2017 20:35:21 +0200</pubDate>
      
      <guid>/post/of-matrices-and-men-part-2/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;/post/second-post/&#34;&gt;last post&lt;/a&gt; we saw how a matrix could operate on a vector to produce a result:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
1 \newline
3 \newline
5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;The matrix used above is a &lt;code&gt;$3 \times 3$&lt;/code&gt; &lt;em&gt;difference matrix&lt;/em&gt;. The general form for this type of operation is:&lt;/p&gt;

&lt;p&gt;\[
Ax = b
\]&lt;/p&gt;

&lt;p&gt;From the above equation, you can see that vector &lt;code&gt;$b$&lt;/code&gt; is produced by applying matrix &lt;code&gt;$A$&lt;/code&gt; to vector &lt;code&gt;$x$&lt;/code&gt;. This &lt;code&gt;$Ax = b$&lt;/code&gt; is pretty much the central equation of linear algebra.&lt;/p&gt;

&lt;p&gt;Say you now know what &lt;code&gt;$b$&lt;/code&gt; is and you wish to recover &lt;code&gt;$x$&lt;/code&gt;. That is, given &lt;code&gt;$Ax = b$&lt;/code&gt;, which is:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
 b_1 \newline
 b_2 \newline
 b_3
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We assume that &lt;code&gt;$b$&lt;/code&gt; is known, and we now wish to retrieve &lt;code&gt;$x$&lt;/code&gt;.
We can write out &lt;code&gt;$Ax = b$&lt;/code&gt; as a set of linear equations:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}
1 \cdot x_1 + 0 \cdot x_2 + 0 \cdot x_3 = b_1
\end{equation}
\]
\[
\begin{equation}
-1 \cdot x_1 + 1 \cdot x_2 + 0 \cdot x_3 = b_2
\end{equation}
\]
\[
\begin{equation}
0 \cdot x_1 -1 \cdot x_2 + 1 \cdot x_3 = b_3
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;And now solving for the &lt;code&gt;$x_i$&lt;/code&gt;s:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation}
x_1 = b_1 \qquad
\end{equation}
\]
\[
\begin{equation}
x_2 = b_1 + b_2
\end{equation}
\]
\[
\begin{equation}
\qquad x_3 = b_1 + b_2 + b_3
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;We were able to solve for the &lt;code&gt;$x_i$&lt;/code&gt;s so easily because our matrix &lt;code&gt;$A$&lt;/code&gt; is &lt;em&gt;lower triangular&lt;/em&gt;. This just means that all the entries above the diagonal entries are equal to &lt;code&gt;$0$&lt;/code&gt;. In general, we can recover a unique &lt;code&gt;$x$&lt;/code&gt;, given a &lt;code&gt;$b$&lt;/code&gt;, if the matrix which produced &lt;code&gt;$b$&lt;/code&gt; is &lt;em&gt;invertible&lt;/em&gt;. I&amp;rsquo;ll use my next post to discuss invertibility and why it was possible for us to so easily solve for the &lt;code&gt;$x_i$&lt;/code&gt;s.
For now, just know that from any vector &lt;code&gt;$b$&lt;/code&gt; produced by the the difference matrix &lt;code&gt;$A$&lt;/code&gt;, we can solve for the &lt;code&gt;$x$&lt;/code&gt; which led to it. Let&amp;rsquo;s try
\[
b = \begin{bmatrix} b_1 \newline b_2 \newline b_3 \end{bmatrix} = \begin{bmatrix} 1 \newline 3 \newline 5 \end{bmatrix}
\]
This will give us
\[
x = \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} = \begin{bmatrix} b_1 \qquad \qquad \newline b_1 + b_2 \qquad \newline b_1 + b_2 +b_3 \end{bmatrix}= \begin{bmatrix} 1 \newline 1 + 3 \newline 1 + 3 + 5 \end{bmatrix} = \begin{bmatrix} 1 \newline 4 \newline 9 \end{bmatrix}
\]
This is in fact correct, and should come as no surprise to anybody who has endured high school algebra.&lt;/p&gt;

&lt;p&gt;The cool part comes now.&lt;/p&gt;

&lt;p&gt;From the above, we can write &lt;code&gt;$ \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} $&lt;/code&gt; as follows:
\[
 \begin{bmatrix} b_1 \qquad \qquad \newline b_1 +b_2 \qquad \newline b_1 + b_2 +b_3 \end{bmatrix} = \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix}
\begin{bmatrix} b_1 \newline b_2 \newline b_3 \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t worry too much about exactly &lt;em&gt;how&lt;/em&gt; we found the matrix \begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix} (which we&amp;rsquo;ll call &lt;code&gt;$S$&lt;/code&gt;), just convince yourself that it is, in fact, the correct matrix to have here. The neat thing about this matrix, is that it&amp;rsquo;s actually a &lt;em&gt;sum matrix&lt;/em&gt;; as opposed to the &lt;em&gt;difference matrix&lt;/em&gt;, which is its inverse. There is a nice duality or symmetry going on here. The inverse of the difference matrix is the sum matrix.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take the result of our initial &lt;code&gt;$Ax = b$&lt;/code&gt;, which is
\[
b = \begin{bmatrix}
 1 \newline
 3 \newline
 5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s then apply the sum matrix to it:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 0 \newline
  1 &amp;amp; 1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 1 \newline
 3 \newline
 5
\end{bmatrix}
%
&amp;#61;
%
%
\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We get back our original &lt;code&gt;$x = \begin{bmatrix} 1 \newline 4 \newline 9 \end{bmatrix}$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So, &lt;code&gt;$Ax = b$&lt;/code&gt;, and &lt;code&gt;$Sb = x$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pretty cool, right?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of Matrices and Men (pt. 1)</title>
      <link>/post/second-post/</link>
      <pubDate>Sun, 22 Oct 2017 20:35:21 +0200</pubDate>
      
      <guid>/post/second-post/</guid>
      <description>&lt;p&gt;One of the most crucial changes in my viewpoint came when I learned that matrices, much like functions, &lt;em&gt;act&lt;/em&gt; on inputs. Until then, a matrix was just a collection of numbers and matrix multiplication was just a mechanical set of rules used to produce another collection of numbers. I knew that the result of matrix multiplication could be a scalar, a vector or another matrix&amp;hellip; but I abandoned any further investigations.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m going to assume that the reader is familiar with the mechanics of matrix multiplication. What I&amp;rsquo;ll be talking about is how a matrix affects a vector; in this case, how a matrix transforms a vector into a different vector and that the transformation is encoded in the matrix itself.&lt;/p&gt;

&lt;p&gt;An example Prof. Strang uses in his lectures and book is the &lt;em&gt;difference matrix&lt;/em&gt;. It looks like this:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
&lt;br&gt;
Now, let&amp;rsquo;s say we have a vector: &lt;code&gt;$ \begin{bmatrix} x_1 \newline x_2 \newline x_3 \end{bmatrix} \in  \mathbb{R}^3$&lt;/code&gt;.
Let&amp;rsquo;s say we wish to multiply the above matrix with this vector:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
  1 &amp;amp; 0 &amp;amp; 0 \newline
  -1 &amp;amp; 1 &amp;amp; 0 \newline
  0 &amp;amp; -1 &amp;amp; 1
 \end{bmatrix}
%
\begin{bmatrix}
 x_1 \newline
 x_2 \newline
 x_3
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;We could look at the matrix as the collection of vectors &lt;code&gt;$ \begin{bmatrix} 1 \newline -1 \newline 0 \end{bmatrix}$&lt;/code&gt;, &lt;code&gt;$ \begin{bmatrix} 0 \newline 1 \newline -1 \end{bmatrix}$&lt;/code&gt; and &lt;code&gt;$ \begin{bmatrix} 0 \newline 0 \newline 1 \end{bmatrix}$&lt;/code&gt; and then take the linear combination of columns:
\[
x_1
%
\begin{bmatrix}
1 \newline
-1 \newline
0
\end{bmatrix}
%
+
%
x_2
%
\begin{bmatrix}
0 \newline
1 \newline
-1
\end{bmatrix}
%
+
%
x_3
%
\begin{bmatrix}
0 \newline
0 \newline
1
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;This does give you the correct result. The numbers &lt;code&gt;$x_1$&lt;/code&gt;, &lt;code&gt;$x_2$&lt;/code&gt; and &lt;code&gt;$x_3$&lt;/code&gt; multiply the columns which make up the matrix. But we want to change our point of view. We want to see the matrix as a &lt;em&gt;thing&lt;/em&gt; which somehow transforms the vector. So&amp;hellip; let&amp;rsquo;s finally look at the result of this matrix multiplication:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
x_1 \newline
x_2 - x_1 \newline
x_3 - x_2
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;There it is. Can you see why we called this matrix a difference matrix? The top difference is &lt;code&gt;$x_1 - x_0 = x_1 - 0 = x_1$&lt;/code&gt;.
Let&amp;rsquo;s try this with an example. Let&amp;rsquo;s take a vector with &lt;code&gt;$x_1 = 1$&lt;/code&gt;, &lt;code&gt;$x_2= 4$&lt;/code&gt; and &lt;code&gt;$x_3 = 9$&lt;/code&gt; that is:&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
 1 \newline
 4 \newline
 9
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;The result here is:&lt;/p&gt;

&lt;p&gt;\[
\begin{bmatrix}
 1 - 0 \newline
 4 - 1 \newline
 9 - 4
\end{bmatrix}
%
&amp;#61;
%
\begin{bmatrix}
1 \newline
3 \newline
5
\end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;This matrix will always behave like this - for any vector in &lt;code&gt;$\mathbb{R}^3$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Seeing matrix multiplication between a matrix and a vector as a linear combination of vectors (as we did initially) is obviously correct, and yields a correct result. But it&amp;rsquo;s also important to note that matrices are things which operate on vectors. In this case, our difference matrix will take as &amp;ldquo;input&amp;rdquo; a vector and as &amp;ldquo;output&amp;rdquo; produce a new vector which contains the differences of the &amp;ldquo;input&amp;rdquo; vector. (And here I hope to stop my liberal usage of quotes).&lt;/p&gt;

&lt;p&gt;This post discussed obtaining the result of matrix multiplication. Next time I&amp;rsquo;ll talk about how to find out which combination of columns give you a certain result (and if that&amp;rsquo;s even possible). Unlike this post, I hope to write it while not jetlagged (and I apologize if this post is slightly incoherent. I&amp;rsquo;m blaming timezone differences).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hi There. This is my first post. Or: Linear Algebra, bra?</title>
      <link>/post/my-first-post/</link>
      <pubDate>Sun, 01 Oct 2017 17:55:17 +0200</pubDate>
      
      <guid>/post/my-first-post/</guid>
      <description>&lt;p&gt;I need a place to document my various ideas as well as the potential results of those ideas.&lt;/p&gt;

&lt;p&gt;I also need a place where I can track my progress; that is, progress on a project I may be working on; or progress on material I&amp;rsquo;m busy studying.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll try document such things here. I figure, this way, if I commit to something, there will be some evidence of that commitment&amp;hellip; If I see evidence of it I can say: &amp;ldquo;Oh yeah, at some point I actually intended to do that!&amp;rdquo; and it&amp;rsquo;ll be harder for me to back out of that commitment. So really, I&amp;rsquo;m hoping that this blog helps keep me on track with my goals.&lt;/p&gt;

&lt;p&gt;Currently, I am busy with &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&#34;&gt;MIT&amp;rsquo;s 18.06 course on linear algebra&lt;/a&gt;. It&amp;rsquo;s an incredible course presented by &lt;a href=&#34;https://en.wikipedia.org/wiki/Gilbert_Strang&#34;&gt;Gilbert Strang&lt;/a&gt;; a man whose passion for mathematics is infectious. As an undergraduate student, I went through the motions of linear algebra mechanically, in order to obtain marks and thus progress to the next year of study. I never really internalized the &lt;em&gt;concepts&lt;/em&gt;. This is something which hindered me later in my academic career. I&amp;rsquo;m interested in machine learning and, as with many subjects, linear algebra is a cornerstone. I chose 18.06 because it doesn&amp;rsquo;t spend too much time on proving theorems; rather it aims to convey intuition and understanding by using examples and exposition. Not that proving can&amp;rsquo;t do that, just that it can be difficult to link up those abstract concepts with concrete applications. Maybe I just like Prof Strang&amp;rsquo;s style of teaching. Either way, I&amp;rsquo;ve commited to 18.06 and I&amp;rsquo;ve gone through about 60% of the material. Right now I&amp;rsquo;m on the part of the course which deals with Eigenvalues and Eigenvectors. For the next while, each post will contain something which I find interesting or particularly illuminating about the section of linear algebra I&amp;rsquo;m busy learning. It could be an example, or a concept. Either way, it&amp;rsquo;ll come from the section I&amp;rsquo;m busy with.&lt;/p&gt;

&lt;p&gt;My original aim was to complete this course by the end of the 2017 year. But, I don&amp;rsquo;t think I&amp;rsquo;ll manage to. I&amp;rsquo;m travelling to the US for two weeks at the end of October, and to Cape Town at the end of December. I won&amp;rsquo;t get much done during those periods. I&amp;rsquo;ll give myself til&amp;rsquo; the end of January 2018 to finish up. There it is. I said it. Now there is proof that I said it.&lt;/p&gt;

&lt;p&gt;Yikes.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>